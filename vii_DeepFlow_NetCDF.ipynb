{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb1f18c3-8fa2-4a29-ae48-5ce04c80ceac",
   "metadata": {},
   "source": [
    "# DeepFlow NetCDF\n",
    "Since we're now using NetCDF files instead of png, we will need to modify a lot of the functions we used to visualize our result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e488cd2-646e-4533-bfdc-206cf1c4345b",
   "metadata": {},
   "source": [
    "## Importing necessary libraries and notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fa7fa1-0e84-490b-97fd-75ca1c792c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import io\n",
    "import os\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import netCDF4 as nc\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import ticker\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from matplotlib.collections import LineCollection\n",
    "from IPython.display import Image, display, clear_output\n",
    "from PIL import Image as PILImage\n",
    "import concurrent.futures\n",
    "\n",
    "# Import the other notebooks without running their cells\n",
    "from ii_Data_Manipulation import visualize_4\n",
    "from iii_GOES_average import time_list, visualize_aggregate, calculate_median\n",
    "from iv_Image_Processing import collect_times, crop_image, save_aggregate, binarize_image, bilateral_image, process_dates, process_directory\n",
    "from vii_Flow_Analysis import haversine\n",
    "from v_i_OF_Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d498027-901f-4cbe-8f6d-c9877e6daf99",
   "metadata": {},
   "source": [
    "## DeepFlow on NetCDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121dcc72-5070-43e0-b076-fb489bf877b5",
   "metadata": {},
   "source": [
    "### *visualize*\n",
    "A generalization of the visualize function in the very first notebook with the added option of plotting flow vectors and choosing the step (density) and scale of the flow vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82008ea3-7c07-4d0c-bbc4-41ad9078ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(file_path, variable_key=\"fai_anomaly\", lat_range=None, lon_range=None, color=\"viridis\", colorbar_label=\"\", title=\"\", flow_vectors=None, quiver_step=None, quiver_scale=None):\n",
    "    \"\"\"\n",
    "    Visualizes the NetCDF data and optionally overlays flow vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the NetCDF file.\n",
    "    - variable_key: Key for the variable of interest in the NetCDF dataset.\n",
    "    - lat_range: Tuple of (min, max) latitude to subset the data.\n",
    "    - lon_range: Tuple of (min, max) longitude to subset the data.\n",
    "    - color: Color map for the plot.\n",
    "    - colorbar_label: Label for the color bar.\n",
    "    - title: Title of the plot.\n",
    "    - flow_vectors: Optional tuple of flow vector components (flow_u, flow_v).\n",
    "    - quiver_step: Sampling step for displaying quiver arrows, controls density.\n",
    "    - quiver_scale: Scaling factor for quiver arrows, controls size.\n",
    "    \"\"\"\n",
    "    # Load the netCDF data\n",
    "    data = xr.open_dataset(file_path)\n",
    "    \n",
    "    # If ranges are specified, apply them to select the desired subset\n",
    "    if lat_range and 'latitude' in data.coords:\n",
    "        data = data.sel(latitude=slice(*lat_range))\n",
    "    if lon_range and 'longitude' in data.coords:\n",
    "        data = data.sel(longitude=slice(*lon_range))\n",
    "\n",
    "    # Set up a plot with geographic projections\n",
    "    fig, ax = plt.subplots(figsize=(12, 10), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    \n",
    "    # Extract relevant data \n",
    "    index_data = data[variable_key]\n",
    "\n",
    "    # Plot the data\n",
    "    im = index_data.plot(ax=ax, x='longitude', y='latitude', transform=ccrs.PlateCarree(),\n",
    "                         cmap=color, add_colorbar=True, extend='both', cbar_kwargs={'shrink': 0.35})\n",
    "\n",
    "    # Add color bar details\n",
    "    im.colorbar.set_label(colorbar_label)\n",
    "\n",
    "    # Customize the map with coastlines and features\n",
    "    ax.coastlines(resolution='10m', color='black')\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "    gl = ax.gridlines(draw_labels=True, linewidth=1, color='gray', alpha=0.5, linestyle='--')\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "\n",
    "    # Plot flow vectors if provided\n",
    "    if flow_vectors:\n",
    "        # Automatically determine quiver_step and quiver_scale if not provided\n",
    "        if quiver_step is None:\n",
    "            quiver_step = max(1, int(len(data.latitude) / 20))  # Sample about 20 arrows along the latitude\n",
    "        if quiver_scale is None:\n",
    "            quiver_scale = max(1, int(len(data.latitude) / 2))  # Scale according to number of latitude points\n",
    "\n",
    "        # Create a meshgrid for the flow vectors that matches the data subset\n",
    "        Y, X = np.meshgrid(data.latitude, data.longitude, indexing='ij')\n",
    "        # Apply the step for vector density and scale for vector size\n",
    "        ax.quiver(X[::quiver_step, ::quiver_step], Y[::quiver_step, ::quiver_step], flow_vectors[0][::quiver_step, ::quiver_step], flow_vectors[1][::quiver_step, ::quiver_step], color='red', scale=quiver_scale)\n",
    "\n",
    "    # Show the plot with title\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de105aa-7561-4818-9024-6e72f225d23c",
   "metadata": {},
   "source": [
    "### *calculate_deepflow*\n",
    "A function to calculate and return deepflow using as input two NetCDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d782fed-1dc1-4a94-8fbd-18ac03e6d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_deepflow(nc_file1, nc_file2, variable_key=\"fai_anomaly\", time_interval=86400):\n",
    "    # Load data\n",
    "    data1 = xr.open_dataset(nc_file1)\n",
    "    data2 = xr.open_dataset(nc_file2)\n",
    "    img1 = data1[variable_key].values\n",
    "    img2 = data2[variable_key].values\n",
    "\n",
    "    # Ensure data is 2D\n",
    "    if img1.ndim == 3:\n",
    "        img1 = img1[0]\n",
    "    if img2.ndim == 3:\n",
    "        img2 = img2[0]\n",
    "\n",
    "    # Normalize and convert to 8-bit grayscale\n",
    "    img1 = cv2.normalize(img1, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    img2 = cv2.normalize(img2, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    # Compute DeepFlow\n",
    "    deep_flow = cv2.optflow.createOptFlow_DeepFlow()\n",
    "    flow = deep_flow.calc(img1, img2, None)\n",
    "\n",
    "    # Get latitude and longitude values\n",
    "    latitudes = data1.latitude.values\n",
    "    longitudes = data1.longitude.values\n",
    "\n",
    "    # Calculate consistent distances between consecutive latitudes and longitudes\n",
    "    d_lat_m = haversine(longitudes[0], latitudes[0], longitudes[0], latitudes[1]) * 1000\n",
    "    d_lon_m = haversine(longitudes[0], latitudes[0], longitudes[1], latitudes[0]) * 1000\n",
    "\n",
    "    # Convert pixel flow to real-world distance flow in meters per second\n",
    "    flow_u_mps = flow[..., 0] * (d_lon_m / time_interval)\n",
    "    flow_v_mps = flow[..., 1] * (d_lat_m / time_interval)\n",
    "\n",
    "    return flow_u_mps, flow_v_mps  # Return flow in meters per second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6993696-2642-42cd-b773-579cd3f53b68",
   "metadata": {},
   "source": [
    "### *calculate_farneback*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943161b9-7f7e-4fc8-a735-465af97046e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_farneback(nc_file1, nc_file2, variable_key=\"fai_anomaly\", time_interval=86400):\n",
    "    # Load data\n",
    "    data1 = xr.open_dataset(nc_file1)\n",
    "    data2 = xr.open_dataset(nc_file2)\n",
    "    img1 = data1[variable_key].values\n",
    "    img2 = data2[variable_key].values\n",
    "\n",
    "    # Get the latitude and longitude values from the dataset\n",
    "    latitudes = data1.latitude.values\n",
    "    longitudes = data1.longitude.values\n",
    "\n",
    "    # Calculate consistent distances between consecutive latitudes and longitudes\n",
    "    d_lat_m = haversine(longitudes[0], latitudes[0], longitudes[0], latitudes[1]) * 1000\n",
    "    d_lon_m = haversine(longitudes[0], latitudes[0], longitudes[1], latitudes[0]) * 1000\n",
    "\n",
    "    # Ensure data is 2D\n",
    "    if img1.ndim == 3:\n",
    "        img1 = img1[0]\n",
    "    if img2.ndim == 3:\n",
    "        img2 = img2[0]\n",
    "\n",
    "    # Normalize and convert to 8-bit grayscale\n",
    "    img1 = cv2.normalize(img1, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    img2 = cv2.normalize(img2, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    # Compute Farneback Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(img1, img2, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    # Convert pixel flow to real-world distance flow in meters per second\n",
    "    flow_u_mps = flow[..., 0] * (d_lon_m / time_interval)\n",
    "    flow_v_mps = flow[..., 1] * (d_lat_m / time_interval)\n",
    "\n",
    "    return flow_u_mps, flow_v_mps  # Return flow in meters per second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b5cc7-7ce7-4b56-8f67-d0fec23608b6",
   "metadata": {},
   "source": [
    "### *calculate_lk*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008cd242-4f43-4bdc-96d1-02f0fef1f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lk(nc_file1, nc_file2, variable_key=\"fai_anomaly\"):\n",
    "    # Load data\n",
    "    data1 = xr.open_dataset(nc_file1)\n",
    "    data2 = xr.open_dataset(nc_file2)\n",
    "    img1 = data1[variable_key].values\n",
    "    img2 = data2[variable_key].values\n",
    "\n",
    "    # Compute distances in meters between consecutive latitude and longitude points\n",
    "    latitudes = data1.latitude.values\n",
    "    longitudes = data1.longitude.values\n",
    "    d_lat_km = haversine(longitudes[0], latitudes[0], longitudes[0], latitudes[1])\n",
    "    d_lon_km = haversine(longitudes[0], latitudes[0], longitudes[1], latitudes[0])\n",
    "\n",
    "    # Ensure data is 2D\n",
    "    if img1.ndim > 2:\n",
    "        img1 = img1.squeeze()  # Reduce dimensions if necessary\n",
    "    if img2.ndim > 2:\n",
    "        img2 = img2.squeeze()\n",
    "\n",
    "    # Normalize and convert to 8-bit grayscale\n",
    "    img1 = cv2.normalize(img1.astype(np.float32), None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    img2 = cv2.normalize(img2.astype(np.float32), None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    # Detect good features to track\n",
    "    p0 = cv2.goodFeaturesToTrack(img1, maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "    if p0 is not None:\n",
    "        # Calculate optical flow using Lucas-Kanade method\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(img1, img2, p0, None)\n",
    "        good_new = p1[st == 1] if p1 is not None else np.empty((0, 2))\n",
    "        good_old = p0[st == 1]\n",
    "    else:\n",
    "        good_new = np.empty((0, 2))\n",
    "        good_old = np.empty((0, 2))\n",
    "\n",
    "    if good_new.size > 0:\n",
    "        # Calculate flow in kilometers\n",
    "        flow_u_km = (good_new[:, 0] - good_old[:, 0]) * (d_lon_km / len(longitudes))\n",
    "        flow_v_km = (good_new[:, 1] - good_old[:, 1]) * (d_lat_km / len(latitudes))\n",
    "    else:\n",
    "        flow_u_km = np.array([])\n",
    "        flow_v_km = np.array([])\n",
    "\n",
    "    return flow_u_km, flow_v_km  # Return flow in kilometers per pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfefb79-45b1-435b-86dc-882656bad7ec",
   "metadata": {},
   "source": [
    "### Default Color (Viridis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b2ca8-f525-4714-b5bf-190b96803116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Flow\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '/media/yahia/ballena/ABI/NetCDF/Partition/n = 24/Averages/[14.333333333333334,15.5],[-63.33333333333333,-67.0]/algae_distribution_20220723.nc'\n",
    "    visualize(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad629a-af17-483e-becf-a62e96c7af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow on Antilles\n",
    "if __name__ == \"__main__\":\n",
    "    prev_nc = '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/ABI_Averages/algae_distribution_20220723.nc'\n",
    "    next_nc = '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/ABI_Averages/algae_distribution_20220724.nc'\n",
    "    \n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc)\n",
    "    visualize(prev_nc, variable_key=\"fai_anomaly\", colorbar_label=\"FAI\", title=\"Optical Flow\", flow_vectors=flow_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e517ac-b188-4c23-8fe5-d3bf9b4be7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow on Atlantic\n",
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Averages/algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Averages/algae_distribution_20220724.nc\"\n",
    "    \n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc)\n",
    "    visualize(prev_nc, variable_key=\"fai_anomaly\", colorbar_label=\"FAI\", title=\"Optical Flow\", flow_vectors=flow_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9578d138-7d61-40ea-b905-7ec5e62b91e4",
   "metadata": {},
   "source": [
    "### Binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d3c257-d355-4c18-a78f-3c92b444a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Flow\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '/media/yahia/ballena/ABI/NetCDF/Partition/n = 24/Averages_Binarized/[14.333333333333334,15.5],[-63.33333333333333,-67.0]/Processed_algae_distribution_20220723.nc'\n",
    "    visualize(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5464c757-a7c0-456e-adc6-4dc345f8792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow on Antilles\n",
    "if __name__ == \"__main__\":\n",
    "    prev_nc = '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Processed_ABI_Averages/Processed_algae_distribution_20220723.nc'\n",
    "    next_nc = '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Processed_ABI_Averages/Processed_algae_distribution_20220724.nc'\n",
    "    \n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc)\n",
    "    visualize(prev_nc, variable_key=\"fai_anomaly\", colorbar_label=\"FAI\", title=\"Optical Flow\", flow_vectors=flow_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9599885-a310-40e1-8843-d0af60e536fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow on Atlantic\n",
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Averages_Binarized/Processed_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Averages_Binarized/Processed_algae_distribution_20220724.nc\"\n",
    "    \n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc)\n",
    "    visualize(prev_nc, variable_key=\"fai_anomaly\", colorbar_label=\"FAI\", title=\"Optical Flow\", flow_vectors=flow_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67857bdc-f7f7-447b-9e76-a615c5f7e981",
   "metadata": {},
   "source": [
    "### *save_flow*\n",
    "This function takes as input the path for the NetCDF image and the (already calculated) flow and creates a new NetCDF file with two new variables containing the flow vector components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c461e375-5c6f-418e-80b4-ffd8927c1122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_flow(file_path, output_path=\"output.nc\", lat_range=None, lon_range=None, \n",
    "              flow_vectors=None, flow_vectors_m=None, flow_vectors_f=None, mask_data=False):\n",
    "    \"\"\"\n",
    "    Saves the original NetCDF data and optionally overlays two sets of flow vectors as new data variables into another NetCDF file,\n",
    "    masking the flow data based on 'fai_anomaly' being zero if mask_data is True.\n",
    "    \"\"\"\n",
    "    # Load the NetCDF data\n",
    "    data = xr.open_dataset(file_path)\n",
    "    \n",
    "    # Subset the data based on provided latitude and longitude ranges\n",
    "    if lat_range and 'latitude' in data.coords:\n",
    "        data = data.sel(latitude=slice(*lat_range))\n",
    "    if lon_range and 'longitude' in data.coords:\n",
    "        data = data.sel(longitude=slice(*lon_range))\n",
    "\n",
    "    # Prepare the mask based on 'fai_anomaly' if mask_data is True\n",
    "    mask = data['fai_anomaly'] != 0 if mask_data else None\n",
    "\n",
    "    # Function to prepare flow data\n",
    "    def prepare_flow_data(flow_data, mask, mask_data):\n",
    "        flow_u, flow_v = flow_data\n",
    "        if mask_data:\n",
    "            flow_u = xr.where(mask, flow_u, 0)\n",
    "            flow_v = xr.where(mask, flow_v, 0)\n",
    "        return flow_u, flow_v\n",
    "\n",
    "    # Include the flow vectors as new data variables if provided\n",
    "    if flow_vectors:\n",
    "        flow_u, flow_v = prepare_flow_data(flow_vectors, mask, mask_data)\n",
    "        data['flow_u'] = xr.DataArray(flow_u, dims=(\"latitude\", \"longitude\"), coords={\"latitude\": data.latitude, \"longitude\": data.longitude})\n",
    "        data['flow_v'] = xr.DataArray(flow_v, dims=(\"latitude\", \"longitude\"), coords={\"latitude\": data.latitude, \"longitude\": data.longitude})\n",
    "        \n",
    "    if flow_vectors_m:\n",
    "        flow_u_m, flow_v_m = prepare_flow_data(flow_vectors_m, mask, mask_data)\n",
    "        data['flow_u_m'] = xr.DataArray(flow_u_m, dims=(\"latitude\", \"longitude\"), coords={\"latitude\": data.latitude, \"longitude\": data.longitude})\n",
    "        data['flow_v_m'] = xr.DataArray(flow_v_m, dims=(\"latitude\", \"longitude\"), coords={\"latitude\": data.latitude, \"longitude\": data.longitude})\n",
    "        \n",
    "    if flow_vectors_f:\n",
    "        flow_u_f, flow_v_f = prepare_flow_data(flow_vectors_f, mask, mask_data)\n",
    "        data['flow_u_f'] = xr.DataArray(flow_u_f, dims=(\"latitude\", \"longitude\"), coords={\"latitude\": data.latitude, \"longitude\": data.longitude})\n",
    "        data['flow_v_f'] = xr.DataArray(flow_v_f, dims=(\"latitude\", \"longitude\"), coords={\"latitude\": data.latitude, \"longitude\": data.longitude})\n",
    "\n",
    "    # Save the modified dataset to a new NetCDF file\n",
    "    data.to_netcdf(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d36a7ba-7467-4371-b36b-87d8834f4e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on Antilles\n",
    "if __name__ == \"__main__\":\n",
    "    prev_nc = '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Processed_ABI_Averages/Processed_algae_distribution_20220723.nc'\n",
    "    next_nc = '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Processed_ABI_Averages/Processed_algae_distribution_20220724.nc'\n",
    "    \n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc)\n",
    "    # Example usage\n",
    "    save_flow(prev_nc, output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Antilles_with_flow.nc\", flow_vectors=flow_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d95f8ea-6063-46fb-9d1a-145564f32469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on Atlantic\n",
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Averages_Binarized/Processed_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Averages_Binarized/Processed_algae_distribution_20220724.nc\"\n",
    "    \n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc)\n",
    "    # Example usage\n",
    "    save_flow(prev_nc, 'fai_anomaly', output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Atlantic_with_flow.nc\", flow_vectors=flow_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea307e75-c5d7-46a2-aff6-b489d48a89e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on Filtered Atlantic\n",
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered/Filtered_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered/Filtered_algae_distribution_20220724.nc\"\n",
    "    \n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc)\n",
    "    # Example usage\n",
    "    save_flow(prev_nc, 'fai_anomaly', output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered/Atlantic_with_flow.nc\", flow_vectors=flow_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429b0e8-fdfd-4cf7-a757-e7937cbcfbef",
   "metadata": {},
   "source": [
    "### *visualize_quiver* \n",
    "Takes as input a NetCDF file with the added variables **flow_u** and **flow_v** and uses them to overlay the vectors on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d291ca6-c52d-4f9d-a7dc-49cb1adaa6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_quiver(file_path, variable_key=\"fai_anomaly\", quiver_step=10, quiver_scale=100, save_path=None, mask_data=True):\n",
    "    \"\"\"\n",
    "    Visualizes the optical flow vectors on top of the variable image from a NetCDF file.\n",
    "    Only shows vectors where the data is non-zero if masking is enabled.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the NetCDF file.\n",
    "    - variable_key (str): Key for the variable of interest (typically the image data).\n",
    "    - quiver_step (int): Step size for downsampling the quiver plot to reduce vector density.\n",
    "    - quiver_scale (int): Scaling factor for the vectors to control their size.\n",
    "    - save_path (str, optional): Path to save the figure as a high-resolution PNG image. If None, the image is not saved.\n",
    "    - mask_data (bool, optional): If True, vectors are only displayed where the data is non-zero.\n",
    "    \"\"\"\n",
    "    data = xr.open_dataset(file_path)\n",
    "\n",
    "    # Increase figure size for better resolution in the saved image\n",
    "    fig, ax = plt.subplots(figsize=(25, 20), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    data[variable_key].plot(ax=ax, x='longitude', y='latitude', transform=ccrs.PlateCarree(), cmap='gray', add_colorbar=False)\n",
    "\n",
    "    # Calculate the step size for displaying vectors\n",
    "    skip = (slice(None, None, quiver_step), slice(None, None, quiver_step))\n",
    "    X, Y = np.meshgrid(data.longitude, data.latitude)\n",
    "    U = data['flow_u'].values\n",
    "    V = data['flow_v'].values\n",
    "\n",
    "    if mask_data:\n",
    "        # Mask where the data is zero\n",
    "        mask = data[variable_key].values != 0\n",
    "        masked_X = X[skip][mask[skip]]\n",
    "        masked_Y = Y[skip][mask[skip]]\n",
    "        masked_U = U[skip][mask[skip]]\n",
    "        masked_V = V[skip][mask[skip]]\n",
    "    else:\n",
    "        masked_X = X[skip]\n",
    "        masked_Y = Y[skip]\n",
    "        masked_U = U[skip]\n",
    "        masked_V = V[skip]\n",
    "\n",
    "    # Overlay the flow vectors\n",
    "    ax.quiver(masked_X, masked_Y, masked_U, masked_V, color='red', scale=quiver_scale)\n",
    "\n",
    "    plt.title('Optical Flow on Image')\n",
    "\n",
    "    # Save the figure if a save_path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)  # Close the figure to free up memory\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829aebea-ce3c-4e7f-bf78-d7b0b0c79eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antilles\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_quiver(\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Antilles_with_flow.nc\", 'fai_anomaly', quiver_step=12, quiver_scale=500, mask_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0be51c5-14e9-421b-9115-b6472e8e0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antilles (masked)\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_quiver(\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Antilles_with_flow.nc\", 'fai_anomaly', quiver_step=5, quiver_scale=1000, mask_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f82731-1178-43d0-9226-f68aec4baefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atlantic\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_quiver(\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Atlantic_with_flow.nc\", 'fai_anomaly', quiver_step=45, quiver_scale=3000, save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fc0b64-5658-4392-81fa-34102985be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atlantic\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_quiver(\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Atlantic_with_flow.nc\", 'fai_anomaly', quiver_step=15, quiver_scale=5000, save_path='/home/yahia/Bureau/atlantic_flow_masked', mask_data=True)\n",
    "    # visualize_quiver(\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Atlantic_with_flow.nc\", 'fai_anomaly', quiver_step=10, quiver_scale=5000, save_path=None, mask_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df50c7-8317-47d8-8467-f17c68b50356",
   "metadata": {},
   "source": [
    "### *visualize_flow*\n",
    "Similar to *visualize_quiver*, but uses custom vectors instead of quiver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e9da5e-6733-43c3-b969-0f19832b3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_flow(file_path, variable_key=\"fai_anomaly\", quiver_step=10, line_width=0.5, quiver_scale=1.0, save_path=None, mask_data=True):\n",
    "    data = xr.open_dataset(file_path)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 16), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    data[variable_key].plot(ax=ax, x='longitude', y='latitude', transform=ccrs.PlateCarree(), cmap='gray', add_colorbar=False)\n",
    "\n",
    "    # Generate meshgrid for coordinates and vector components, applying downsampling by quiver_step\n",
    "    X, Y = np.meshgrid(data.longitude[::quiver_step], data.latitude[::quiver_step])\n",
    "    U = data['flow_u'].values[::quiver_step, ::quiver_step]\n",
    "    V = data['flow_v'].values[::quiver_step, ::quiver_step]\n",
    "\n",
    "    if mask_data:\n",
    "        mask = data[variable_key].values[::quiver_step, ::quiver_step] != 0\n",
    "        X, Y, U, V = X[mask], Y[mask], U[mask], V[mask]\n",
    "\n",
    "    # Scale the vectors using quiver_scale\n",
    "    U, V = U * quiver_scale, V * quiver_scale\n",
    "\n",
    "    # Calculate end points of vectors\n",
    "    end_X, end_Y = X + U, Y + V\n",
    "    lines = np.array([[[x, y], [ex, ey]] for x, y, ex, ey in zip(X.flatten(), Y.flatten(), end_X.flatten(), end_Y.flatten())])\n",
    "\n",
    "    # Create a LineCollection from the arrays of line segments\n",
    "    lc = LineCollection(lines, colors='red', linewidths=line_width, transform=ccrs.PlateCarree())\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "    plt.title('Optical Flow on Image')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)  # Close the figure to free up memory\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12be288-0568-4bf9-b2d3-111202b4530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antilles\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_flow(\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Antilles_with_flow.nc\", 'fai_anomaly', quiver_step=12, quiver_scale=0.005, mask_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8718c1e-de63-426d-9286-f0921f7a8ea0",
   "metadata": {},
   "source": [
    "## DeepFlowing Atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073268cf-2a7e-471c-babd-a22b1daa8c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()  # Record the start time\n",
    "\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220724.nc\"\n",
    "    \n",
    "    # Calculate flow vectors for each variable key\n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc, variable_key=\"fai_anomaly\")\n",
    "    flow_vectors_m = calculate_deepflow(prev_nc, next_nc, variable_key=\"masked_land\")\n",
    "    flow_vectors_f = calculate_deepflow(prev_nc, next_nc, variable_key=\"filtered\")\n",
    "    \n",
    "    # Save the flow data\n",
    "    save_flow(prev_nc, output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow.nc\",\n",
    "              flow_vectors=flow_vectors, flow_vectors_f=flow_vectors_f, flow_vectors_m=flow_vectors_m)\n",
    "\n",
    "    end_time = time.time()  # Record the end time after operations are complete\n",
    "    print(f\"Elapsed time: {end_time - start_time:.2f} seconds\")  # Print the elapsed time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6bafa-ea35-4044-9d3a-f96900eb7842",
   "metadata": {},
   "source": [
    "Sequential: \n",
    "- 1st run: 280.08s\n",
    "- 2nd run (with d_lon, d_lat loop): 446.59s (loop 1: 35.51s , loop 2: 28.19s, loop 3: 28.54s)\n",
    "- 3rd run: 224.53s (no loop)\n",
    "Parallel: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e58007-ae33-4e3f-bb15-811f8e1153cb",
   "metadata": {},
   "source": [
    "#### DeepFlow (Masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baf30a2-2e2a-4d03-8a40-8ac26d5a0876",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220724.nc\"\n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc, variable_key=\"fai_anomaly\")\n",
    "    flow_vectors_m = calculate_deepflow(prev_nc, next_nc, variable_key=\"masked_land\")\n",
    "    flow_vectors_f = calculate_deepflow(prev_nc, next_nc, variable_key=\"filtered\")\n",
    "    save_flow(prev_nc, output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow_masked.nc\", flow_vectors=flow_vectors, flow_vectors_f=flow_vectors_f,\n",
    "             flow_vectors_m=flow_vectors_m, mask_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76553a4f-56c9-477e-942e-202a66e44f06",
   "metadata": {},
   "source": [
    "#### Sub-daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02358505-4aae-4f19-ac7c-f5c03bfbccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220724.nc\"\n",
    "    \n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc, variable_key=\"fai_anomaly\", time_interval=14400)\n",
    "    # flow_vectors_m = calculate_deepflow(prev_nc, next_nc, variable_key=\"masked_land\")\n",
    "    # flow_vectors_f = calculate_deepflow(prev_nc, next_nc, variable_key=\"filtered\")\n",
    "    save_flow(prev_nc, output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow_4h.nc\", flow_vectors=flow_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d6e3f5-65fd-4f3f-b12b-499729ef71fe",
   "metadata": {},
   "source": [
    "## Farnebacking Atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718a2b5-d056-4784-aae6-a462dcb64dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220724.nc\"\n",
    "    \n",
    "    flow_vectors = calculate_farneback(prev_nc, next_nc, variable_key=\"fai_anomaly\")\n",
    "    flow_vectors_m = calculate_farneback(prev_nc, next_nc, variable_key=\"masked_land\")\n",
    "    flow_vectors_f = calculate_farneback(prev_nc, next_nc, variable_key=\"filtered\")\n",
    "    save_flow(prev_nc, output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback.nc\", flow_vectors=flow_vectors, flow_vectors_f=flow_vectors_f,\n",
    "             flow_vectors_m=flow_vectors_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc368ea-a22f-4b75-9aa1-34ca3db8ddef",
   "metadata": {},
   "source": [
    "#### Farneback (Masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805beec5-9a3b-469b-a0b1-075abfaa8a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220724.nc\"\n",
    "    flow_vectors = calculate_farneback(prev_nc, next_nc, variable_key=\"fai_anomaly\")\n",
    "    flow_vectors_m = calculate_farneback(prev_nc, next_nc, variable_key=\"masked_land\")\n",
    "    flow_vectors_f = calculate_farneback(prev_nc, next_nc, variable_key=\"filtered\")\n",
    "    save_flow(prev_nc, output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback_masked.nc\", flow_vectors=flow_vectors, flow_vectors_f=flow_vectors_f,\n",
    "             flow_vectors_m=flow_vectors_m, mask_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c800d-a8f2-4558-9086-9d111b9ce7a7",
   "metadata": {},
   "source": [
    "## LKing Atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4f89c-511a-40c8-bfb9-0cf7113c061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220724.nc\"\n",
    "    \n",
    "    flow_vectors = calculate_lk(prev_nc, next_nc, variable_key=\"fai_anomaly\")\n",
    "    flow_vectors_m = calculate_lk(prev_nc, next_nc, variable_key=\"masked_land\")\n",
    "    flow_vectors_f = calculate_lk(prev_nc, next_nc, variable_key=\"filtered\")\n",
    "    save_flow(prev_nc, output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_lk.nc\", flow_vectors=flow_vectors, flow_vectors_f=flow_vectors_f,\n",
    "             flow_vectors_m=flow_vectors_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1806601a-6f93-444f-8bc6-ff9ee5b5a886",
   "metadata": {},
   "source": [
    "## Comparison with Glorys12 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec5e49f-e196-462a-b1e8-768612b9a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # #Calculate flow\n",
    "    # prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220723.nc\"\n",
    "    # next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220724.nc\"\n",
    "    \n",
    "    # flow_u, flow_v = calculate_deepflow(prev_nc, next_nc)\n",
    "    print(flow_u.min(), flow_u.max())\n",
    "    print(np.median(flow_u))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c84a35-b27f-4d51-9286-4c0cc712d75f",
   "metadata": {},
   "source": [
    "### *compare_flows*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49c0af-4424-4a73-b900-b5d0db574a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_flows(my_dataset_path, output_path, glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc', comparison_date='2022-07-23'):\n",
    "    \"\"\"\n",
    "    Compares flow vectors from a custom dataset with ocean currents from the GLORYS12 dataset for a specific date\n",
    "    and saves the differences. The custom dataset is downsampled to match the GLORYS dataset grid.\n",
    "    \n",
    "    Parameters:\n",
    "    - my_dataset_path (str): Path to the custom NetCDF file with flow variables.\n",
    "    - glorys_dataset_path (str): Path to the GLORYS12 ocean currents NetCDF file.\n",
    "    - output_path (str): Path to save the resulting differences as a NetCDF file.\n",
    "    - comparison_date (str): Specific date to extract data for comparison (default '2022-07-23').\n",
    "    \"\"\"\n",
    "    # Load the custom and GLORYS datasets\n",
    "    my_data = xr.open_dataset(my_dataset_path)\n",
    "    glorys_data = xr.open_dataset(glorys_dataset_path)\n",
    "    \n",
    "    # Extract data for the specific date and remove singleton time dimensions\n",
    "    current_u = glorys_data['uo'].sel(time=comparison_date).squeeze()\n",
    "    current_v = glorys_data['vo'].sel(time=comparison_date).squeeze()\n",
    "    \n",
    "    # Optional: Subset GLORYS data to the extent of my_data to ensure proper overlap\n",
    "    current_u = current_u.sel(latitude=slice(my_data.latitude.min(), my_data.latitude.max()),\n",
    "                              longitude=slice(my_data.longitude.min(), my_data.longitude.max()))\n",
    "    current_v = current_v.sel(latitude=slice(my_data.latitude.min(), my_data.latitude.max()),\n",
    "                              longitude=slice(my_data.longitude.min(), my_data.longitude.max()))\n",
    "    \n",
    "    # Downsample my_data to match the GLORYS data grid\n",
    "    my_data_downsampled = my_data.reindex_like(current_u, method='nearest')\n",
    "\n",
    "    # Compute differences for each set of flow variables\n",
    "    d_flow_u = abs(my_data_downsampled['flow_u'] - current_u)\n",
    "    d_flow_v = abs(my_data_downsampled['flow_v'] - current_v)\n",
    "    d_flow_u_m = abs(my_data_downsampled['flow_u_m'] - current_u)\n",
    "    d_flow_v_m = abs(my_data_downsampled['flow_v_m'] - current_v)\n",
    "    d_flow_u_f = abs(my_data_downsampled['flow_u_f'] - current_u)\n",
    "    d_flow_v_f = abs(my_data_downsampled['flow_v_f'] - current_v)\n",
    "    \n",
    "    # Create a new dataset to store the differences\n",
    "    diff_dataset = xr.Dataset({\n",
    "        'd_flow_u': d_flow_u,\n",
    "        'd_flow_v': d_flow_v,\n",
    "        'd_flow_u_m': d_flow_u_m,\n",
    "        'd_flow_v_m': d_flow_v_m,\n",
    "        'd_flow_u_f': d_flow_u_f,\n",
    "        'd_flow_v_f': d_flow_v_f\n",
    "    })\n",
    "    \n",
    "    # Save the differences dataset to a NetCDF file\n",
    "    diff_dataset.to_netcdf(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a832fef-cae7-487d-aa88-c653b8fb6d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFlow\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        output_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/difference_flow.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f74d4e-cee8-4fcf-b8e7-9c4d779df200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Farneback\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        output_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/difference_farneback.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d28d3-b850-4446-9003-1131e069d22d",
   "metadata": {},
   "source": [
    "### *compare_flows_scatter*\n",
    "Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0e38c5-35e5-478b-a79d-47a27edc6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_flows_scatter(my_dataset_path, glorys_dataset_path, comparison_date='2022-07-23', \n",
    "                                     flow_u_name='flow_u', flow_v_name='flow_v'):\n",
    "    # Load datasets\n",
    "    my_data = xr.open_dataset(my_dataset_path)\n",
    "    glorys_data = xr.open_dataset(glorys_dataset_path)\n",
    "\n",
    "    # Select specific date and squeeze out singletons for both u and v components from GLORYS dataset\n",
    "    glorys_u = glorys_data['uo'].sel(time=comparison_date).squeeze()\n",
    "    glorys_v = glorys_data['vo'].sel(time=comparison_date).squeeze()\n",
    "    \n",
    "    # Extract the custom flow components using the provided names\n",
    "    my_flow_u = my_data[flow_u_name]\n",
    "    my_flow_v = my_data[flow_v_name]\n",
    "\n",
    "    # Output minimum and maximum values for diagnostics\n",
    "    print(f\"Min/Max {flow_u_name}: {my_flow_u.min().data}, {my_flow_u.max().data}\")\n",
    "    print(f\"Min/Max {flow_v_name}: {my_flow_v.min().data}, {my_flow_v.max().data}\")\n",
    "\n",
    "    # Restrict both datasets to the common spatial extent (for safety)\n",
    "    common_lat_min = max(min(my_flow_u.latitude), min(glorys_u.latitude))\n",
    "    common_lat_max = min(max(my_flow_u.latitude), max(glorys_u.latitude))\n",
    "    common_lon_min = max(min(my_flow_u.longitude), min(glorys_u.longitude))\n",
    "    common_lon_max = min(max(my_flow_u.longitude), max(glorys_u.longitude))\n",
    "\n",
    "    # Restrict both datasets to the common spatial extent\n",
    "    my_flow_u = my_flow_u.sel(latitude=slice(common_lat_min, common_lat_max), longitude=slice(common_lon_min, common_lon_max))\n",
    "    my_flow_v = my_flow_v.sel(latitude=slice(common_lat_min, common_lat_max), longitude=slice(common_lon_min, common_lon_max))\n",
    "    glorys_u = glorys_u.sel(latitude=slice(common_lat_min, common_lat_max), longitude=slice(common_lon_min, common_lon_max))\n",
    "    glorys_v = glorys_v.sel(latitude=slice(common_lat_min, common_lat_max), longitude=slice(common_lon_min, common_lon_max))\n",
    "    \n",
    "    # Downsample my dataset to match GLORYS grid using reindex_like for both u and v components\n",
    "    my_flow_u_downsampled = my_flow_u.reindex_like(glorys_u, method='nearest')\n",
    "    my_flow_v_downsampled = my_flow_v.reindex_like(glorys_v, method='nearest')\n",
    "\n",
    "    # Flatten the data to 1D arrays for statistical analysis\n",
    "    glorys_u_flat = glorys_u.values.flatten()\n",
    "    my_flow_u_flat = my_flow_u_downsampled.values.flatten()\n",
    "    glorys_v_flat = glorys_v.values.flatten()\n",
    "    my_flow_v_flat = my_flow_v_downsampled.values.flatten()\n",
    "\n",
    "    # Clean non-finite values from data arrays\n",
    "    valid_indices_u = np.isfinite(glorys_u_flat) & np.isfinite(my_flow_u_flat)\n",
    "    valid_indices_v = np.isfinite(glorys_v_flat) & np.isfinite(my_flow_v_flat)\n",
    "    glorys_u_flat = glorys_u_flat[valid_indices_u]\n",
    "    my_flow_u_flat = my_flow_u_flat[valid_indices_u]\n",
    "    glorys_v_flat = glorys_v_flat[valid_indices_v]\n",
    "    my_flow_v_flat = my_flow_v_flat[valid_indices_v]\n",
    "\n",
    "    # Calculate correlation for both u and v components\n",
    "    if len(glorys_u_flat) == len(my_flow_u_flat) and len(glorys_v_flat) == len(my_flow_v_flat):\n",
    "        correlation_u, _ = scipy.stats.pearsonr(glorys_u_flat, my_flow_u_flat)\n",
    "        correlation_v, _ = scipy.stats.pearsonr(glorys_v_flat, my_flow_v_flat)\n",
    "        print(\"Correlation coefficient for u:\", correlation_u)\n",
    "        print(\"Correlation coefficient for v:\", correlation_v)\n",
    "    else:\n",
    "        print(\"Data arrays do not match in size or are empty after cleaning.\")\n",
    "\n",
    "    # Plotting for u component\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(glorys_u_flat, my_flow_u_flat, alpha=0.5)\n",
    "    plt.title(f'Scatter Plot of GLORYS uo vs. {flow_u_name}')\n",
    "    plt.xlabel('GLORYS uo (m/s)')\n",
    "    plt.ylabel(f'{flow_u_name} (m/s)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting for v component\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(glorys_v_flat, my_flow_v_flat, alpha=0.5)\n",
    "    plt.title(f'Scatter Plot of GLORYS vo vs. {flow_v_name}')\n",
    "    plt.xlabel('GLORYS vo (m/s)')\n",
    "    plt.ylabel(f'{flow_v_name} (m/s)')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08071795-05d9-455f-ae51-b595b6ffc0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFlow\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a7da3-8be8-40f5-8910-e83accde2a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked DeepFlow\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_m\", flow_v_name=\"flow_v_m\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084842c-ab11-4369-96ea-4bb57b306c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered DeepFlow\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_f\", flow_v_name=\"flow_v_f\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8182d934-79b7-4caf-ac39-3c1700e35539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Farneback\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e5ca83-ede7-433a-b20d-5f548c1a7b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Farneback\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_m\", flow_v_name=\"flow_v_m\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c935c3e-406f-4541-afab-63d7f75fa87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered Farneback\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_f\", flow_v_name=\"flow_v_f\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f34227-d563-4c3e-a059-a28e3cc802d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
