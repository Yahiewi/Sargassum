{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb1f18c3-8fa2-4a29-ae48-5ce04c80ceac",
   "metadata": {},
   "source": [
    "# Flow NetCDF\n",
    "Since we're now using NetCDF files instead of png, we will need to modify a lot of the functions we used to visualize our result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e488cd2-646e-4533-bfdc-206cf1c4345b",
   "metadata": {},
   "source": [
    "## Importing necessary libraries and notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7fa7fa1-0e84-490b-97fd-75ca1c792c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import io\n",
    "import os\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import netCDF4 as nc\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import ticker\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from matplotlib.collections import LineCollection\n",
    "from IPython.display import Image, display, clear_output\n",
    "from PIL import Image as PILImage\n",
    "import concurrent.futures\n",
    "\n",
    "# Import the other notebooks without running their cells\n",
    "from ii_Data_Manipulation import visualize_4\n",
    "from vii_Flow_Analysis import haversine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d498027-901f-4cbe-8f6d-c9877e6daf99",
   "metadata": {},
   "source": [
    "## NetCDF Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121dcc72-5070-43e0-b076-fb489bf877b5",
   "metadata": {},
   "source": [
    "### *visualize*\n",
    "A generalization of the visualize function in the very first notebook with the added option of plotting flow vectors and choosing the step (density) and scale of the flow vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82008ea3-7c07-4d0c-bbc4-41ad9078ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(file_path, variable_key=\"fai_anomaly\", lat_range=None, lon_range=None, color=\"viridis\", colorbar_label=\"\", title=\"\", flow_vectors=None, quiver_step=None, quiver_scale=None):\n",
    "    \"\"\"\n",
    "    Visualizes the NetCDF data and optionally overlays flow vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the NetCDF file.\n",
    "    - variable_key: Key for the variable of interest in the NetCDF dataset.\n",
    "    - lat_range: Tuple of (min, max) latitude to subset the data.\n",
    "    - lon_range: Tuple of (min, max) longitude to subset the data.\n",
    "    - color: Color map for the plot.\n",
    "    - colorbar_label: Label for the color bar.\n",
    "    - title: Title of the plot.\n",
    "    - flow_vectors: Optional tuple of flow vector components (flow_u, flow_v).\n",
    "    - quiver_step: Sampling step for displaying quiver arrows, controls density.\n",
    "    - quiver_scale: Scaling factor for quiver arrows, controls size.\n",
    "    \"\"\"\n",
    "    # Load the netCDF data\n",
    "    data = xr.open_dataset(file_path)\n",
    "    \n",
    "    # If ranges are specified, apply them to select the desired subset\n",
    "    if lat_range and 'latitude' in data.coords:\n",
    "        data = data.sel(latitude=slice(*lat_range))\n",
    "    if lon_range and 'longitude' in data.coords:\n",
    "        data = data.sel(longitude=slice(*lon_range))\n",
    "\n",
    "    # Set up a plot with geographic projections\n",
    "    fig, ax = plt.subplots(figsize=(12, 10), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    \n",
    "    # Extract relevant data \n",
    "    index_data = data[variable_key]\n",
    "\n",
    "    # Plot the data\n",
    "    im = index_data.plot(ax=ax, x='longitude', y='latitude', transform=ccrs.PlateCarree(),\n",
    "                         cmap=color, add_colorbar=True, extend='both', cbar_kwargs={'shrink': 0.35})\n",
    "\n",
    "    # Add color bar details\n",
    "    im.colorbar.set_label(colorbar_label)\n",
    "\n",
    "    # Customize the map with coastlines and features\n",
    "    ax.coastlines(resolution='10m', color='black')\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
    "    gl = ax.gridlines(draw_labels=True, linewidth=1, color='gray', alpha=0.5, linestyle='--')\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "\n",
    "    # Plot flow vectors if provided\n",
    "    if flow_vectors:\n",
    "        # Automatically determine quiver_step and quiver_scale if not provided\n",
    "        if quiver_step is None:\n",
    "            quiver_step = max(1, int(len(data.latitude) / 20))  # Sample about 20 arrows along the latitude\n",
    "        if quiver_scale is None:\n",
    "            quiver_scale = max(1, int(len(data.latitude) / 2))  # Scale according to number of latitude points\n",
    "\n",
    "        # Create a meshgrid for the flow vectors that matches the data subset\n",
    "        Y, X = np.meshgrid(data.latitude, data.longitude, indexing='ij')\n",
    "        # Apply the step for vector density and scale for vector size\n",
    "        ax.quiver(X[::quiver_step, ::quiver_step], Y[::quiver_step, ::quiver_step], flow_vectors[0][::quiver_step, ::quiver_step], flow_vectors[1][::quiver_step, ::quiver_step], color='red', scale=quiver_scale)\n",
    "\n",
    "    # Show the plot with title\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de105aa-7561-4818-9024-6e72f225d23c",
   "metadata": {},
   "source": [
    "### *calculate_deepflow*\n",
    "A function to calculate and return deepflow using as input two NetCDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d782fed-1dc1-4a94-8fbd-18ac03e6d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_deepflow(nc_file1, nc_file2, variable_key=\"fai_anomaly\", time_interval=86400):\n",
    "    # Load data\n",
    "    data1 = xr.open_dataset(nc_file1)\n",
    "    data2 = xr.open_dataset(nc_file2)\n",
    "    img1 = data1[variable_key].values\n",
    "    img2 = data2[variable_key].values\n",
    "\n",
    "    # Ensure data is 2D\n",
    "    if img1.ndim == 3:\n",
    "        img1 = img1[0]\n",
    "    if img2.ndim == 3:\n",
    "        img2 = img2[0]\n",
    "\n",
    "    # Normalize and convert to 8-bit grayscale\n",
    "    img1 = cv2.normalize(img1, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    img2 = cv2.normalize(img2, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    # Compute DeepFlow\n",
    "    deep_flow = cv2.optflow.createOptFlow_DeepFlow()\n",
    "    flow = deep_flow.calc(img1, img2, None)\n",
    "\n",
    "    # Get latitude and longitude values\n",
    "    latitudes = data1.latitude.values\n",
    "    longitudes = data1.longitude.values\n",
    "\n",
    "    # Calculate consistent distances between consecutive latitudes and longitudes\n",
    "    d_lat_m = haversine(longitudes[0], latitudes[0], longitudes[0], latitudes[1]) * 1000\n",
    "    d_lon_m = haversine(longitudes[0], latitudes[0], longitudes[1], latitudes[0]) * 1000\n",
    "\n",
    "    # Convert pixel flow to real-world distance flow in meters per second\n",
    "    flow_u_mps = flow[..., 0] * (d_lon_m / time_interval)\n",
    "    flow_v_mps = flow[..., 1] * (d_lat_m / time_interval)\n",
    "\n",
    "    return flow_u_mps, flow_v_mps  # Return flow in meters per second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6993696-2642-42cd-b773-579cd3f53b68",
   "metadata": {},
   "source": [
    "### *calculate_farneback*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943161b9-7f7e-4fc8-a735-465af97046e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_farneback(nc_file1, nc_file2, variable_key=\"fai_anomaly\", time_interval=86400):\n",
    "    # Load data\n",
    "    data1 = xr.open_dataset(nc_file1)\n",
    "    data2 = xr.open_dataset(nc_file2)\n",
    "    img1 = data1[variable_key].values\n",
    "    img2 = data2[variable_key].values\n",
    "\n",
    "    # Get the latitude and longitude values from the dataset\n",
    "    latitudes = data1.latitude.values\n",
    "    longitudes = data1.longitude.values\n",
    "\n",
    "    # Calculate consistent distances between consecutive latitudes and longitudes\n",
    "    d_lat_m = haversine(longitudes[0], latitudes[0], longitudes[0], latitudes[1]) * 1000\n",
    "    d_lon_m = haversine(longitudes[0], latitudes[0], longitudes[1], latitudes[0]) * 1000\n",
    "\n",
    "    # Ensure data is 2D\n",
    "    if img1.ndim == 3:\n",
    "        img1 = img1[0]\n",
    "    if img2.ndim == 3:\n",
    "        img2 = img2[0]\n",
    "\n",
    "    # Normalize and convert to 8-bit grayscale\n",
    "    img1 = cv2.normalize(img1, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    img2 = cv2.normalize(img2, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    # Compute Farneback Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(img1, img2, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    # Convert pixel flow to real-world distance flow in meters per second\n",
    "    flow_u_mps = flow[..., 0] * (d_lon_m / time_interval)\n",
    "    flow_v_mps = flow[..., 1] * (d_lat_m / time_interval)\n",
    "\n",
    "    return flow_u_mps, flow_v_mps  # Return flow in meters per second"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b5cc7-7ce7-4b56-8f67-d0fec23608b6",
   "metadata": {},
   "source": [
    "### *calculate_lk*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008cd242-4f43-4bdc-96d1-02f0fef1f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lk(nc_file1, nc_file2, variable_key=\"fai_anomaly\"):\n",
    "    # Load data\n",
    "    data1 = xr.open_dataset(nc_file1)\n",
    "    data2 = xr.open_dataset(nc_file2)\n",
    "    img1 = data1[variable_key].values\n",
    "    img2 = data2[variable_key].values\n",
    "\n",
    "    # Compute distances in meters between consecutive latitude and longitude points\n",
    "    latitudes = data1.latitude.values\n",
    "    longitudes = data1.longitude.values\n",
    "    d_lat_km = haversine(longitudes[0], latitudes[0], longitudes[0], latitudes[1])\n",
    "    d_lon_km = haversine(longitudes[0], latitudes[0], longitudes[1], latitudes[0])\n",
    "\n",
    "    # Ensure data is 2D\n",
    "    if img1.ndim > 2:\n",
    "        img1 = img1.squeeze()  # Reduce dimensions if necessary\n",
    "    if img2.ndim > 2:\n",
    "        img2 = img2.squeeze()\n",
    "\n",
    "    # Normalize and convert to 8-bit grayscale\n",
    "    img1 = cv2.normalize(img1.astype(np.float32), None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    img2 = cv2.normalize(img2.astype(np.float32), None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    # Detect good features to track\n",
    "    p0 = cv2.goodFeaturesToTrack(img1, maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "    if p0 is not None:\n",
    "        # Calculate optical flow using Lucas-Kanade method\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(img1, img2, p0, None)\n",
    "        good_new = p1[st == 1] if p1 is not None else np.empty((0, 2))\n",
    "        good_old = p0[st == 1]\n",
    "    else:\n",
    "        good_new = np.empty((0, 2))\n",
    "        good_old = np.empty((0, 2))\n",
    "\n",
    "    if good_new.size > 0:\n",
    "        # Calculate flow in kilometers\n",
    "        flow_u_km = (good_new[:, 0] - good_old[:, 0]) * (d_lon_km / len(longitudes))\n",
    "        flow_v_km = (good_new[:, 1] - good_old[:, 1]) * (d_lat_km / len(latitudes))\n",
    "    else:\n",
    "        flow_u_km = np.array([])\n",
    "        flow_v_km = np.array([])\n",
    "\n",
    "    return flow_u_km, flow_v_km  # Return flow in kilometers per pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3093f1-75ac-4d70-be61-1eebbf2b210c",
   "metadata": {},
   "source": [
    "### *calculate_block_matching*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0a42690-6a92-4dbe-915d-10de3012ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_block_matching(nc_file1, nc_file2, variable_key=\"fai_anomaly\", time_interval=86400, block_size=(16, 16), search_area=(32, 32)):\n",
    "    \"\"\"\n",
    "    Calculate flow using block matching between two images from NetCDF files.\n",
    "\n",
    "    Parameters:\n",
    "    - nc_file1: Path to the first NetCDF file.\n",
    "    - nc_file2: Path to the second NetCDF file.\n",
    "    - variable_key: Key to extract the relevant variable from NetCDF files.\n",
    "    - time_interval: Time interval in seconds between the two images.\n",
    "    - block_size: Size of each block (height, width).\n",
    "    - search_area: Size of the area to search for a match around the block's original position.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data1 = xr.open_dataset(nc_file1)\n",
    "    data2 = xr.open_dataset(nc_file2)\n",
    "    img1 = data1[variable_key].values\n",
    "    img2 = data2[variable_key].values\n",
    "\n",
    "    # Ensure data is 2D\n",
    "    if img1.ndim == 3:\n",
    "        img1 = img1[0]\n",
    "    if img2.ndim == 3:\n",
    "        img2 = img2[0]\n",
    "\n",
    "    # Normalize and convert to 8-bit grayscale\n",
    "    img1 = cv2.normalize(img1, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    img2 = cv2.normalize(img2, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    # Flow vectors\n",
    "    flow_u = np.zeros_like(img1, dtype=np.float32)\n",
    "    flow_v = np.zeros_like(img1, dtype=np.float32)\n",
    "\n",
    "    # Block matching algorithm\n",
    "    for y in range(0, img1.shape[0], block_size[1]):\n",
    "        for x in range(0, img1.shape[1], block_size[0]):\n",
    "            block = img1[y:y+block_size[1], x:x+block_size[0]]\n",
    "            # Define search area in img2\n",
    "            search_start_x = max(x - search_area[0]//2, 0)\n",
    "            search_end_x = min(x + search_area[0]//2 + block_size[0], img2.shape[1])\n",
    "            search_start_y = max(y - search_area[1]//2, 0)\n",
    "            search_end_y = min(y + search_area[1]//2 + block_size[1], img2.shape[0])\n",
    "\n",
    "            search_region = img2[search_start_y:search_end_y, search_start_x:search_end_x]\n",
    "\n",
    "            # Apply template matching\n",
    "            result = cv2.matchTemplate(search_region, block, cv2.TM_SQDIFF_NORMED)\n",
    "            min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "            best_match_top_left = min_loc\n",
    "            best_match_center = (best_match_top_left[0] + block_size[0] // 2, best_match_top_left[1] + block_size[1] // 2)\n",
    "\n",
    "            # Calculate flow vectors\n",
    "            flow_u[y:y+block_size[1], x:x+block_size[0]] = best_match_center[0] + search_start_x - x - block_size[0] // 2\n",
    "            flow_v[y:y+block_size[1], x:x+block_size[0]] = best_match_center[1] + search_start_y - y - block_size[1] // 2\n",
    "\n",
    "    # Normalize flow vectors to meters per second\n",
    "    latitudes = data1.latitude.values\n",
    "    longitudes = data1.longitude.values\n",
    "    d_lat_m = haversine(longitudes[0], latitudes[0], longitudes[0], latitudes[1]) * 1000\n",
    "    d_lon_m = haversine(longitudes[0], latitudes[0], longitudes[1], latitudes[0]) * 1000\n",
    "\n",
    "    flow_u_mps = flow_u * (d_lon_m / time_interval)\n",
    "    flow_v_mps = flow_v * (d_lat_m / time_interval)\n",
    "\n",
    "    return flow_u_mps, flow_v_mps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfefb79-45b1-435b-86dc-882656bad7ec",
   "metadata": {},
   "source": [
    "### Default Color (Viridis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b2ca8-f525-4714-b5bf-190b96803116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Flow\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '/media/yahia/ballena/ABI/NetCDF/Partition/n = 24/Averages/[14.333333333333334,15.5],[-63.33333333333333,-67.0]/algae_distribution_20220723.nc'\n",
    "    visualize(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad629a-af17-483e-becf-a62e96c7af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow on Antilles\n",
    "if __name__ == \"__main__\":\n",
    "    prev_nc = '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/ABI_Averages/algae_distribution_20220723.nc'\n",
    "    next_nc = '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/ABI_Averages/algae_distribution_20220724.nc'\n",
    "    \n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc)\n",
    "    visualize(prev_nc, variable_key=\"fai_anomaly\", colorbar_label=\"FAI\", title=\"Optical Flow\", flow_vectors=flow_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e517ac-b188-4c23-8fe5-d3bf9b4be7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow on Atlantic\n",
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Averages/algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Averages/algae_distribution_20220724.nc\"\n",
    "    \n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc)\n",
    "    visualize(prev_nc, variable_key=\"fai_anomaly\", colorbar_label=\"FAI\", title=\"Optical Flow\", flow_vectors=flow_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9578d138-7d61-40ea-b905-7ec5e62b91e4",
   "metadata": {},
   "source": [
    "### Binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d3c257-d355-4c18-a78f-3c92b444a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Flow\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '/media/yahia/ballena/ABI/NetCDF/Partition/n = 24/Averages_Binarized/[14.333333333333334,15.5],[-63.33333333333333,-67.0]/Processed_algae_distribution_20220723.nc'\n",
    "    visualize(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5464c757-a7c0-456e-adc6-4dc345f8792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow on Antilles\n",
    "if __name__ == \"__main__\":\n",
    "    prev_nc = '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Processed_ABI_Averages/Processed_algae_distribution_20220723.nc'\n",
    "    next_nc = '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Processed_ABI_Averages/Processed_algae_distribution_20220724.nc'\n",
    "    \n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc)\n",
    "    visualize(prev_nc, variable_key=\"fai_anomaly\", colorbar_label=\"FAI\", title=\"Optical Flow\", flow_vectors=flow_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9599885-a310-40e1-8843-d0af60e536fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow on Atlantic\n",
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Averages_Binarized/Processed_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Averages_Binarized/Processed_algae_distribution_20220724.nc\"\n",
    "    \n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc)\n",
    "    visualize(prev_nc, variable_key=\"fai_anomaly\", colorbar_label=\"FAI\", title=\"Optical Flow\", flow_vectors=flow_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67857bdc-f7f7-447b-9e76-a615c5f7e981",
   "metadata": {},
   "source": [
    "### *save_flow*\n",
    "This function takes as input the path for the NetCDF image and the (already calculated) flow and creates a new NetCDF file with two new variables containing the flow vector components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c461e375-5c6f-418e-80b4-ffd8927c1122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_flow(file_path, output_path=\"output.nc\", lat_range=None, lon_range=None, \n",
    "              flow_vectors=None, flow_vectors_m=None, flow_vectors_f=None, mask_data=False):\n",
    "    \"\"\"\n",
    "    Saves the original NetCDF data and optionally overlays two sets of flow vectors as new data variables into another NetCDF file,\n",
    "    masking the flow data based on 'fai_anomaly' being zero if mask_data is True.\n",
    "    \"\"\"\n",
    "    # Load the NetCDF data\n",
    "    data = xr.open_dataset(file_path)\n",
    "    \n",
    "    # Subset the data based on provided latitude and longitude ranges\n",
    "    if lat_range and 'latitude' in data.coords:\n",
    "        data = data.sel(latitude=slice(*lat_range))\n",
    "    if lon_range and 'longitude' in data.coords:\n",
    "        data = data.sel(longitude=slice(*lon_range))\n",
    "\n",
    "    # Prepare the mask based on 'fai_anomaly' if mask_data is True\n",
    "    mask = data['fai_anomaly'] != 0 if mask_data else None\n",
    "\n",
    "    # Function to prepare flow data\n",
    "    def prepare_flow_data(flow_data, mask, mask_data):\n",
    "        flow_u, flow_v = flow_data\n",
    "        if mask_data:\n",
    "            flow_u = xr.where(mask, flow_u, 0)\n",
    "            flow_v = xr.where(mask, flow_v, 0)\n",
    "        return flow_u, flow_v\n",
    "\n",
    "    # Include the flow vectors as new data variables if provided\n",
    "    if flow_vectors:\n",
    "        flow_u, flow_v = prepare_flow_data(flow_vectors, mask, mask_data)\n",
    "        data['flow_u'] = xr.DataArray(flow_u, dims=(\"latitude\", \"longitude\"), coords={\"latitude\": data.latitude, \"longitude\": data.longitude})\n",
    "        data['flow_v'] = xr.DataArray(flow_v, dims=(\"latitude\", \"longitude\"), coords={\"latitude\": data.latitude, \"longitude\": data.longitude})\n",
    "        \n",
    "    if flow_vectors_m:\n",
    "        flow_u_m, flow_v_m = prepare_flow_data(flow_vectors_m, mask, mask_data)\n",
    "        data['flow_u_m'] = xr.DataArray(flow_u_m, dims=(\"latitude\", \"longitude\"), coords={\"latitude\": data.latitude, \"longitude\": data.longitude})\n",
    "        data['flow_v_m'] = xr.DataArray(flow_v_m, dims=(\"latitude\", \"longitude\"), coords={\"latitude\": data.latitude, \"longitude\": data.longitude})\n",
    "        \n",
    "    if flow_vectors_f:\n",
    "        flow_u_f, flow_v_f = prepare_flow_data(flow_vectors_f, mask, mask_data)\n",
    "        data['flow_u_f'] = xr.DataArray(flow_u_f, dims=(\"latitude\", \"longitude\"), coords={\"latitude\": data.latitude, \"longitude\": data.longitude})\n",
    "        data['flow_v_f'] = xr.DataArray(flow_v_f, dims=(\"latitude\", \"longitude\"), coords={\"latitude\": data.latitude, \"longitude\": data.longitude})\n",
    "\n",
    "    # Save the modified dataset to a new NetCDF file\n",
    "    data.to_netcdf(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d36a7ba-7467-4371-b36b-87d8834f4e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on Antilles\n",
    "if __name__ == \"__main__\":\n",
    "    prev_nc = '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Processed_ABI_Averages/Processed_algae_distribution_20220723.nc'\n",
    "    next_nc = '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Processed_ABI_Averages/Processed_algae_distribution_20220724.nc'\n",
    "    \n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc)\n",
    "    # Example usage\n",
    "    save_flow(prev_nc, output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Antilles_with_flow.nc\", flow_vectors=flow_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d95f8ea-6063-46fb-9d1a-145564f32469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on Atlantic\n",
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Averages_Binarized/Processed_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Averages_Binarized/Processed_algae_distribution_20220724.nc\"\n",
    "    \n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc)\n",
    "    # Example usage\n",
    "    save_flow(prev_nc, 'fai_anomaly', output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Atlantic_with_flow.nc\", flow_vectors=flow_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea307e75-c5d7-46a2-aff6-b489d48a89e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on Filtered Atlantic\n",
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered/Filtered_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered/Filtered_algae_distribution_20220724.nc\"\n",
    "    \n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc)\n",
    "    # Example usage\n",
    "    save_flow(prev_nc, 'fai_anomaly', output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered/Atlantic_with_flow.nc\", flow_vectors=flow_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3429b0e8-fdfd-4cf7-a757-e7937cbcfbef",
   "metadata": {},
   "source": [
    "### *visualize_quiver* \n",
    "Takes as input a NetCDF file with the added variables **flow_u** and **flow_v** and uses them to overlay the vectors on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d291ca6-c52d-4f9d-a7dc-49cb1adaa6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_quiver(file_path, variable_key=\"fai_anomaly\", quiver_step=10, quiver_scale=100, save_path=None, mask_data=True):\n",
    "    \"\"\"\n",
    "    Visualizes the optical flow vectors on top of the variable image from a NetCDF file.\n",
    "    Only shows vectors where the data is non-zero if masking is enabled.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the NetCDF file.\n",
    "    - variable_key (str): Key for the variable of interest (typically the image data).\n",
    "    - quiver_step (int): Step size for downsampling the quiver plot to reduce vector density.\n",
    "    - quiver_scale (int): Scaling factor for the vectors to control their size.\n",
    "    - save_path (str, optional): Path to save the figure as a high-resolution PNG image. If None, the image is not saved.\n",
    "    - mask_data (bool, optional): If True, vectors are only displayed where the data is non-zero.\n",
    "    \"\"\"\n",
    "    data = xr.open_dataset(file_path)\n",
    "\n",
    "    # Increase figure size for better resolution in the saved image\n",
    "    fig, ax = plt.subplots(figsize=(25, 20), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    data[variable_key].plot(ax=ax, x='longitude', y='latitude', transform=ccrs.PlateCarree(), cmap='gray', add_colorbar=False)\n",
    "\n",
    "    # Calculate the step size for displaying vectors\n",
    "    skip = (slice(None, None, quiver_step), slice(None, None, quiver_step))\n",
    "    X, Y = np.meshgrid(data.longitude, data.latitude)\n",
    "    U = data['flow_u'].values\n",
    "    V = data['flow_v'].values\n",
    "\n",
    "    if mask_data:\n",
    "        # Mask where the data is zero\n",
    "        mask = data[variable_key].values != 0\n",
    "        masked_X = X[skip][mask[skip]]\n",
    "        masked_Y = Y[skip][mask[skip]]\n",
    "        masked_U = U[skip][mask[skip]]\n",
    "        masked_V = V[skip][mask[skip]]\n",
    "    else:\n",
    "        masked_X = X[skip]\n",
    "        masked_Y = Y[skip]\n",
    "        masked_U = U[skip]\n",
    "        masked_V = V[skip]\n",
    "\n",
    "    # Overlay the flow vectors\n",
    "    ax.quiver(masked_X, masked_Y, masked_U, masked_V, color='red', scale=quiver_scale)\n",
    "\n",
    "    plt.title('Optical Flow on Image')\n",
    "\n",
    "    # Save the figure if a save_path is provided\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)  # Close the figure to free up memory\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829aebea-ce3c-4e7f-bf78-d7b0b0c79eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antilles\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_quiver(\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Antilles_with_flow.nc\", 'fai_anomaly', quiver_step=12, quiver_scale=500, mask_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0be51c5-14e9-421b-9115-b6472e8e0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antilles (masked)\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_quiver(\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Antilles_with_flow.nc\", 'fai_anomaly', quiver_step=5, quiver_scale=1000, mask_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f82731-1178-43d0-9226-f68aec4baefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atlantic\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_quiver(\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Atlantic_with_flow.nc\", 'fai_anomaly', quiver_step=45, quiver_scale=3000, save_path=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fc0b64-5658-4392-81fa-34102985be80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atlantic\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_quiver(\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Atlantic_with_flow.nc\", 'fai_anomaly', quiver_step=15, quiver_scale=5000, save_path='/home/yahia/Bureau/atlantic_flow_masked', mask_data=True)\n",
    "    # visualize_quiver(\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Atlantic_with_flow.nc\", 'fai_anomaly', quiver_step=10, quiver_scale=5000, save_path=None, mask_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df50c7-8317-47d8-8467-f17c68b50356",
   "metadata": {},
   "source": [
    "### *visualize_flow*\n",
    "Similar to *visualize_quiver*, but uses custom vectors instead of quiver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e9da5e-6733-43c3-b969-0f19832b3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_flow(file_path, variable_key=\"fai_anomaly\", quiver_step=10, line_width=0.5, quiver_scale=1.0, save_path=None, mask_data=True):\n",
    "    data = xr.open_dataset(file_path)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 16), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    data[variable_key].plot(ax=ax, x='longitude', y='latitude', transform=ccrs.PlateCarree(), cmap='gray', add_colorbar=False)\n",
    "\n",
    "    # Generate meshgrid for coordinates and vector components, applying downsampling by quiver_step\n",
    "    X, Y = np.meshgrid(data.longitude[::quiver_step], data.latitude[::quiver_step])\n",
    "    U = data['flow_u'].values[::quiver_step, ::quiver_step]\n",
    "    V = data['flow_v'].values[::quiver_step, ::quiver_step]\n",
    "\n",
    "    if mask_data:\n",
    "        mask = data[variable_key].values[::quiver_step, ::quiver_step] != 0\n",
    "        X, Y, U, V = X[mask], Y[mask], U[mask], V[mask]\n",
    "\n",
    "    # Scale the vectors using quiver_scale\n",
    "    U, V = U * quiver_scale, V * quiver_scale\n",
    "\n",
    "    # Calculate end points of vectors\n",
    "    end_X, end_Y = X + U, Y + V\n",
    "    lines = np.array([[[x, y], [ex, ey]] for x, y, ex, ey in zip(X.flatten(), Y.flatten(), end_X.flatten(), end_Y.flatten())])\n",
    "\n",
    "    # Create a LineCollection from the arrays of line segments\n",
    "    lc = LineCollection(lines, colors='red', linewidths=line_width, transform=ccrs.PlateCarree())\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "    plt.title('Optical Flow on Image')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)  # Close the figure to free up memory\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12be288-0568-4bf9-b2d3-111202b4530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antilles\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_flow(\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Antilles_with_flow.nc\", 'fai_anomaly', quiver_step=12, quiver_scale=0.005, mask_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8718c1e-de63-426d-9286-f0921f7a8ea0",
   "metadata": {},
   "source": [
    "## DeepFlowing Atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073268cf-2a7e-471c-babd-a22b1daa8c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()  # Record the start time\n",
    "\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220724.nc\"\n",
    "    \n",
    "    # Calculate flow vectors for each variable key\n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc, variable_key=\"fai_anomaly\")\n",
    "    flow_vectors_m = calculate_deepflow(prev_nc, next_nc, variable_key=\"masked_land\")\n",
    "    flow_vectors_f = calculate_deepflow(prev_nc, next_nc, variable_key=\"filtered\")\n",
    "    \n",
    "    # Save the flow data\n",
    "    save_flow(prev_nc, output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow.nc\",\n",
    "              flow_vectors=flow_vectors, flow_vectors_f=flow_vectors_f, flow_vectors_m=flow_vectors_m)\n",
    "\n",
    "    end_time = time.time()  # Record the end time after operations are complete\n",
    "    print(f\"Elapsed time: {end_time - start_time:.2f} seconds\")  # Print the elapsed time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6bafa-ea35-4044-9d3a-f96900eb7842",
   "metadata": {},
   "source": [
    "Sequential: \n",
    "- 1st run: 280.08s\n",
    "- 2nd run (with d_lon, d_lat loop): 446.59s (loop 1: 35.51s , loop 2: 28.19s, loop 3: 28.54s)\n",
    "- 3rd run: 224.53s (no loop)\n",
    "Parallel: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e58007-ae33-4e3f-bb15-811f8e1153cb",
   "metadata": {},
   "source": [
    "#### DeepFlow (Masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baf30a2-2e2a-4d03-8a40-8ac26d5a0876",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220724.nc\"\n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc, variable_key=\"fai_anomaly\")\n",
    "    flow_vectors_m = calculate_deepflow(prev_nc, next_nc, variable_key=\"masked_land\")\n",
    "    flow_vectors_f = calculate_deepflow(prev_nc, next_nc, variable_key=\"filtered\")\n",
    "    save_flow(prev_nc, output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow_masked.nc\", flow_vectors=flow_vectors, flow_vectors_f=flow_vectors_f,\n",
    "             flow_vectors_m=flow_vectors_m, mask_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76553a4f-56c9-477e-942e-202a66e44f06",
   "metadata": {},
   "source": [
    "#### Sub-daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02358505-4aae-4f19-ac7c-f5c03bfbccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220724.nc\"\n",
    "    \n",
    "    flow_vectors = calculate_deepflow(prev_nc, next_nc, variable_key=\"fai_anomaly\", time_interval=14400)\n",
    "    # flow_vectors_m = calculate_deepflow(prev_nc, next_nc, variable_key=\"masked_land\")\n",
    "    # flow_vectors_f = calculate_deepflow(prev_nc, next_nc, variable_key=\"filtered\")\n",
    "    save_flow(prev_nc, output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow_4h.nc\", flow_vectors=flow_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d6e3f5-65fd-4f3f-b12b-499729ef71fe",
   "metadata": {},
   "source": [
    "## Farnebacking Atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718a2b5-d056-4784-aae6-a462dcb64dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220724.nc\"\n",
    "    \n",
    "    flow_vectors = calculate_farneback(prev_nc, next_nc, variable_key=\"fai_anomaly\")\n",
    "    flow_vectors_m = calculate_farneback(prev_nc, next_nc, variable_key=\"masked_land\")\n",
    "    flow_vectors_f = calculate_farneback(prev_nc, next_nc, variable_key=\"filtered\")\n",
    "    save_flow(prev_nc, output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback.nc\", flow_vectors=flow_vectors, flow_vectors_f=flow_vectors_f,\n",
    "             flow_vectors_m=flow_vectors_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc368ea-a22f-4b75-9aa1-34ca3db8ddef",
   "metadata": {},
   "source": [
    "#### Farneback (Masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805beec5-9a3b-469b-a0b1-075abfaa8a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220724.nc\"\n",
    "    flow_vectors = calculate_farneback(prev_nc, next_nc, variable_key=\"fai_anomaly\")\n",
    "    flow_vectors_m = calculate_farneback(prev_nc, next_nc, variable_key=\"masked_land\")\n",
    "    flow_vectors_f = calculate_farneback(prev_nc, next_nc, variable_key=\"filtered\")\n",
    "    save_flow(prev_nc, output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback_masked.nc\", flow_vectors=flow_vectors, flow_vectors_f=flow_vectors_f,\n",
    "             flow_vectors_m=flow_vectors_m, mask_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c800d-a8f2-4558-9086-9d111b9ce7a7",
   "metadata": {},
   "source": [
    "## LKing Atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4f89c-511a-40c8-bfb9-0cf7113c061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220724.nc\"\n",
    "    \n",
    "    flow_vectors = calculate_lk(prev_nc, next_nc, variable_key=\"fai_anomaly\")\n",
    "    flow_vectors_m = calculate_lk(prev_nc, next_nc, variable_key=\"masked_land\")\n",
    "    flow_vectors_f = calculate_lk(prev_nc, next_nc, variable_key=\"filtered\")\n",
    "    save_flow(prev_nc, output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_lk.nc\", flow_vectors=flow_vectors, flow_vectors_f=flow_vectors_f,\n",
    "             flow_vectors_m=flow_vectors_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86008145-2359-4c26-a5b1-0ea7bc1c3c5d",
   "metadata": {},
   "source": [
    "## Block-Matching Atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fae409ba-bdaf-44f9-9bf4-b18188c46194",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220724.nc\"\n",
    "    # 197 = 86 + 25 + 86\n",
    "    flow_vectors = calculate_block_matching(prev_nc, next_nc, variable_key=\"fai_anomaly\", block_size=(10, 10), search_area=(50, 50)) \n",
    "    flow_vectors_f = calculate_block_matching(prev_nc, next_nc, variable_key=\"filtered\", block_size=(10, 10), search_area=(50, 50))\n",
    "    save_flow(prev_nc, output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Block.nc\", flow_vectors=flow_vectors, flow_vectors_f=flow_vectors_f,\n",
    "              mask_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f9665-3180-455e-b361-8dd26d754d98",
   "metadata": {},
   "source": [
    "## Size Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e2996-66a6-40e2-a957-ccfec1dcf88e",
   "metadata": {},
   "source": [
    "### *calculate_flow_and_update_nc*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d935aca2-7a54-4f64-916f-d7c23ecacb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_flow_and_update_nc(nc_file_day1, nc_file_day2, output_nc_file, dt=86400, max_pixel_move=100):\n",
    "    ds1 = xr.open_dataset(nc_file_day1)\n",
    "    ds2 = xr.open_dataset(nc_file_day2)\n",
    "    \n",
    "    binarized1 = ds1[\"filtered\"].values\n",
    "    binarized2 = ds2[\"filtered\"].values\n",
    "    \n",
    "    labels1, _ = label(binarized1)\n",
    "    labels2, _ = label(binarized2)\n",
    "    \n",
    "    latitudes = ds1.latitude.values\n",
    "    longitudes = ds1.longitude.values\n",
    "    \n",
    "    flow_u = np.zeros_like(binarized1, dtype=np.float32)\n",
    "    flow_v = np.zeros_like(binarized1, dtype=np.float32)\n",
    "    \n",
    "    props1 = regionprops(labels1)\n",
    "    props2 = regionprops(labels2)\n",
    "    for prop1 in props1:\n",
    "        centroid1 = prop1.centroid\n",
    "        best_match, min_dist = None, np.inf\n",
    "        for prop2 in props2:\n",
    "            centroid2 = prop2.centroid\n",
    "            dist = np.linalg.norm(np.array(centroid1) - np.array(centroid2))\n",
    "            if dist <= max_pixel_move and dist < min_dist:\n",
    "                min_dist, best_match = dist, prop2\n",
    "\n",
    "        if best_match:\n",
    "            dx = best_match.centroid[1] - prop1.centroid[1]\n",
    "            dy = best_match.centroid[0] - prop1.centroid[0]\n",
    "            lon1, lat1 = longitudes[int(prop1.centroid[1])], latitudes[int(prop1.centroid[0])]\n",
    "            lon2, lat2 = longitudes[int(prop1.centroid[1] + dx)], latitudes[int(prop1.centroid[0] + dy)]\n",
    "            \n",
    "            distance_x = haversine(lon1, lat1, lon2, lat1) * np.sign(dx) * 1000\n",
    "            distance_y = haversine(lon1, lat1, lon1, lat2) * np.sign(dy) * 1000\n",
    "            flow_u[labels1 == prop1.label] = distance_x / dt\n",
    "            flow_v[labels1 == prop1.label] = distance_y / dt\n",
    "\n",
    "    new_ds = xr.Dataset({\n",
    "        'flow_u': (('latitude', 'longitude'), flow_u),\n",
    "        'flow_v': (('latitude', 'longitude'), flow_v)\n",
    "    }, coords={'latitude': ds1.latitude, 'longitude': ds1.longitude})\n",
    "    new_ds.to_netcdf(output_nc_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65c90880-a043-431c-88ce-b51b94800ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    prev_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220723.nc\"\n",
    "    next_nc = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220724.nc\"\n",
    "    output_path=\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/matching.nc\"\n",
    "    calculate_flow_and_update_nc(prev_nc, next_nc, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eba360-c7bd-443f-a30b-73551845c02c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
