{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a7142ac-4d67-4697-a8e1-d23a81908f82",
   "metadata": {},
   "source": [
    "# Image Generator\n",
    "This notebook was created to ease the image generation process, i.e turning the netCDF data into something the OF algorithms can take as input and saving it to the hard drive.\n",
    "\n",
    "**N.B: The functions used here do not create the directories, they have to be created manually. (NO LONGER)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ae2488-f567-468e-a3ab-5a5e5dd482c6",
   "metadata": {},
   "source": [
    "## Importing necessary libraries and notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e1a4c-8b03-43d8-806f-ab125a525295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import ticker\n",
    "from IPython.display import Image, display, HTML\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# Import the other notebooks without running their cells\n",
    "from ii_Data_Manipulation import visualize_4\n",
    "from iii_GOES_average import time_list, visualize_aggregate, calculate_median\n",
    "from iv_Image_Processing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3a1833-cefa-4c01-9374-8568fddfc059",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Antilles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6157c68d-c816-41f8-a28a-c45c9dfdd534",
   "metadata": {},
   "source": [
    "### ABI_Averages_Antilles\n",
    "We're going to average and process all the ABI-GOES images and save them to the directory ABI_Averages on the hard drive \"ballena\". Running this block might take a while. To optimize we could try and parallelize this process using the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cc640b-4437-4496-b3ce-62320a1d0696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     start_date = '20221121'\n",
    "#     end_date = '20221231'\n",
    "#     directory = '/media/yahia/ballena/CLS/abi-goes-global-hr' \n",
    "#     output_directory = '/media/yahia/ballena/ABI_Averages_Antilles' \n",
    "#     latitude_range = (12, 17)  \n",
    "#     longitude_range = (-67, -60) \n",
    "    \n",
    "#     # Calculate the 1-day averages and save them\n",
    "#     process_dates(start_date, end_date, directory, output_directory, latitude_range, longitude_range, color=\"viridis\")\n",
    "    \n",
    "#     # Paths\n",
    "#     source_directory = '/media/yahia/ballena/ABI_Averages_Antilles' \n",
    "#     destination_directory = '/media/yahia/ballena/ABI_Averages_Antilles_Processed' \n",
    "    \n",
    "#     # Process the directory (filter, binarize and crop the images)\n",
    "#     process_directory(source_directory, destination_directory, threshold=180, bilateral=False, binarize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4dd0e9-771f-48f7-9d3c-bd54f39b7a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Binarized and bilateral images\n",
    "# if __name__ == '__main__':\n",
    "#     # Paths\n",
    "#     source_directory = '/media/yahia/ballena/ABI/ABI_Averages_Antilles' \n",
    "#     destination_directory = '/media/yahia/ballena/ABI/ABI_Averages_Antilles_Binarized_Bilateral' \n",
    "    \n",
    "#     # Process the directory (filter, binarize and crop the images)\n",
    "#     process_directory(source_directory, destination_directory, threshold=100, bilateral=True, binarize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ca3178-8ac2-496e-b159-129980d42b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Binarized and bilateral images (negative)\n",
    "# if __name__ == '__main__':\n",
    "#     # Paths\n",
    "#     source_directory = '/media/yahia/ballena/ABI/ABI_Averages_Antilles' \n",
    "#     destination_directory = '/media/yahia/ballena/ABI/ABI_Averages_Antilles_Binarized_Bilateral_Negative' \n",
    "    \n",
    "#     # Process the directory (filter, binarize and crop the images)\n",
    "#     process_directory(source_directory, destination_directory, threshold=100, bilateral=True, binarize=True, negative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7025b89d-1259-4772-86bf-003c96943f81",
   "metadata": {},
   "source": [
    "### MODIS_Images\n",
    "The function **process_dates** we previously defined is only adapted to ABI-GOES images, we will need to write a function that does the same for MODIS and OLCI images. We will also need to do the same for **save_aggregate**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de50e851-05e9-4a16-b13f-ec269ffbe320",
   "metadata": {},
   "source": [
    "Generating the MODIS images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df5195a-685d-4368-961a-0c3712c6cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     start_date = '20201207'\n",
    "#     end_date = '20221231'\n",
    "#     directory = '/media/yahia/ballena/CLS/modis-aqua-global-lr' \n",
    "#     output_directory = '/media/yahia/ballena/MODIS_Antilles' \n",
    "#     latitude_range = (12, 17)  \n",
    "#     longitude_range = (-67, -60) \n",
    "    \n",
    "#     # Calculate the 1-day averages and save them\n",
    "#     process_dates2(start_date, end_date, directory, output_directory, latitude_range, longitude_range, color=\"viridis\")\n",
    "    \n",
    "#     # Paths\n",
    "#     source_directory = '/media/yahia/ballena/MODIS_Antilles' \n",
    "#     destination_directory = '/media/yahia/ballena/MODIS_Antilles_Processed' \n",
    "    \n",
    "#     # Process the directory (filter, binarize and crop the images)\n",
    "#     process_directory(source_directory, destination_directory, threshold=180, bilateral=False, binarize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa738af-1bc1-4d1f-a484-173c5e1bb8c5",
   "metadata": {},
   "source": [
    "### OLCI_Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314a6e4d-41a7-4d15-b052-a86da7f8485d",
   "metadata": {},
   "source": [
    "Generating the OLCI images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54451ff-9a0c-4c2a-9b8a-02093d01385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     start_date = '20201207'\n",
    "#     end_date = '20240122'\n",
    "#     directory = '/media/yahia/ballena/CLS/olci-s3-global-lr' \n",
    "#     output_directory = '/media/yahia/ballena/OLCI_Antilles' \n",
    "#     latitude_range = (12, 17)  \n",
    "#     longitude_range = (-67, -60) \n",
    "    \n",
    "#     # Calculate the 1-day averages and save them\n",
    "#     process_dates2(start_date, end_date, directory, output_directory, latitude_range, longitude_range, color=\"viridis\")\n",
    "    \n",
    "#     # Paths\n",
    "#     source_directory = '/media/yahia/ballena/OLCI_Antilles' \n",
    "#     destination_directory = '/media/yahia/ballena/OLCI_Antilles_Processed' \n",
    "    \n",
    "#     # Process the directory (filter, binarize and crop the images)\n",
    "#     process_directory(source_directory, destination_directory, threshold=180, bilateral=False, binarize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aa07bc-8ad4-46ef-a2ef-2fac784f3898",
   "metadata": {},
   "source": [
    "## Range: (14, 15) (-66, -65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef29d36-f402-4e71-bd80-e8c0013e7b51",
   "metadata": {},
   "source": [
    "### ABI_Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed830d6-82d5-4ba8-94ca-cc3c28fb4c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     start_date = '20220701'\n",
    "#     end_date = '20220730'\n",
    "#     directory = '/media/yahia/ballena/CLS/abi-goes-global-hr' \n",
    "#     output_directory = '/media/yahia/ballena/ABI/Spiral/ABI_Averages_Spiral' \n",
    "#     latitude_range = (14, 15)  \n",
    "#     longitude_range = (-66, -65) \n",
    "    \n",
    "#     # Calculate the 1-day averages and save them\n",
    "#     process_dates(start_date, end_date, directory, output_directory, latitude_range, longitude_range, color=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04d3c99-ec7b-49a8-800a-41a901edf132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cropped and Bilateral\n",
    "# if __name__ == '__main__':\n",
    "#     # Paths\n",
    "#     source_directory = '/media/yahia/ballena/ABI/Spiral/ABI_Averages_Spiral'\n",
    "#     destination_directory = '/media/yahia/ballena/ABI/Spiral/ABI_Averages_Spiral_Processed'\n",
    "    \n",
    "#     # Process the directory (filter, binarize and crop the images)\n",
    "#     process_directory(source_directory, destination_directory, threshold=180, bilateral=True, binarize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c9bfa7-f535-4dd6-8d60-54627e0b2e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Binarized and bilateral images\n",
    "# if __name__ == '__main__':\n",
    "#     # Paths\n",
    "#     source_directory = '/media/yahia/ballena/ABI/Spiral/ABI_Averages_Spiral'\n",
    "#     destination_directory = '/media/yahia/ballena/ABI/Spiral/ABI_Averages_Spiral_Binarized_Bilateral'\n",
    "    \n",
    "#     # Process the directory (filter, binarize and crop the images)\n",
    "#     process_directory(source_directory, destination_directory, threshold=90, bilateral=True, binarize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac78325-536b-462e-8866-e2a594e47079",
   "metadata": {},
   "source": [
    "## Atlantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a18eef-7bcd-41b4-9fa3-46e9063a6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     start_date = '20220528'\n",
    "#     end_date = '20221231'\n",
    "#     directory = '/media/yahia/ballena/CLS/abi-goes-global-hr' \n",
    "#     output_directory = '/media/yahia/ballena/ABI/Atlantic/Averages' \n",
    "    \n",
    "#     # Calculate the 1-day averages and save them\n",
    "#     process_dates(start_date, end_date, directory, output_directory, color=\"viridis\")\n",
    "    \n",
    "#     # Paths\n",
    "#     source_directory = '/media/yahia/ballena/ABI/Atlantic/Averages' \n",
    "#     destination_directory = '/media/yahia/ballena/ABI/Atlantic/Averages_Cropped' \n",
    "    \n",
    "#     # Process the directory (crop the images)\n",
    "#     process_directory(source_directory, destination_directory, threshold=180, bilateral=False, binarize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ef9eb-3f6e-4c5b-abb4-923986695e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Binarized and bilateral images\n",
    "# if __name__ == '__main__':\n",
    "#     # Paths\n",
    "#     source_directory = '/media/yahia/ballena/ABI/Atlantic/Averages' \n",
    "#     destination_directory = '/media/yahia/ballena/ABI/Atlantic/Averages_Binarized_Bilateral' \n",
    "    \n",
    "#     # Process the directory (filter, binarize and crop the images)\n",
    "#     process_directory(source_directory, destination_directory, threshold=100, bilateral=True, binarize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54d28a-d75b-477a-aaf3-ba9a454f0f1f",
   "metadata": {},
   "source": [
    "### Partitioning the Atlantic\n",
    "We're going to divide the Atlantic into $n²$ regions (latitudes: 12°N-40°N, longitudes: 12°W-100°W), then process each region (average the ABI-GOES images, then apply filters) so we can later apply an OF algorithm on them and finally combine the result. We're going to use **concurrent** code to make the image generation process faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0440253-ae51-462a-9742-6d6d7f814205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_range(value):\n",
    "    \"\"\" Helper function to format the float values consistently for directory names. \"\"\"\n",
    "    return f\"{value:.6f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640711c8-1bc1-44a7-92ea-eaa65d7bbed9",
   "metadata": {},
   "source": [
    "#### *process_partition*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea64ff8b-bee3-460c-a270-f3b1d5424db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partition(lat_range, lon_range, start_date, end_date, directory, base_output_directory, color, save_image=True, save_netcdf=False):\n",
    "    formatted_lat_range = f\"[{format_range(lat_range[0])},{format_range(lat_range[1])}]\"\n",
    "    formatted_lon_range = f\"[{format_range(lon_range[1])},{format_range(lon_range[0])}]\"\n",
    "    output_directory = os.path.join(base_output_directory, f\"{formatted_lat_range},{formatted_lon_range}\")\n",
    "    process_dates(start_date, end_date, directory, output_directory, lat_range, lon_range, color, save_image=save_image, save_netcdf=save_netcdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb80e32f-8fcf-45a2-8d74-a19586736796",
   "metadata": {},
   "source": [
    "#### No Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f765a163-7101-4bd2-9970-5f09a5c55a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     n = 24\n",
    "#     lat_splits = np.linspace(12, 40, n+1)\n",
    "#     lon_splits = np.linspace(-12, -100, n+1)\n",
    "#     lat_splits = lat_splits.tolist()\n",
    "#     lon_splits = lon_splits.tolist()\n",
    "#     start_date = '20220723'\n",
    "#     end_date = '20220724'\n",
    "#     directory = '/media/yahia/ballena/CLS/abi-goes-global-hr' \n",
    "#     base_output_directory = f'/media/yahia/ballena/ABI/Partition/n = {n}/Averages'\n",
    "#     color = \"viridis\"\n",
    "    \n",
    "#     start_time = time.time()\n",
    "#     tasks = []\n",
    "    \n",
    "#     with ProcessPoolExecutor() as executor:\n",
    "#         for i in range(len(lat_splits)-1):\n",
    "#             for j in range(len(lon_splits)-1):\n",
    "#                 lat_range = (lat_splits[i], lat_splits[i+1])\n",
    "#                 lon_range = (lon_splits[j+1], lon_splits[j])\n",
    "#                 tasks.append(executor.submit(process_partition, lat_range, lon_range, start_date, end_date, directory, base_output_directory, color, True, False))\n",
    "        \n",
    "#         # Optionally, wait for all tasks to complete\n",
    "#         for task in tasks:\n",
    "#             task.result()\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     # Calculate and print the elapsed time\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(f\"Total execution time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b11f8-8750-4185-b812-76f560c92cf6",
   "metadata": {},
   "source": [
    "Total execution time: 200.18 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aff614-9763-4c30-bfe4-d144e48b669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cropped\n",
    "# if __name__ == '__main__':\n",
    "#     for i in range(len(lat_splits)-1):\n",
    "#         for j in range(len(lon_splits)-1):\n",
    "#             # Calculate the 1-day averages and save them\n",
    "#             source_directory = f'/media/yahia/ballena/ABI/Partition/n = {n}/Averages/[{lat_splits[i]},{lat_splits[i+1]}],[{lon_splits[j]},{lon_splits[j+1]}]' \n",
    "#             destination_directory = f'/media/yahia/ballena/ABI/Partition/n = {n}/Averages_Cropped/[{lat_splits[i]},{lat_splits[i+1]}],[{lon_splits[j]},{lon_splits[j+1]}]' \n",
    "#             process_directory(source_directory, destination_directory, threshold=180, bilateral=False, binarize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8123255b-a230-471e-8366-156536f11239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Binarized_Bilateral\n",
    "# if __name__ == '__main__':\n",
    "#     for i in range(len(lat_splits)-1):\n",
    "#         for j in range(len(lon_splits)-1):\n",
    "#             # Calculate the 1-day averages and save them\n",
    "#             source_directory = f'/media/yahia/ballena/ABI/Partition/n = {n}/Averages/[{lat_splits[i]},{lat_splits[i+1]}],[{lon_splits[j]},{lon_splits[j+1]}]' \n",
    "#             destination_directory = f'/media/yahia/ballena/ABI/Partition/n = {n}/Averages_Binarized_Bilateral/[{lat_splits[i]},{lat_splits[i+1]}],[{lon_splits[j]},{lon_splits[j+1]}]' \n",
    "#             process_directory(source_directory, destination_directory, threshold=100, bilateral=True, binarize=True, negative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a875b-0088-43c6-8063-9d6bb54df99c",
   "metadata": {},
   "source": [
    "#### Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b015152f-0e95-4007-abf1-1a5cc0979c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    n = 24\n",
    "    overlap_factor = 0.1  # Define the percentage of overlap, e.g., 10%\n",
    "    lat_splits = np.linspace(12, 40, n+1)\n",
    "    lon_splits = np.linspace(-12, -100, n+1)\n",
    "    start_date = '20220723'\n",
    "    end_date = '20220724'\n",
    "    directory = '/media/yahia/ballena/CLS/abi-goes-global-hr'\n",
    "    base_output_directory = f'/media/yahia/ballena/ABI/Partition_Overlap/n = {n}/Averages'\n",
    "    color = \"viridis\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    tasks = []\n",
    "\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        for i in range(len(lat_splits)-1):\n",
    "            for j in range(len(lon_splits)-1):\n",
    "                # Extend each range by a certain overlap factor\n",
    "                lat_range_lower = lat_splits[i] - (lat_splits[i+1] - lat_splits[i]) * overlap_factor\n",
    "                lat_range_upper = lat_splits[i+1] + (lat_splits[i+1] - lat_splits[i]) * overlap_factor\n",
    "                lon_range_lower = lon_splits[j+1] - (lon_splits[j] - lon_splits[j+1]) * overlap_factor\n",
    "                lon_range_upper = lon_splits[j] + (lon_splits[j] - lon_splits[j+1]) * overlap_factor\n",
    "                \n",
    "                # Correct the ranges to not exceed the overall boundaries\n",
    "                lat_range_lower = max(lat_range_lower, 12)\n",
    "                lat_range_upper = min(lat_range_upper, 40)\n",
    "                lon_range_lower = max(lon_range_lower, -100)\n",
    "                lon_range_upper = min(lon_range_upper, -12)\n",
    "\n",
    "                lat_range = (lat_range_lower, lat_range_upper)\n",
    "                lon_range = (lon_range_lower, lon_range_upper)\n",
    "                \n",
    "                tasks.append(executor.submit(process_partition, lat_range, lon_range, start_date, end_date, directory, base_output_directory, color, True, False))\n",
    "        \n",
    "        # Optionally, wait for all tasks to complete\n",
    "        for task in tasks:\n",
    "            task.result()\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Total execution time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f8831-a29a-4c2f-8df4-0c4b99fe3b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropped\n",
    "if __name__ == '__main__':\n",
    "    n = 24\n",
    "    lat_splits = np.linspace(12, 40, n+1)\n",
    "    lon_splits = np.linspace(-12, -100, n+1)\n",
    "    \n",
    "    for i in range(len(lat_splits)-1):\n",
    "        for j in range(len(lon_splits)-1):\n",
    "            # Format the directory paths using the consistent format\n",
    "            lat_range = f\"[{format_range(lat_splits[i])},{format_range(lat_splits[i+1])}]\"\n",
    "            lon_range = f\"[{format_range(lon_splits[j+1])},{format_range(lon_splits[j])}]\"\n",
    "            \n",
    "            source_directory = f'/media/yahia/ballena/ABI/Partition_Overlap/n = {n}/Averages/{lat_range},{lon_range}'\n",
    "            destination_directory = f'/media/yahia/ballena/ABI/Partition_Overlap/n = {n}/Averages_Cropped/{lat_range},{lon_range}'\n",
    "            \n",
    "            # Assuming process_directory function exists and performs the cropping\n",
    "            process_directory(source_directory, destination_directory, threshold=180, bilateral=False, binarize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8f7365-2b58-4df6-a3da-4dc910cc8e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarized_Bilateral\n",
    "if __name__ == '__main__':\n",
    "    for i in range(len(lat_splits)-1):\n",
    "        for j in range(len(lon_splits)-1):\n",
    "            # Calculate the 1-day averages and save them\n",
    "            source_directory = f'/media/yahia/ballena/ABI/Partition_Overlap/n = {n}/Averages/[{lat_splits[i]},{lat_splits[i+1]}],[{lon_splits[j]},{lon_splits[j+1]}]' \n",
    "            destination_directory = f'/media/yahia/ballena/ABI/Partition_Overlap/n = {n}/Averages_Binarized_Bilateral/[{lat_splits[i]},{lat_splits[i+1]}],[{lon_splits[j]},{lon_splits[j+1]}]' \n",
    "            process_directory(source_directory, destination_directory, threshold=100, bilateral=True, binarize=True, negative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82986b7-e43d-4db8-837a-c64e0cf35e3c",
   "metadata": {},
   "source": [
    "#### NetCDF Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43402e-b5fa-4f11-bdfa-789eb3a9a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    n = 24\n",
    "    lat_splits = np.linspace(12, 40, n+1)\n",
    "    lon_splits = np.linspace(-12, -100, n+1)\n",
    "    lat_splits = lat_splits.tolist()\n",
    "    lon_splits = lon_splits.tolist()\n",
    "    start_date = '20220723'\n",
    "    end_date = '20220724'\n",
    "    directory = '/media/yahia/ballena/CLS/abi-goes-global-hr' \n",
    "    base_output_directory = f'/media/yahia/ballena/ABI/NetCDF/Partition/n = {n}/Averages'\n",
    "    color = \"viridis\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    tasks = []\n",
    "    \n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        for i in range(len(lat_splits)-1):\n",
    "            for j in range(len(lon_splits)-1):\n",
    "                lat_range = (lat_splits[i], lat_splits[i+1])\n",
    "                lon_range = (lon_splits[j+1], lon_splits[j])\n",
    "                tasks.append(executor.submit(process_partition, lat_range, lon_range, start_date, end_date, directory, base_output_directory, color, False, True))\n",
    "        \n",
    "        # Optionally, wait for all tasks to complete\n",
    "        for task in tasks:\n",
    "            task.result()\n",
    "\n",
    "    end_time = time.time()\n",
    "    # Calculate and print the elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Total execution time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a568e38f-e1e9-4891-8761-eac960c19969",
   "metadata": {},
   "source": [
    "Total execution time: 351.47 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e1e14f-02c8-449c-bb89-5f6734e2cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarized\n",
    "if __name__ == '__main__':\n",
    "    for i in range(len(lat_splits)-1):\n",
    "        for j in range(len(lon_splits)-1):\n",
    "            # Calculate the 1-day averages and save them\n",
    "            source_directory = f'/media/yahia/ballena/ABI/NetCDF/Partition/n = {n}/Averages/[{lat_splits[i]},{lat_splits[i+1]}],[{lon_splits[j]},{lon_splits[j+1]}]' \n",
    "            destination_directory = f'/media/yahia/ballena/ABI/NetCDF/Partition/n = {n}/Averages_Binarized/[{lat_splits[i]},{lat_splits[i+1]}],[{lon_splits[j]},{lon_splits[j+1]}]' \n",
    "            process_directory_netCDF(source_directory, destination_directory, threshold=10, bilateral=False, binarize=True, negative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbe09a2-ff52-4721-9a71-2f16c14bc4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarized_Bilateral\n",
    "if __name__ == '__main__':\n",
    "    for i in range(len(lat_splits)-1):\n",
    "        for j in range(len(lon_splits)-1):\n",
    "            # Calculate the 1-day averages and save them\n",
    "            source_directory = f'/media/yahia/ballena/ABI/NetCDF/Partition/n = {n}/Averages/[{lat_splits[i]},{lat_splits[i+1]}],[{lon_splits[j]},{lon_splits[j+1]}]' \n",
    "            destination_directory = f'/media/yahia/ballena/ABI/NetCDF/Partition/n = {n}/Averages_Binarized_Bilateral/[{lat_splits[i]},{lat_splits[i+1]}],[{lon_splits[j]},{lon_splits[j+1]}]' \n",
    "            process_directory_netCDF(source_directory, destination_directory, threshold=9, bilateral=True, binarize=True, negative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c8e07-5047-48d6-8a0e-93f519dba3ba",
   "metadata": {},
   "source": [
    "### Atlantic (without partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdfeaf3-fe77-48ac-8edb-d6e664cdbf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Atlantic (without partition)\n",
    "if __name__ == '__main__':\n",
    "    start_date = '20220701'\n",
    "    end_date = '20220731'\n",
    "    directory = '/media/yahia/ballena/CLS/abi-goes-global-hr' \n",
    "    output_directory = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Averages' \n",
    "    \n",
    "    # Calculate the 1-day averages and save them\n",
    "    process_dates(start_date, end_date, directory, output_directory, color=\"viridis\", save_image=False, save_netcdf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeda393-25cb-48c9-bffa-5d7d4a2115f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Paths\n",
    "    source_directory = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Averages' \n",
    "    destination_directory = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Averages_Binarized' \n",
    "    \n",
    "    # Process the directory (binarize the images)\n",
    "    process_directory_netCDF(source_directory, destination_directory, threshold=1, bilateral=False, binarize=True, negative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06250eb6-7d0f-4c65-bc87-744fabdf5cfa",
   "metadata": {},
   "source": [
    "File size: 98 Mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ba2301-636c-430b-a8bb-a60ce34cb7e0",
   "metadata": {},
   "source": [
    "### PWC-Net images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb3023-a712-4aa2-abcc-d7f77533dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def netcdf_to_png(input_file_path, output_file_path, variable_name, threshold_value, dpi=300):\n",
    "    \"\"\"\n",
    "    Convert a specified variable from a NetCDF file to a binarized PNG image at high resolution.\n",
    "\n",
    "    Parameters:\n",
    "    - input_file_path: Path to the input NetCDF file.\n",
    "    - output_file_path: Path where the output PNG image will be saved.\n",
    "    - variable_name: The name of the variable in the NetCDF file to plot and save.\n",
    "    - threshold_value: Threshold value for binarization.\n",
    "    - dpi: Dots per inch for the output image resolution.\n",
    "    \"\"\"\n",
    "    # Load the NetCDF file\n",
    "    dataset = xr.open_dataset(input_file_path)\n",
    "    \n",
    "    # Access the variable to plot\n",
    "    data = dataset[variable_name].values\n",
    "    \n",
    "    # Handle potential multiple dimensions (assuming time or level might be present)\n",
    "    if data.ndim > 2:\n",
    "        data = data[0]  # Take the first slice if it's 3D or higher\n",
    "    \n",
    "    # Normalize the data to 0-255\n",
    "    normalized_data = cv2.normalize(data, None, 0, 255, norm_type=cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Convert to 8-bit image\n",
    "    img_8bit = np.uint8(normalized_data)\n",
    "    \n",
    "    # Binarize the image\n",
    "    _, binarized_img = cv2.threshold(img_8bit, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Create a figure with high resolution\n",
    "    plt.figure(figsize=(50, 40), dpi=dpi)\n",
    "    plt.imshow(binarized_img, cmap='gray', origin='lower')\n",
    "    plt.axis('off')  # Turn off the axis\n",
    "    plt.savefig(output_file_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "    # Close the dataset\n",
    "    dataset.close()\n",
    "    print(f\"Image saved as {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa2e19-1e49-4165-866b-bdf6ad4c367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    netcdf_file_path = '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered/Filtered_algae_distribution_20220723.nc'  # Path to your NetCDF file\n",
    "    output_png_path = '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered/23.png'         # Desired output PNG file path\n",
    "    variable_to_plot = 'fai_anomaly'             # Variable name to be plotted\n",
    "    \n",
    "    netcdf_to_png(netcdf_file_path, output_png_path, variable_to_plot, 127)\n",
    "    \n",
    "    netcdf_file_path = '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered/Filtered_algae_distribution_20220724.nc'  # Path to your NetCDF file\n",
    "    output_png_path = '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered/24.png'         # Desired output PNG file path\n",
    "    variable_to_plot = 'fai_anomaly'             # Variable name to be plotted\n",
    "    \n",
    "    netcdf_to_png(netcdf_file_path, output_png_path, variable_to_plot, 127)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a54c20-78cc-439d-853f-9625d22f31f2",
   "metadata": {},
   "source": [
    "### Filtered Atlantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f931e72c-a9b6-4196-ad2a-c3b30dc33006",
   "metadata": {},
   "source": [
    "#### *process_file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff57972c-5c7a-41c8-9e33-59ace654bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filename, source_directory, destination_directory):\n",
    "    \"\"\"\n",
    "    Process a single NetCDF file and save the processed result.\n",
    "    \"\"\"\n",
    "    source_path = os.path.join(source_directory, filename)\n",
    "    new_filename = 'Filtered_' + filename\n",
    "    dest_path = os.path.join(destination_directory, new_filename)\n",
    "    \n",
    "    # Process the NetCDF file\n",
    "    fai_anomaly_result = process_netCDF(source_path, threshold=1, bilateral=False, binarize=True, negative=False, \n",
    "                                         filter_small=False, land_mask=False, coast_mask=False)\n",
    "    \n",
    "    filtered_result = process_netCDF(source_path, threshold=1, bilateral=False, binarize=True, negative=False, \n",
    "                                      filter_small=False, size_threshold=10, land_mask=True, coast_mask=True, \n",
    "                                      coast_threshold=50000, adaptive_small=True, base_threshold=15, higher_threshold=10000, \n",
    "                                      latitude_limit=30)\n",
    "    \n",
    "    # Convert DataArray to Dataset if needed\n",
    "    if isinstance(fai_anomaly_result, xr.DataArray):\n",
    "        fai_anomaly_result = fai_anomaly_result.to_dataset(name='fai_anomaly')\n",
    "    if isinstance(filtered_result, xr.DataArray):\n",
    "        filtered_result = filtered_result.to_dataset(name='filtered')\n",
    "\n",
    "    # Merge datasets\n",
    "    combined_dataset = xr.merge([fai_anomaly_result, filtered_result])\n",
    "\n",
    "    # Save the combined dataset\n",
    "    combined_dataset.to_netcdf(dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696f9bdb-89f1-42a4-b9af-f32dd6a19304",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Paths\n",
    "    source_directory = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Averages' \n",
    "    destination_directory = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered' \n",
    "    \n",
    "    # Process the directory (binarize the images)\n",
    "    # Iterate over all files in the source directory\n",
    "    for filename in os.listdir(source_directory):\n",
    "        if filename.endswith('.nc'):\n",
    "            # Original NetCDF file path\n",
    "            source_path = os.path.join(source_directory, filename)\n",
    "            \n",
    "            # New filename with 'Processed' prefix\n",
    "            new_filename = 'Filtered_' + filename\n",
    "            \n",
    "            # Define the output path for the processed NetCDF file\n",
    "            dest_path = os.path.join(destination_directory, new_filename)\n",
    "            \n",
    "            # Process the NetCDF file\n",
    "            # First dimension\n",
    "            fai_anomaly_dataset = process_netCDF(source_path, threshold=1, bilateral=False, binarize=True, crop=False, negative=False, \n",
    "                                  filter_small=False, land_mask=False, coast_mask=False)\n",
    "\n",
    "            # Second dimension\n",
    "            masked_land = process_netCDF(source_path, threshold=1, bilateral=False, binarize=True, crop=False, negative=False, \n",
    "                                  filter_small=False, land_mask=True, coast_mask=False)\n",
    "            \n",
    "            # Third dimension\n",
    "            filtered_dataset = process_netCDF(source_path, threshold=1, bilateral=False, binarize=True, crop=False, negative=False, \n",
    "                               filter_small=True, size_threshold=10, land_mask=True, coast_mask=True, coast_threshold=50000)\n",
    "        \n",
    "            # Extract the main variable from each dataset\n",
    "            fai_anomaly_data = fai_anomaly_dataset[list(fai_anomaly_dataset.data_vars)[0]]\n",
    "            masked_land = masked_land[list(masked_land.data_vars)[0]]\n",
    "            filtered_data = filtered_dataset[list(filtered_dataset.data_vars)[0]]\n",
    "            \n",
    "            # Combine both datasets into a new dataset with both variables\n",
    "            combined_dataset = xr.Dataset({\n",
    "                'fai_anomaly': fai_anomaly_data,\n",
    "                'masked_land': masked_land,\n",
    "                'filtered': filtered_data\n",
    "            })\n",
    "\n",
    "            # Saving the file\n",
    "            combined_dataset.to_netcdf(dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e80ae6-d345-4a88-930e-683e299e69b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    source_directory = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Averages'\n",
    "    destination_directory = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered'\n",
    "    \n",
    "    # Get all .nc files in the source directory\n",
    "    filenames = [f for f in os.listdir(source_directory) if f.endswith('.nc')]\n",
    "    \n",
    "    # Use ProcessPoolExecutor to process files in parallel\n",
    "    with ProcessPoolExecutor(max_workers=8) as executor:\n",
    "        # Map the processing function to each file\n",
    "        futures = [executor.submit(process_file, filename, source_directory, destination_directory) for filename in filenames]\n",
    "        \n",
    "        # Optionally collect results or handle exceptions\n",
    "        for future in futures:\n",
    "            try:\n",
    "                result = future.result()  # Wait for each future to complete if needed\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262160b3-435a-4dd6-b136-2dfb0756c619",
   "metadata": {},
   "source": [
    "### Sub-daily Atlantic\n",
    "The function process_dates by default averages the images over a day. We need to modify it first before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72542ad-2049-47fb-8062-24661da68998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dates_modified(start_date, end_date, directory, output_dir, lat_range=None, lon_range=None, \n",
    "                  color=\"viridis\", save_image=True, save_netcdf=False, start_time=\"00:00\", end_time=\"23:59\"):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert the start and end dates from strings to datetime objects\n",
    "    current_date = datetime.strptime(start_date, '%Y%m%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y%m%d')\n",
    "    \n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime('%Y%m%d')\n",
    "        \n",
    "        # Formulate the full datetime strings for the start and end times on the current date\n",
    "        full_start_time = datetime.strptime(f\"{date_str}_{start_time}\", '%Y%m%d_%H:%M')\n",
    "        full_end_time = datetime.strptime(f\"{date_str}_{end_time}\", '%Y%m%d_%H:%M')\n",
    "\n",
    "        # Generate time intervals within the specified start and end times\n",
    "        times_for_day = time_list(full_start_time, full_end_time, 10)  # 10-minute intervals\n",
    "\n",
    "        # Calculate the median distribution of the specified variable (e.g., algae) based on the list of timestamps\n",
    "        median_distribution = calculate_median(times_for_day, lat_range, lon_range)\n",
    "        \n",
    "        # Prepare the output file paths\n",
    "        output_image_path = os.path.join(output_dir, f'algae_distribution_{date_str}.png') if save_image else None\n",
    "        output_netcdf_path = os.path.join(output_dir, f'algae_distribution_{date_str}.nc') if save_netcdf else None\n",
    "        \n",
    "        # Save visualizations and data\n",
    "        save_aggregate(median_distribution, lat_range, lon_range, color=color, \n",
    "                       output_filepath=output_image_path, netcdf_filepath=output_netcdf_path, display=False)\n",
    "        \n",
    "        # Increment the current date by one day\n",
    "        current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59968e6-b72f-49c9-a729-495afc293504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Atlantic (without partition)\n",
    "if __name__ == '__main__':\n",
    "    start_date = '20220723'\n",
    "    end_date = '20220723'\n",
    "    directory = '/media/yahia/ballena/CLS/abi-goes-global-hr' \n",
    "    output_directory = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Averages_4h' \n",
    "    \n",
    "    # Calculate the 1-day averages and save them\n",
    "    process_dates_modified(start_date, end_date, directory, output_directory, color=\"viridis\", save_image=False, save_netcdf=True, start_time=\"13:00\", end_time=\"17:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fdb58f-40cc-42e4-9f12-a7cbae0f1173",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Paths\n",
    "    source_directory = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Averages_4h' \n",
    "    destination_directory = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered_4h' \n",
    "    \n",
    "    # Process the directory (binarize the images)\n",
    "    # Iterate over all files in the source directory\n",
    "    for filename in os.listdir(source_directory):\n",
    "        if filename.endswith('.nc'):\n",
    "            # Original NetCDF file path\n",
    "            source_path = os.path.join(source_directory, filename)\n",
    "            \n",
    "            # New filename with 'Processed' prefix\n",
    "            new_filename = 'Filtered_' + filename\n",
    "            \n",
    "            # Define the output path for the processed NetCDF file\n",
    "            dest_path = os.path.join(destination_directory, new_filename)\n",
    "            \n",
    "            # Process the NetCDF file\n",
    "            # First dimension\n",
    "            fai_anomaly_dataset = process_netCDF(source_path, threshold=1, bilateral=False, binarize=True, crop=False, negative=False, \n",
    "                                  filter_small=False, land_mask=False, coast_mask=False)\n",
    "\n",
    "            # Second dimension\n",
    "            masked_land = process_netCDF(source_path, threshold=1, bilateral=False, binarize=True, crop=False, negative=False, \n",
    "                                  filter_small=False, land_mask=True, coast_mask=False)\n",
    "            \n",
    "            # Third dimension\n",
    "            filtered_dataset = process_netCDF(source_path, threshold=1, bilateral=False, binarize=True, crop=False, negative=False, \n",
    "                               filter_small=True, size_threshold=10, land_mask=True, coast_mask=True, coast_threshold=50000)\n",
    "        \n",
    "            # Extract the main variable from each dataset\n",
    "            fai_anomaly_data = fai_anomaly_dataset[list(fai_anomaly_dataset.data_vars)[0]]\n",
    "            masked_land = masked_land[list(masked_land.data_vars)[0]]\n",
    "            filtered_data = filtered_dataset[list(filtered_dataset.data_vars)[0]]\n",
    "            \n",
    "            # Combine both datasets into a new dataset with both variables\n",
    "            combined_dataset = xr.Dataset({\n",
    "                'fai_anomaly': fai_anomaly_data,\n",
    "                'masked_land': masked_land,\n",
    "                'filtered': filtered_data\n",
    "            })\n",
    "\n",
    "            # Saving the file\n",
    "            combined_dataset.to_netcdf(dest_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
