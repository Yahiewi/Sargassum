{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46196525-1771-4d78-a337-32ec77b21b77",
   "metadata": {},
   "source": [
    "# Flow Generator\n",
    "Similar to the Image Generator notebook, we'll use this notebook to streamline the process of generating NetCDF files that contain the two variables **flow_u** and **flow_v**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c4f525-f7cb-4e0f-948a-fc67648e8733",
   "metadata": {},
   "source": [
    "## Importing necessary libraries and notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5957ca01-fe9e-4136-ad5f-e7f4431d552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import io\n",
    "import os\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import ticker\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from matplotlib.collections import LineCollection\n",
    "from IPython.display import Image, display, clear_output\n",
    "from PIL import Image as PILImage\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# Import the other notebooks without running their cells\n",
    "from ii_Data_Manipulation import visualize_4\n",
    "from iii_GOES_average import time_list, visualize_aggregate, calculate_median\n",
    "from iv_Image_Processing import collect_times, crop_image, save_aggregate, binarize_image, bilateral_image, process_dates, process_directory\n",
    "from vii_Flow_Analysis import haversine\n",
    "from vii_Flow_NetCDF import *\n",
    "from v_i_OF_Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc4476-e5db-4ade-80de-0dcae1c7bb8b",
   "metadata": {},
   "source": [
    "## DeepFlow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba1957c-facb-4d02-8b71-782958dc4509",
   "metadata": {},
   "source": [
    "### *process_file_pair*\n",
    "Function to parallelize the process of calculating the flow over the different pairs of images.\n",
    "\n",
    "**Crashes when run in parallel (no longer when limiting the max_workers).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b668a-dbca-4753-962b-bf33e2fa8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_pair(file_paths, destination_directory):\n",
    "    first_path, second_path = file_paths\n",
    "    # Calculate flow vectors for different variables\n",
    "    flow_vectors = calculate_deepflow(first_path, second_path, variable_key=\"fai_anomaly\")\n",
    "    flow_vectors_f = calculate_deepflow(first_path, second_path, variable_key=\"filtered\")\n",
    "\n",
    "    # Define the output filename and path\n",
    "    first_file = os.path.basename(first_path)\n",
    "    output_filename = 'DeepFlow_' + first_file\n",
    "    output_path = os.path.join(destination_directory, output_filename)\n",
    "\n",
    "    # Save flow data\n",
    "    save_flow(first_path, output_path, lat_range=None, lon_range=None, flow_vectors=flow_vectors, flow_vectors_m=None, flow_vectors_f=flow_vectors_f, mask_data=False)\n",
    "    print(f\"Processed and saved flow data for {first_file} to {output_filename}\")\n",
    "    return output_filename  # Return something to signify completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0036e656-a222-4ec0-8bb4-ce6a67b8ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Paths\n",
    "    source_directory = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered'\n",
    "    destination_directory = '/media/yahia/ballena/Flow/DeepFlow'\n",
    "\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(destination_directory, exist_ok=True)\n",
    "\n",
    "    # Get all .nc files sorted to ensure chronological order\n",
    "    files = sorted([f for f in os.listdir(source_directory) if f.endswith('.nc')])\n",
    "\n",
    "    # Prepare pairs of files for processing\n",
    "    file_pairs = [(os.path.join(source_directory, files[i]), os.path.join(source_directory, files[i+1])) for i in range(len(files) - 1)]\n",
    "\n",
    "    # Adjust the number of max_workers based on your system capabilities\n",
    "    max_workers = os.cpu_count() // 2  # For example, use half of your CPU cores\n",
    "\n",
    "    # Use ProcessPoolExecutor to execute multiple processes\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Map the helper function to each pair of files\n",
    "        futures = [executor.submit(process_file_pair, pair, destination_directory) for pair in file_pairs]\n",
    "\n",
    "        # Optionally handle results as they complete\n",
    "        for future in futures:\n",
    "            try:\n",
    "                print(future.result())  # Accessing result() will wait for the process to complete\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f148a204-0122-4be1-83fe-ee65c82a033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential\n",
    "if __name__ == '__main__':\n",
    "    # Paths\n",
    "    source_directory = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered' \n",
    "    destination_directory = '/media/yahia/ballena/Flow/DeepFlow' \n",
    "\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(destination_directory, exist_ok=True)\n",
    "\n",
    "    # Get all .nc files sorted to ensure chronological order\n",
    "    files = sorted([f for f in os.listdir(source_directory) if f.endswith('.nc')])\n",
    "\n",
    "    # Process each pair of consecutive files\n",
    "    for i in range(len(files) - 1):\n",
    "        first_file = files[i]\n",
    "        second_file = files[i + 1]\n",
    "\n",
    "        # Define full paths for both files\n",
    "        first_path = os.path.join(source_directory, first_file)\n",
    "        second_path = os.path.join(source_directory, second_file)\n",
    "\n",
    "        # Calculate flow vectors\n",
    "        flow_vectors = calculate_deepflow(first_path, second_path, variable_key=\"fai_anomaly\")\n",
    "        # flow_vectors_m = calculate_deepflow(first_path, second_path, variable_key=\"masked_land\")\n",
    "        flow_vectors_f = calculate_deepflow(first_path, second_path, variable_key=\"filtered\")\n",
    "\n",
    "        # Define the output path for the processed NetCDF file\n",
    "        output_filename = 'DeepFlow_' + first_file\n",
    "        output_path = os.path.join(destination_directory, output_filename)\n",
    "\n",
    "        # Save flow data\n",
    "        save_flow(first_path, output_path, lat_range=None, lon_range=None, flow_vectors=flow_vectors, flow_vectors_m=None, flow_vectors_f=flow_vectors_f,  mask_data=False) \n",
    "        print(f\"Processed and saved flow data for {first_file} to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db627384-5d28-4659-ad0a-ca85f94e6d04",
   "metadata": {},
   "source": [
    "Sequential (1 month, flow+filtered_flow): 1 hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baea4026-63cb-44dd-ba08-75b2af8572cd",
   "metadata": {},
   "source": [
    "## DeepFlow (Masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e2995-8071-44f1-9a2c-568b5fa3de7f",
   "metadata": {},
   "source": [
    "### *mask_flow*\n",
    "This takes in the images for which flow is already calculated and sets flow to 0 in pixels where there is no algae detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd90e2-1075-4a0a-ad05-71f43323df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_flow(data, output_path):\n",
    "    mask = data['fai_anomaly'] != 0\n",
    "    for var in data.data_vars:\n",
    "        # Check if there are filtered flow variables\n",
    "        if ('flow_u_f' in var) or ('flow_v_f' in var):\n",
    "            mask_filtered = data['filtered'] != 0\n",
    "            data[var] = xr.where(mask_filtered, data[var], 0)\n",
    "        if 'flow' in var:\n",
    "            data[var] = xr.where(mask, data[var], 0)\n",
    "    # Save the modified dataset to a new NetCDF file\n",
    "    data.to_netcdf(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf5e515-9fb6-4a35-bdec-8cb752b4fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking\n",
    "if __name__ == '__main__':\n",
    "    # Source directory with DeepFlow data\n",
    "    source_directory = '/media/yahia/ballena/Flow/DeepFlow'\n",
    "    destination_directory = '/media/yahia/ballena/Flow/DeepFlow_Masked'\n",
    "\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(destination_directory, exist_ok=True)\n",
    "\n",
    "    # Process each .nc file in the source directory\n",
    "    for filename in os.listdir(source_directory):\n",
    "        if filename.endswith('.nc'):\n",
    "            file_path = os.path.join(source_directory, filename)\n",
    "            output_path = os.path.join(destination_directory, 'Masked_' + filename)\n",
    "\n",
    "            # Load the dataset\n",
    "            data = xr.open_dataset(file_path)\n",
    "\n",
    "            # Mask the flow data and save the modified file\n",
    "            mask_flow(data, output_path)\n",
    "\n",
    "        print(f\"Masked flow data for {filename} to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcaae04-8ce5-41f5-afdb-a77ef032b4fd",
   "metadata": {},
   "source": [
    "## Farneback\n",
    "This is much faster than DeepFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e95fc88-c71a-4ccc-a9e8-def527be8ff1",
   "metadata": {},
   "source": [
    "### *process_file_pair_farneback*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a10a3-236d-4332-81ec-61432ad653a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_pair_farneback(file_paths, destination_directory):\n",
    "    first_path, second_path = file_paths\n",
    "    # Calculate flow vectors for different variables\n",
    "    flow_vectors = calculate_farneback(first_path, second_path, variable_key=\"fai_anomaly\")\n",
    "    flow_vectors_f = calculate_farneback(first_path, second_path, variable_key=\"filtered\")\n",
    "\n",
    "    # Define the output filename and path\n",
    "    first_file = os.path.basename(first_path)\n",
    "    output_filename = 'Farneback_' + first_file\n",
    "    output_path = os.path.join(destination_directory, output_filename)\n",
    "\n",
    "    # Save flow data\n",
    "    save_flow(first_path, output_path, lat_range=None, lon_range=None, flow_vectors=flow_vectors, flow_vectors_m=None, flow_vectors_f=flow_vectors_f, mask_data=False)\n",
    "    print(f\"Processed and saved flow data for {first_file} to {output_filename}\")\n",
    "    return output_filename  # Return something to signify completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9faf8ac-38d5-46d9-9b2d-1de22ccba45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Paths\n",
    "    source_directory = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered'\n",
    "    destination_directory = '/media/yahia/ballena/Flow/Farneback'\n",
    "\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(destination_directory, exist_ok=True)\n",
    "\n",
    "    # Get all .nc files sorted to ensure chronological order\n",
    "    files = sorted([f for f in os.listdir(source_directory) if f.endswith('.nc')])\n",
    "\n",
    "    # Prepare pairs of files for processing\n",
    "    file_pairs = [(os.path.join(source_directory, files[i]), os.path.join(source_directory, files[i+1])) for i in range(len(files) - 1)]\n",
    "\n",
    "    # Adjust the number of max_workers based on your system capabilities\n",
    "    max_workers = os.cpu_count() // 2  # For example, use half of your CPU cores\n",
    "\n",
    "    # Use ProcessPoolExecutor to execute multiple processes\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Map the helper function to each pair of files\n",
    "        futures = [executor.submit(process_file_pair_farneback, pair, destination_directory) for pair in file_pairs]\n",
    "\n",
    "        # Optionally handle results as they complete\n",
    "        for future in futures:\n",
    "            try:\n",
    "                print(future.result())  # Accessing result() will wait for the process to complete\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd4acbf-da63-42b2-83a6-a6534edb554b",
   "metadata": {},
   "source": [
    "## Farneback (Masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb0be7e-1ecb-4a73-8670-b57f230cfbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking\n",
    "if __name__ == '__main__':\n",
    "    # Source directory with DeepFlow data\n",
    "    source_directory = '/media/yahia/ballena/Flow/Farneback'\n",
    "    destination_directory = '/media/yahia/ballena/Flow/Farneback_Masked'\n",
    "\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(destination_directory, exist_ok=True)\n",
    "\n",
    "    # Process each .nc file in the source directory\n",
    "    for filename in os.listdir(source_directory):\n",
    "        if filename.endswith('.nc'):\n",
    "            file_path = os.path.join(source_directory, filename)\n",
    "            output_path = os.path.join(destination_directory, 'Masked_' + filename)\n",
    "\n",
    "            # Load the dataset\n",
    "            data = xr.open_dataset(file_path)\n",
    "\n",
    "            # Mask the flow data and save the modified file\n",
    "            mask_flow(data, output_path)\n",
    "\n",
    "        print(f\"Masked flow data for {filename} to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba7c4d8-f4bd-4948-b129-4e2288a56c43",
   "metadata": {},
   "source": [
    "## Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef43be1f-054a-4bfa-a953-f95ef5f135dd",
   "metadata": {},
   "source": [
    "### *combine_netcdfs*\n",
    "For now, this always crashes the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ea54f1-d16f-46f7-8dd5-f4e5265c15ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_netcdfs(source_directory, output_path, time_variable_name='time', batch_size=10):\n",
    "    \"\"\"\n",
    "    Combines multiple NetCDF files into one, adding a time dimension, in batches to manage memory usage.\n",
    "    \"\"\"\n",
    "    # List all NetCDF files in the source directory\n",
    "    nc_files = sorted([os.path.join(source_directory, f) for f in os.listdir(source_directory) if f.endswith('.nc')])\n",
    "\n",
    "    # Prepare an empty list to collect batch results\n",
    "    combined_batches = []\n",
    "\n",
    "    # Process in batches\n",
    "    for i in range(0, len(nc_files), batch_size):\n",
    "        batch_files = nc_files[i:i+batch_size]\n",
    "        datasets = [xr.open_dataset(f, chunks={}) for f in batch_files]  # Ensure that files are read in chunks\n",
    "        combined_batch = xr.concat(datasets, dim=time_variable_name)\n",
    "        combined_batches.append(combined_batch)\n",
    "        # Close datasets to free up memory\n",
    "        for ds in datasets:\n",
    "            ds.close()\n",
    "\n",
    "    # Combine all batches\n",
    "    final_combined = xr.concat(combined_batches, dim=time_variable_name)\n",
    "    final_combined[time_variable_name] = range(len(nc_files))\n",
    "\n",
    "    # Save the combined dataset\n",
    "    final_combined.to_netcdf(output_path)\n",
    "    print(f\"Combined NetCDF saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faf5637-372f-4d15-9031-c22ac4883c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yahia/anaconda3/lib/python3.11/site-packages/xarray/core/indexing.py:1443: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n",
      "/tmp/ipykernel_232913/512518665.py:26: SerializationWarning: saving variable filtered with floating point data as an integer dtype without any _FillValue to use for NaNs\n",
      "  final_combined.to_netcdf(output_path)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    source_dir = '/media/yahia/ballena/Flow/DeepFlow_Masked'\n",
    "    output_file_path = '/media/yahia/ballena/Flow/DeepFlow_Time.nc'\n",
    "    combine_netcdfs(source_dir, output_file_path, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857901c-2324-431e-a52f-ee77c440a2c0",
   "metadata": {},
   "source": [
    "### *calculate_and_concatenate_flows*\n",
    "Maybe this won't crash the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d26bfe-3bd9-42f4-a808-5aa899820a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_concatenate_flows(source_directory, output_path):\n",
    "    xr.set_options(enable_cftimeindex=True)  # Ensure cf-time compatibility if needed\n",
    "\n",
    "    # Get all .nc files sorted to ensure chronological order\n",
    "    files = sorted([os.path.join(source_directory, f) for f in os.listdir(source_directory) if f.endswith('.nc')])\n",
    "\n",
    "    # Prepare to store datasets\n",
    "    datasets = []\n",
    "    \n",
    "    # Process each pair of files\n",
    "    for i in range(len(files) - 1):\n",
    "        first_path = files[i]\n",
    "        second_path = files[i+1]\n",
    "        \n",
    "        # Calculate flow vectors\n",
    "        flow_u, flow_v = calculate_deepflow(first_path, second_path, variable_key=\"fai_anomaly\")\n",
    "        \n",
    "        # Load the first file as base for coordinates and other metadata, use Dask for lazy loading\n",
    "        ds = xr.open_dataset(first_path, chunks={\"latitude\": \"auto\", \"longitude\": \"auto\"})\n",
    "        ds['flow_u'] = (('latitude', 'longitude'), flow_u)\n",
    "        ds['flow_v'] = (('latitude', 'longitude'), flow_v)\n",
    "        \n",
    "        # Extract date from filename\n",
    "        try:\n",
    "            date_str = os.path.basename(first_path).split('_')[3].split('.')[0]\n",
    "            date = datetime.strptime(date_str, '%Y%m%d')\n",
    "            ds = ds.assign_coords(time=date)\n",
    "            ds = ds.expand_dims('time')\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {first_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        datasets.append(ds)\n",
    "    \n",
    "    # Concatenate all datasets along time dimension\n",
    "    if datasets:\n",
    "        combined_dataset = xr.concat(datasets, dim='time')\n",
    "        combined_dataset.to_netcdf(output_path, mode='w')\n",
    "        print(f\"Combined NetCDF with flow data saved to {output_path}\")\n",
    "    else:\n",
    "        print(\"No datasets to combine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e8e32-40f2-4cc2-80d6-e726cf67ab90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yahia/anaconda3/lib/python3.11/site-packages/xarray/core/options.py:115: FutureWarning: The enable_cftimeindex option is now a no-op and will be removed in a future version of xarray.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    source_directory = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered'\n",
    "    output_path = '/media/yahia/ballena/Flow/Combined_Flow.nc'   \n",
    "    calculate_and_concatenate_flows(source_directory, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec630404-2c0e-4c2a-a685-c0c7ed9342f1",
   "metadata": {},
   "source": [
    "### *calculate_flow_for_two_days*\n",
    "Still crashing so let's try for just two days.\n",
    "\n",
    "This works, let's try with more days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef615bce-4bf7-4030-a02e-821e61192f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_flow_for_two_days(first_path, second_path, output_path):\n",
    "    # Load the datasets with chunking to manage memory\n",
    "    ds1 = xr.open_dataset(first_path, chunks={\"latitude\": \"auto\", \"longitude\": \"auto\"})\n",
    "    ds2 = xr.open_dataset(second_path, chunks={\"latitude\": \"auto\", \"longitude\": \"auto\"})\n",
    "\n",
    "    # Calculate flow vectors assuming a function that does this exists\n",
    "    flow_u, flow_v = calculate_deepflow(first_path, second_path, variable_key=\"fai_anomaly\")\n",
    "\n",
    "    # Prepare the results with proper dimensions and coordinates\n",
    "    ds_combined = xr.Dataset({\n",
    "        'flow_u': (['time', 'latitude', 'longitude'], [flow_u, flow_u]),  # Using the same data as a placeholder for demonstration\n",
    "        'flow_v': (['time', 'latitude', 'longitude'], [flow_v, flow_v]),\n",
    "    }, coords={\n",
    "        'latitude': ds1.latitude,\n",
    "        'longitude': ds1.longitude\n",
    "    })\n",
    "\n",
    "    # Parse dates and assign them as a time coordinate\n",
    "    try:\n",
    "        date_str1 = os.path.basename(first_path).split('_')[3].split('.')[0]\n",
    "        date_str2 = os.path.basename(second_path).split('_')[3].split('.')[0]\n",
    "        date1 = datetime.strptime(date_str1, '%Y%m%d')\n",
    "        date2 = datetime.strptime(date_str2, '%Y%m%d')\n",
    "        ds_combined = ds_combined.assign_coords(time=[date1, date2])\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse dates from filenames: {e}\")\n",
    "\n",
    "    # Save the dataset\n",
    "    ds_combined.to_netcdf(output_path)\n",
    "    print(f\"Flow data for two days saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "633e4a0b-9030-4cb0-9ac6-e4916af88d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow data for two days saved to /media/yahia/ballena/Flow/Two_Days_Flow.nc\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    first_path = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220701.nc'\n",
    "    second_path = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220702.nc'\n",
    "    output_path = '/media/yahia/ballena/Flow/Two_Days_Flow.nc'\n",
    "    calculate_flow_for_two_days(first_path, second_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c26a5e-8ec5-495b-8f24-e7a2aa1a9ad7",
   "metadata": {},
   "source": [
    "### *calculate_flow_for_n_days*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16a50515-7a0d-422f-a72f-61f45ada581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_flow_for_n_days(source_directory, output_path, days=10):\n",
    "    \"\"\"\n",
    "    Calculates and concatenates flow data for a specified number of days.\n",
    "\n",
    "    Parameters:\n",
    "    - source_directory (str): Directory containing NetCDF files to process.\n",
    "    - output_path (str): Path to save the combined NetCDF file.\n",
    "    - days (int): Number of days to process. This translates to (days-1) pairs of files.\n",
    "    \"\"\"\n",
    "    # Get all .nc files sorted to ensure chronological order\n",
    "    files = sorted([os.path.join(source_directory, f) for f in os.listdir(source_directory) if f.endswith('.nc')])\n",
    "    \n",
    "    # Limit to the specified number of days, ensuring there are pairs to process\n",
    "    files = files[:days] if len(files) >= days else files\n",
    "\n",
    "    # Prepare to store datasets\n",
    "    datasets = []\n",
    "    \n",
    "    # Process each pair of files\n",
    "    for i in range(len(files) - 1):\n",
    "        first_path = files[i]\n",
    "        second_path = files[i+1]\n",
    "        \n",
    "        # Calculate flow vectors\n",
    "        flow_u, flow_v = calculate_deepflow(first_path, second_path, variable_key=\"fai_anomaly\")\n",
    "        \n",
    "        # Load the first file as base for coordinates and other metadata\n",
    "        ds = xr.open_dataset(first_path, chunks={\"latitude\": \"auto\", \"longitude\": \"auto\"})\n",
    "        ds['flow_u'] = (('latitude', 'longitude'), flow_u)\n",
    "        ds['flow_v'] = (('latitude', 'longitude'), flow_v)\n",
    "        \n",
    "        # Extract date from filename and handle possible errors\n",
    "        try:\n",
    "            date_str = os.path.basename(first_path).split('_')[3].split('.')[0]\n",
    "            date = datetime.strptime(date_str, '%Y%m%d')\n",
    "            ds = ds.assign_coords(time=date)\n",
    "            ds = ds.expand_dims('time')\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse dates from filenames {first_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        datasets.append(ds)\n",
    "    \n",
    "    # Concatenate all datasets along time dimension\n",
    "    if datasets:\n",
    "        combined_dataset = xr.concat(datasets, dim='time')\n",
    "        combined_dataset.to_netcdf(output_path)\n",
    "        print(f\"Flow data for {days-1} days saved to {output_path}\")\n",
    "    else:\n",
    "        print(\"No datasets to combine or an error occurred.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a209b7a5-7995-451c-8639-a173daf66995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow data for 2 days saved to /media/yahia/ballena/Flow/Custom_Days_Flow.nc\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    source_directory = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered'\n",
    "    output_path = '/media/yahia/ballena/Flow/Custom_Days_Flow.nc'\n",
    "    days = 3\n",
    "    calculate_flow_for_n_days(source_directory, output_path, days=days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f053a91d-c506-4a8e-ab8e-845906c176e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
