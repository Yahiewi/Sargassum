{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46196525-1771-4d78-a337-32ec77b21b77",
   "metadata": {},
   "source": [
    "# Flow Generator\n",
    "Similar to the Image Generator notebook, we'll use this notebook to streamline the process of generating NetCDF files that contain the two variables **flow_u** and **flow_v**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c4f525-f7cb-4e0f-948a-fc67648e8733",
   "metadata": {},
   "source": [
    "## Importing necessary libraries and notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5957ca01-fe9e-4136-ad5f-e7f4431d552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import io\n",
    "import os\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import ticker\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from matplotlib.collections import LineCollection\n",
    "from IPython.display import Image, display, clear_output\n",
    "from PIL import Image as PILImage\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# Import the other notebooks without running their cells\n",
    "from ii_Data_Manipulation import visualize_4\n",
    "from iii_GOES_average import time_list, visualize_aggregate, calculate_median\n",
    "from iv_Image_Processing import collect_times, crop_image, save_aggregate, binarize_image, bilateral_image, process_dates, process_directory\n",
    "from vii_Flow_Analysis import haversine\n",
    "from vii_DeepFlow_NetCDF import *\n",
    "from v_i_OF_Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dc4476-e5db-4ade-80de-0dcae1c7bb8b",
   "metadata": {},
   "source": [
    "## DeepFlow "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba1957c-facb-4d02-8b71-782958dc4509",
   "metadata": {},
   "source": [
    "### *process_file_pair*\n",
    "Function to parallelize the process of calculating the flow over the different pairs of images.\n",
    "\n",
    "**Crashes when run in parallel (no longer when limiting the max_workers).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9b668a-dbca-4753-962b-bf33e2fa8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_pair(file_paths, destination_directory):\n",
    "    first_path, second_path = file_paths\n",
    "    # Calculate flow vectors for different variables\n",
    "    flow_vectors = calculate_deepflow(first_path, second_path, variable_key=\"fai_anomaly\")\n",
    "    flow_vectors_f = calculate_deepflow(first_path, second_path, variable_key=\"filtered\")\n",
    "\n",
    "    # Define the output filename and path\n",
    "    first_file = os.path.basename(first_path)\n",
    "    output_filename = 'DeepFlow_' + first_file\n",
    "    output_path = os.path.join(destination_directory, output_filename)\n",
    "\n",
    "    # Save flow data\n",
    "    save_flow(first_path, output_path, lat_range=None, lon_range=None, flow_vectors=flow_vectors, flow_vectors_m=None, flow_vectors_f=flow_vectors_f, mask_data=False)\n",
    "    print(f\"Processed and saved flow data for {first_file} to {output_filename}\")\n",
    "    return output_filename  # Return something to signify completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0036e656-a222-4ec0-8bb4-ce6a67b8ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Paths\n",
    "    source_directory = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered'\n",
    "    destination_directory = '/media/yahia/ballena/Flow/DeepFlow'\n",
    "\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(destination_directory, exist_ok=True)\n",
    "\n",
    "    # Get all .nc files sorted to ensure chronological order\n",
    "    files = sorted([f for f in os.listdir(source_directory) if f.endswith('.nc')])\n",
    "\n",
    "    # Prepare pairs of files for processing\n",
    "    file_pairs = [(os.path.join(source_directory, files[i]), os.path.join(source_directory, files[i+1])) for i in range(len(files) - 1)]\n",
    "\n",
    "    # Adjust the number of max_workers based on your system capabilities\n",
    "    max_workers = os.cpu_count() // 2  # For example, use half of your CPU cores\n",
    "\n",
    "    # Use ProcessPoolExecutor to execute multiple processes\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Map the helper function to each pair of files\n",
    "        futures = [executor.submit(process_file_pair, pair, destination_directory) for pair in file_pairs]\n",
    "\n",
    "        # Optionally handle results as they complete\n",
    "        for future in futures:\n",
    "            try:\n",
    "                print(future.result())  # Accessing result() will wait for the process to complete\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f148a204-0122-4be1-83fe-ee65c82a033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential\n",
    "if __name__ == '__main__':\n",
    "    # Paths\n",
    "    source_directory = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered' \n",
    "    destination_directory = '/media/yahia/ballena/Flow/DeepFlow' \n",
    "\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(destination_directory, exist_ok=True)\n",
    "\n",
    "    # Get all .nc files sorted to ensure chronological order\n",
    "    files = sorted([f for f in os.listdir(source_directory) if f.endswith('.nc')])\n",
    "\n",
    "    # Process each pair of consecutive files\n",
    "    for i in range(len(files) - 1):\n",
    "        first_file = files[i]\n",
    "        second_file = files[i + 1]\n",
    "\n",
    "        # Define full paths for both files\n",
    "        first_path = os.path.join(source_directory, first_file)\n",
    "        second_path = os.path.join(source_directory, second_file)\n",
    "\n",
    "        # Calculate flow vectors\n",
    "        flow_vectors = calculate_deepflow(first_path, second_path, variable_key=\"fai_anomaly\")\n",
    "        # flow_vectors_m = calculate_deepflow(first_path, second_path, variable_key=\"masked_land\")\n",
    "        flow_vectors_f = calculate_deepflow(first_path, second_path, variable_key=\"filtered\")\n",
    "\n",
    "        # Define the output path for the processed NetCDF file\n",
    "        output_filename = 'DeepFlow_' + first_file\n",
    "        output_path = os.path.join(destination_directory, output_filename)\n",
    "\n",
    "        # Save flow data\n",
    "        save_flow(first_path, output_path, lat_range=None, lon_range=None, flow_vectors=flow_vectors, flow_vectors_m=None, flow_vectors_f=flow_vectors_f,  mask_data=False) \n",
    "        print(f\"Processed and saved flow data for {first_file} to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db627384-5d28-4659-ad0a-ca85f94e6d04",
   "metadata": {},
   "source": [
    "Sequential (1 month, flow+filtered_flow): 1 hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baea4026-63cb-44dd-ba08-75b2af8572cd",
   "metadata": {},
   "source": [
    "## DeepFlow (Masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e2995-8071-44f1-9a2c-568b5fa3de7f",
   "metadata": {},
   "source": [
    "### *mask_flow*\n",
    "This takes in the images for which flow is already calculated and sets flow to 0 in pixels where there is no algae detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd90e2-1075-4a0a-ad05-71f43323df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_flow(data, output_path):\n",
    "    mask = data['fai_anomaly'] != 0\n",
    "    for var in data.data_vars:\n",
    "        # Check if there are filtered flow variables\n",
    "        if ('flow_u_f' in var) or ('flow_v_f' in var):\n",
    "            mask_filtered = data['filtered'] != 0\n",
    "            data[var] = xr.where(mask_filtered, data[var], 0)\n",
    "        if 'flow' in var:\n",
    "            data[var] = xr.where(mask, data[var], 0)\n",
    "    # Save the modified dataset to a new NetCDF file\n",
    "    data.to_netcdf(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf5e515-9fb6-4a35-bdec-8cb752b4fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking\n",
    "if __name__ == '__main__':\n",
    "    # Source directory with DeepFlow data\n",
    "    source_directory = '/media/yahia/ballena/Flow/DeepFlow'\n",
    "    destination_directory = '/media/yahia/ballena/Flow/DeepFlow_Masked'\n",
    "\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(destination_directory, exist_ok=True)\n",
    "\n",
    "    # Process each .nc file in the source directory\n",
    "    for filename in os.listdir(source_directory):\n",
    "        if filename.endswith('.nc'):\n",
    "            file_path = os.path.join(source_directory, filename)\n",
    "            output_path = os.path.join(destination_directory, 'Masked_' + filename)\n",
    "\n",
    "            # Load the dataset\n",
    "            data = xr.open_dataset(file_path)\n",
    "\n",
    "            # Mask the flow data and save the modified file\n",
    "            mask_flow(data, output_path)\n",
    "\n",
    "        print(f\"Masked flow data for {filename} to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcaae04-8ce5-41f5-afdb-a77ef032b4fd",
   "metadata": {},
   "source": [
    "## Farneback\n",
    "This is much faster than DeepFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e95fc88-c71a-4ccc-a9e8-def527be8ff1",
   "metadata": {},
   "source": [
    "### *process_file_pair_farneback*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a10a3-236d-4332-81ec-61432ad653a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_pair_farneback(file_paths, destination_directory):\n",
    "    first_path, second_path = file_paths\n",
    "    # Calculate flow vectors for different variables\n",
    "    flow_vectors = calculate_farneback(first_path, second_path, variable_key=\"fai_anomaly\")\n",
    "    flow_vectors_f = calculate_farneback(first_path, second_path, variable_key=\"filtered\")\n",
    "\n",
    "    # Define the output filename and path\n",
    "    first_file = os.path.basename(first_path)\n",
    "    output_filename = 'Farneback_' + first_file\n",
    "    output_path = os.path.join(destination_directory, output_filename)\n",
    "\n",
    "    # Save flow data\n",
    "    save_flow(first_path, output_path, lat_range=None, lon_range=None, flow_vectors=flow_vectors, flow_vectors_m=None, flow_vectors_f=flow_vectors_f, mask_data=False)\n",
    "    print(f\"Processed and saved flow data for {first_file} to {output_filename}\")\n",
    "    return output_filename  # Return something to signify completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9faf8ac-38d5-46d9-9b2d-1de22ccba45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Paths\n",
    "    source_directory = '/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered'\n",
    "    destination_directory = '/media/yahia/ballena/Flow/Farneback'\n",
    "\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(destination_directory, exist_ok=True)\n",
    "\n",
    "    # Get all .nc files sorted to ensure chronological order\n",
    "    files = sorted([f for f in os.listdir(source_directory) if f.endswith('.nc')])\n",
    "\n",
    "    # Prepare pairs of files for processing\n",
    "    file_pairs = [(os.path.join(source_directory, files[i]), os.path.join(source_directory, files[i+1])) for i in range(len(files) - 1)]\n",
    "\n",
    "    # Adjust the number of max_workers based on your system capabilities\n",
    "    max_workers = os.cpu_count() // 2  # For example, use half of your CPU cores\n",
    "\n",
    "    # Use ProcessPoolExecutor to execute multiple processes\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Map the helper function to each pair of files\n",
    "        futures = [executor.submit(process_file_pair_farneback, pair, destination_directory) for pair in file_pairs]\n",
    "\n",
    "        # Optionally handle results as they complete\n",
    "        for future in futures:\n",
    "            try:\n",
    "                print(future.result())  # Accessing result() will wait for the process to complete\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd4acbf-da63-42b2-83a6-a6534edb554b",
   "metadata": {},
   "source": [
    "## Farneback (Masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb0be7e-1ecb-4a73-8670-b57f230cfbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking\n",
    "if __name__ == '__main__':\n",
    "    # Source directory with DeepFlow data\n",
    "    source_directory = '/media/yahia/ballena/Flow/Farneback'\n",
    "    destination_directory = '/media/yahia/ballena/Flow/Farneback_Masked'\n",
    "\n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(destination_directory, exist_ok=True)\n",
    "\n",
    "    # Process each .nc file in the source directory\n",
    "    for filename in os.listdir(source_directory):\n",
    "        if filename.endswith('.nc'):\n",
    "            file_path = os.path.join(source_directory, filename)\n",
    "            output_path = os.path.join(destination_directory, 'Masked_' + filename)\n",
    "\n",
    "            # Load the dataset\n",
    "            data = xr.open_dataset(file_path)\n",
    "\n",
    "            # Mask the flow data and save the modified file\n",
    "            mask_flow(data, output_path)\n",
    "\n",
    "        print(f\"Masked flow data for {filename} to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0a78c2-841c-4e22-a3b2-612f69c4900c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
