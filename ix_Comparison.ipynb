{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea458dd-4de5-4f05-8490-13406ecbbc07",
   "metadata": {},
   "source": [
    "# Comparison\n",
    "The point of this notebook is to compare the results of our algorithms to the glorys12 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2810faf5-4fc1-4d56-8975-a921fb48ad6c",
   "metadata": {},
   "source": [
    "## Importing necessary libraries and notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "530bdef0-1952-453a-afa0-de1b3906095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import io\n",
    "import os\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import netCDF4 as nc\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import ticker\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from matplotlib.collections import LineCollection\n",
    "from IPython.display import Image, display, clear_output\n",
    "from PIL import Image \n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# Import the other notebooks without running their cells\n",
    "from ii_Data_Manipulation import visualize_4\n",
    "from iii_GOES_average import time_list, visualize_aggregate, calculate_median\n",
    "from iv_Image_Processing import collect_times, crop_image, save_aggregate, binarize_image, bilateral_image, process_dates, process_directory\n",
    "from vii_Flow_Analysis import haversine\n",
    "from v_i_OF_Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ee3d7-e306-4c36-8a97-e124dfb1c571",
   "metadata": {},
   "source": [
    "## *compare_flows*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4112df16-c04a-41f0-8df1-a8429a8d7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_flows(my_dataset_path, output_path, glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc', comparison_date='2022-07-23'):\n",
    "    \"\"\"\n",
    "    Compares flow vectors from a custom dataset with ocean currents from the GLORYS12 dataset for a specific date\n",
    "    and saves the differences. The custom dataset is downsampled to match the GLORYS dataset grid.\n",
    "    \n",
    "    Parameters:\n",
    "    - my_dataset_path (str): Path to the custom NetCDF file with flow variables.\n",
    "    - glorys_dataset_path (str): Path to the GLORYS12 ocean currents NetCDF file.\n",
    "    - output_path (str): Path to save the resulting differences as a NetCDF file.\n",
    "    - comparison_date (str): Specific date to extract data for comparison (default '2022-07-23').\n",
    "    \"\"\"\n",
    "    # Load the custom and GLORYS datasets\n",
    "    my_data = xr.open_dataset(my_dataset_path)\n",
    "    glorys_data = xr.open_dataset(glorys_dataset_path)\n",
    "    \n",
    "    # Extract data for the specific date and remove singleton time dimensions\n",
    "    current_u = glorys_data['uo'].sel(time=comparison_date).squeeze()\n",
    "    current_v = glorys_data['vo'].sel(time=comparison_date).squeeze()\n",
    "    \n",
    "    # Optional: Subset GLORYS data to the extent of my_data to ensure proper overlap\n",
    "    current_u = current_u.sel(latitude=slice(my_data.latitude.min(), my_data.latitude.max()),\n",
    "                              longitude=slice(my_data.longitude.min(), my_data.longitude.max()))\n",
    "    current_v = current_v.sel(latitude=slice(my_data.latitude.min(), my_data.latitude.max()),\n",
    "                              longitude=slice(my_data.longitude.min(), my_data.longitude.max()))\n",
    "    \n",
    "    # Downsample my_data to match the GLORYS data grid\n",
    "    my_data_downsampled = my_data.reindex_like(current_u, method='nearest')\n",
    "\n",
    "    # Compute differences for each set of flow variables\n",
    "    d_flow_u = abs(my_data_downsampled['flow_u'] - current_u)\n",
    "    d_flow_v = abs(my_data_downsampled['flow_v'] - current_v)\n",
    "    d_flow_u_m = abs(my_data_downsampled['flow_u_m'] - current_u)\n",
    "    d_flow_v_m = abs(my_data_downsampled['flow_v_m'] - current_v)\n",
    "    d_flow_u_f = abs(my_data_downsampled['flow_u_f'] - current_u)\n",
    "    d_flow_v_f = abs(my_data_downsampled['flow_v_f'] - current_v)\n",
    "    \n",
    "    # Create a new dataset to store the differences\n",
    "    diff_dataset = xr.Dataset({\n",
    "        'd_flow_u': d_flow_u,\n",
    "        'd_flow_v': d_flow_v,\n",
    "        'd_flow_u_m': d_flow_u_m,\n",
    "        'd_flow_v_m': d_flow_v_m,\n",
    "        'd_flow_u_f': d_flow_u_f,\n",
    "        'd_flow_v_f': d_flow_v_f\n",
    "    })\n",
    "    \n",
    "    # Save the differences dataset to a NetCDF file\n",
    "    diff_dataset.to_netcdf(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2f1c80-c0a3-499f-8eee-28e7ba20a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFlow\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        output_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/difference_flow.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ea8b3-7057-4a0d-9b8d-0fdaf35b4da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Farneback\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        output_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/difference_farneback.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5f81a-6ab3-4eb4-8219-7ab1006b6695",
   "metadata": {},
   "source": [
    "## *compare_flows_scatter*\n",
    "Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dcaf75-abe3-464a-a0b1-d4dac5787cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_flows_scatter(my_dataset_path, glorys_dataset_path, comparison_date='2022-07-23', \n",
    "                                     flow_u_name='flow_u', flow_v_name='flow_v'):\n",
    "    # Load datasets\n",
    "    my_data = xr.open_dataset(my_dataset_path)\n",
    "    glorys_data = xr.open_dataset(glorys_dataset_path)\n",
    "\n",
    "    # Select specific date and squeeze out singletons for both u and v components from GLORYS dataset\n",
    "    glorys_u = glorys_data['uo'].sel(time=comparison_date).squeeze()\n",
    "    glorys_v = glorys_data['vo'].sel(time=comparison_date).squeeze()\n",
    "    \n",
    "    # Extract the custom flow components using the provided names\n",
    "    my_flow_u = my_data[flow_u_name]\n",
    "    my_flow_v = my_data[flow_v_name]\n",
    "\n",
    "    # Output minimum and maximum values for diagnostics\n",
    "    print(f\"Min/Max {flow_u_name}: {my_flow_u.min().data}, {my_flow_u.max().data}\")\n",
    "    print(f\"Min/Max {flow_v_name}: {my_flow_v.min().data}, {my_flow_v.max().data}\")\n",
    "\n",
    "    # Restrict both datasets to the common spatial extent (for safety)\n",
    "    common_lat_min = max(min(my_flow_u.latitude), min(glorys_u.latitude))\n",
    "    common_lat_max = min(max(my_flow_u.latitude), max(glorys_u.latitude))\n",
    "    common_lon_min = max(min(my_flow_u.longitude), min(glorys_u.longitude))\n",
    "    common_lon_max = min(max(my_flow_u.longitude), max(glorys_u.longitude))\n",
    "\n",
    "    # Restrict both datasets to the common spatial extent\n",
    "    my_flow_u = my_flow_u.sel(latitude=slice(common_lat_min, common_lat_max), longitude=slice(common_lon_min, common_lon_max))\n",
    "    my_flow_v = my_flow_v.sel(latitude=slice(common_lat_min, common_lat_max), longitude=slice(common_lon_min, common_lon_max))\n",
    "    glorys_u = glorys_u.sel(latitude=slice(common_lat_min, common_lat_max), longitude=slice(common_lon_min, common_lon_max))\n",
    "    glorys_v = glorys_v.sel(latitude=slice(common_lat_min, common_lat_max), longitude=slice(common_lon_min, common_lon_max))\n",
    "    \n",
    "    # Downsample my dataset to match GLORYS grid using reindex_like for both u and v components\n",
    "    my_flow_u_downsampled = my_flow_u.reindex_like(glorys_u, method='nearest')\n",
    "    my_flow_v_downsampled = my_flow_v.reindex_like(glorys_v, method='nearest')\n",
    "\n",
    "    # Flatten the data to 1D arrays for statistical analysis\n",
    "    glorys_u_flat = glorys_u.values.flatten()\n",
    "    my_flow_u_flat = my_flow_u_downsampled.values.flatten()\n",
    "    glorys_v_flat = glorys_v.values.flatten()\n",
    "    my_flow_v_flat = my_flow_v_downsampled.values.flatten()\n",
    "\n",
    "    # Clean non-finite values from data arrays\n",
    "    valid_indices_u = np.isfinite(glorys_u_flat) & np.isfinite(my_flow_u_flat)\n",
    "    valid_indices_v = np.isfinite(glorys_v_flat) & np.isfinite(my_flow_v_flat)\n",
    "    glorys_u_flat = glorys_u_flat[valid_indices_u]\n",
    "    my_flow_u_flat = my_flow_u_flat[valid_indices_u]\n",
    "    glorys_v_flat = glorys_v_flat[valid_indices_v]\n",
    "    my_flow_v_flat = my_flow_v_flat[valid_indices_v]\n",
    "\n",
    "    # Calculate correlation for both u and v components\n",
    "    if len(glorys_u_flat) == len(my_flow_u_flat) and len(glorys_v_flat) == len(my_flow_v_flat):\n",
    "        correlation_u, _ = scipy.stats.pearsonr(glorys_u_flat, my_flow_u_flat)\n",
    "        correlation_v, _ = scipy.stats.pearsonr(glorys_v_flat, my_flow_v_flat)\n",
    "        print(\"Correlation coefficient for u:\", correlation_u)\n",
    "        print(\"Correlation coefficient for v:\", correlation_v)\n",
    "    else:\n",
    "        print(\"Data arrays do not match in size or are empty after cleaning.\")\n",
    "\n",
    "    # Plotting for u component\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(glorys_u_flat, my_flow_u_flat, alpha=0.5)\n",
    "    plt.title(f'Scatter Plot of GLORYS uo vs. {flow_u_name}')\n",
    "    plt.xlabel('GLORYS uo (m/s)')\n",
    "    plt.ylabel(f'{flow_u_name} (m/s)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting for v component\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(glorys_v_flat, my_flow_v_flat, alpha=0.5)\n",
    "    plt.title(f'Scatter Plot of GLORYS vo vs. {flow_v_name}')\n",
    "    plt.xlabel('GLORYS vo (m/s)')\n",
    "    plt.ylabel(f'{flow_v_name} (m/s)')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25c68e0-876c-48cc-8429-bc25f9d8ca45",
   "metadata": {},
   "source": [
    "## DeepFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a6543-ca0c-443e-b046-ae94b59f762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFlow\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e72cd82-2a71-4c29-885c-3dfb37298f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked DeepFlow\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_m\", flow_v_name=\"flow_v_m\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f48ac-2bf4-401c-868f-cf823e730dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered DeepFlow\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_f\", flow_v_name=\"flow_v_f\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3358e9d-fed2-4124-8975-7fab06d1533b",
   "metadata": {},
   "source": [
    "## DeepFlow (Masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e28629-0acb-400e-bea5-7bf60aadbad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFlow\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow_masked.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff436c-7ff4-47db-ad69-d95cb2472553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked DeepFlow\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow_masked.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_m\", flow_v_name=\"flow_v_m\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2330c1-7aaf-4938-949a-3af74f5c6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered DeepFlow\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow_masked.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_f\", flow_v_name=\"flow_v_f\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c743e981-1045-4958-97eb-39f56e0d07d1",
   "metadata": {},
   "source": [
    "## Farneback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d00ef43-e48e-4644-b4b8-2e98c5e80f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Farneback\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd9824-e9da-48d4-acfb-c9f000dea53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Farneback\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_m\", flow_v_name=\"flow_v_m\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193229d2-d0e5-4942-b7fd-1ef149299478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered Farneback\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_f\", flow_v_name=\"flow_v_f\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79398ee9-3150-4f4e-82ce-7fda478b3239",
   "metadata": {},
   "source": [
    "## Farneback (Masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a730c20-94c4-4569-be9e-992fedbdbf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Farneback\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback_masked.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165104ea-1b44-4e0f-b66b-e1c7515057dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Farneback\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback_masked.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_m\", flow_v_name=\"flow_v_m\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821afb6d-af0e-46c1-94b6-8731c7c9c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered Farneback\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback_masked.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_f\", flow_v_name=\"flow_v_f\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c4b8bb-63ab-4c68-a6b6-16e62ae1ef0a",
   "metadata": {},
   "source": [
    "## Overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7836a23-d5b0-4b54-a55b-e547e5a4f031",
   "metadata": {},
   "source": [
    "### Overlay (NetCDF)\n",
    "For every pair of days, we overlay the detections of the second day (as NaN) on the Masked Flow NetCDF that contains the flow values applied on the detections of the first day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8705010a-5a11-4d66-b003-b7e865e35a9d",
   "metadata": {},
   "source": [
    "#### *overlay_detections*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b68f200-b2af-45a4-9856-6f99a0794c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_detections(flow_data_path, detection_data_path, output_path):\n",
    "    \"\"\"\n",
    "    Load the flow data of the first day and the detection data of the second day.\n",
    "    Apply the detection mask from the second day onto the flow data of the first day.\n",
    "    \"\"\"\n",
    "    # Load the flow data from the first day\n",
    "    flow_data = xr.open_dataset(flow_data_path)\n",
    "    \n",
    "    # Load the detection data from the second day\n",
    "    detection_data = xr.open_dataset(detection_data_path)\n",
    "    \n",
    "    # Assume 'fai_anomaly' indicates the presence of detections, convert to mask\n",
    "    detection_mask = detection_data['fai_anomaly'] != 0\n",
    "    \n",
    "    # Overlay the detection mask onto the flow data\n",
    "    for var in flow_data.data_vars:\n",
    "        if 'flow' in var:  # Only modify flow variables\n",
    "            # Set flow data to NaN where there are detections on the second day\n",
    "            flow_data[var] = xr.where(detection_mask, np.nan, flow_data[var])\n",
    "    \n",
    "    # Save the modified flow data\n",
    "    flow_data.to_netcdf(output_path)\n",
    "    print(f\"Modified flow data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4530414b-9246-47f1-b1d1-c34d4ea41acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFlow Overlay\n",
    "if __name__ == '__main__':\n",
    "    # Paths\n",
    "    source_directory = '/media/yahia/ballena/Flow/DeepFlow_Masked'\n",
    "    output_directory = '/media/yahia/ballena/Flow/DeepFlow_Overlay'\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # List of files sorted to ensure chronological order\n",
    "    files = sorted([f for f in os.listdir(source_directory) if f.endswith('.nc')])\n",
    "    \n",
    "    # Process each pair of consecutive files\n",
    "    for i in range(len(files) - 1):\n",
    "        first_file = os.path.join(source_directory, files[i])\n",
    "        second_file = os.path.join(source_directory, files[i + 1])\n",
    "        \n",
    "        # Define the output path for the processed NetCDF file\n",
    "        output_path = os.path.join(output_directory, f'Overlay_{files[i]}')\n",
    "        \n",
    "        # Overlay the detections from the second day onto the flow data from the first day\n",
    "        overlay_detections(first_file, second_file, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a94ca42-7388-4c83-b521-5c2d9f1fb91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220701.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220702.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220703.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220704.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220705.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220706.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220707.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220708.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220709.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220710.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220712.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220713.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220714.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220716.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220717.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220718.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220719.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220720.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220721.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220722.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220723.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220724.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220725.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220726.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220727.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220728.nc\n",
      "Modified flow data saved to /media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220729.nc\n"
     ]
    }
   ],
   "source": [
    "# Farneback Overlay\n",
    "if __name__ == '__main__':\n",
    "    # Paths\n",
    "    source_directory = '/media/yahia/ballena/Flow/Farneback_Masked'\n",
    "    output_directory = '/media/yahia/ballena/Flow/Farneback_Overlay'\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # List of files sorted to ensure chronological order\n",
    "    files = sorted([f for f in os.listdir(source_directory) if f.endswith('.nc')])\n",
    "    \n",
    "    # Process each pair of consecutive files\n",
    "    for i in range(len(files) - 1):\n",
    "        first_file = os.path.join(source_directory, files[i])\n",
    "        second_file = os.path.join(source_directory, files[i + 1])\n",
    "        \n",
    "        # Define the output path for the processed NetCDF file\n",
    "        output_path = os.path.join(output_directory, f'Overlay_{files[i]}')\n",
    "        \n",
    "        # Overlay the detections from the second day onto the flow data from the first day\n",
    "        overlay_detections(first_file, second_file, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cf745d-2d59-4b5b-a120-f0d07695c760",
   "metadata": {},
   "source": [
    "### Overlay (PNG)\n",
    "This second approach uses the result of the previous overlay function and plots corresponding flow vectors on a PNG image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1d24b5-89c9-4dd9-90db-ad3a2b430c7b",
   "metadata": {},
   "source": [
    "#### *overlay_png*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "309a25a7-5142-4baa-8404-5f51bc756f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_png(file_path, output_path, quiver_scale=100, quiver_step=50, lat_range=None, lon_range=None):\n",
    "    \"\"\"\n",
    "    Overlays flow vectors on the geographic map of a NetCDF file and saves it as a high-resolution PNG,\n",
    "    using flow_u or flow_v data to determine the overlay and base colors. Includes options for slicing by latitude and longitude.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: Path to the NetCDF file.\n",
    "    - output_path: Path to save the PNG image.\n",
    "    - quiver_scale: Scaling factor for vectors to adjust their length.\n",
    "    - quiver_step: Sampling rate for vectors to avoid overcrowding the plot.\n",
    "    - lat_range: Tuple of (min_lat, max_lat) for latitude slicing.\n",
    "    - lon_range: Tuple of (min_lon, max_lon) for longitude slicing.\n",
    "    \"\"\"\n",
    "    # Load data from NetCDF\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Slice the dataset if latitude and longitude ranges are provided\n",
    "    if lat_range:\n",
    "        ds = ds.sel(latitude=slice(*lat_range))\n",
    "    if lon_range:\n",
    "        ds = ds.sel(longitude=slice(*lon_range))\n",
    "\n",
    "    u = ds['flow_u']\n",
    "    v = ds['flow_v']\n",
    "\n",
    "    # Create the figure and axes with a geographic projection\n",
    "    fig, ax = plt.subplots(figsize=(15, 10), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    ax.coastlines()  # Add coastlines for reference\n",
    "\n",
    "    # Create color mapping based on flow_u data\n",
    "    # Green where flow_u is not zero and not NaN, yellow where NaN\n",
    "    color = np.where(np.isnan(u), 'yellow', np.where(u != 0, 'green', 'none'))\n",
    "\n",
    "    # Create a meshgrid for the longitude and latitude\n",
    "    lon, lat = np.meshgrid(ds.longitude, ds.latitude)\n",
    "\n",
    "    # Scatter plot for color visualization\n",
    "    ax.scatter(lon.flatten(), lat.flatten(), color=color.flatten(), s=1, transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Mask the vector fields where flow_u is not zero and not NaN\n",
    "    mask = (u != 0) & np.isfinite(u)\n",
    "    U = u.where(mask)\n",
    "    V = v.where(mask)\n",
    "\n",
    "    # Quiver plot for the vector field\n",
    "    ax.quiver(lon[::quiver_step, ::quiver_step], lat[::quiver_step, ::quiver_step],\n",
    "              U[::quiver_step, ::quiver_step], V[::quiver_step, ::quiver_step],\n",
    "              scale=quiver_scale, color='red', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Set title and save the plot\n",
    "    ax.set_title('Overlay of Flow Vectors on Detection Data', fontsize=15)\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)  # Close the figure to free resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37ecf178-0181-4de9-ac2e-faca70168f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFlow Atlantic\n",
    "if __name__ == \"__main__\":\n",
    "    overlay_png(file_path='/media/yahia/ballena/Flow/DeepFlow_Overlay/Overlay_Masked_DeepFlow_Filtered_algae_distribution_20220723.nc',\n",
    "                output_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Overlay/DeepFlow_Overlay.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aa0cb5a-5d51-49e6-a6ef-5e545addb401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFlow Antilles\n",
    "if __name__ == \"__main__\":\n",
    "    overlay_png(file_path='/media/yahia/ballena/Flow/DeepFlow_Overlay/Overlay_Masked_DeepFlow_Filtered_algae_distribution_20220723.nc',\n",
    "                output_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Overlay/DeepFlow_Overlay_Antilles.png',\n",
    "                lat_range = (12, 17) , lon_range = (-67, -60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "545e461a-9efb-4c86-a531-87002ae1e85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Farneback Atlantic\n",
    "if __name__ == \"__main__\":\n",
    "    overlay_png(file_path='/media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220723.nc',\n",
    "                output_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Overlay/Farneback_Overlay.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f17fe7a-2e65-42d2-8baa-de03fb9ea75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Farneback Antilles\n",
    "if __name__ == \"__main__\":\n",
    "    overlay_png(file_path='/media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220723.nc',\n",
    "                output_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Overlay/Farneback_Overlay_Antilles.png',\n",
    "                lat_range = (12, 17) , lon_range = (-67, -60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89ef7c-82ae-4be2-99bf-161f904385ba",
   "metadata": {},
   "source": [
    "#### *visualize_comparative_flow*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fbd095bd-c9ff-4142-a154-d7fc71eb0902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_comparative_flow(deepflow_path, farneback_path, output_path, lat_range=None, lon_range=None, quiver_scale=100, quiver_step=50):\n",
    "    # Load datasets\n",
    "    deepflow_data = xr.open_dataset(deepflow_path)\n",
    "    farneback_data = xr.open_dataset(farneback_path)\n",
    "\n",
    "    # Slice the dataset if latitude and longitude ranges are provided\n",
    "    if lat_range:\n",
    "        deepflow_data = deepflow_data.sel(latitude=slice(*lat_range))\n",
    "        farneback_data = farneback_data.sel(latitude=slice(*lat_range))\n",
    "    if lon_range:\n",
    "        deepflow_data = deepflow_data.sel(longitude=slice(*lon_range))\n",
    "        farneback_data = farneback_data.sel(longitude=slice(*lon_range))\n",
    "\n",
    "    # Extract flow vectors and detection mask\n",
    "    u_deep, v_deep = deepflow_data['flow_u'], deepflow_data['flow_v']\n",
    "    u_far, v_far = farneback_data['flow_u'], farneback_data['flow_v']\n",
    "    detection_mask1 = deepflow_data['fai_anomaly'] != 0\n",
    "    detection_mask2 = np.isnan(deepflow_data['flow_u'])\n",
    "\n",
    "    # Setup the plot with geographic projection\n",
    "    fig, ax = plt.subplots(figsize=(15, 10), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    ax.coastlines()\n",
    "\n",
    "    # Scatter plot for detections\n",
    "    lon, lat = np.meshgrid(deepflow_data.longitude, deepflow_data.latitude)\n",
    "    ax.scatter(lon[detection_mask1], lat[detection_mask1], color='cyan', s=1, label='First Day Detections', transform=ccrs.PlateCarree())\n",
    "    ax.scatter(lon[detection_mask2], lat[detection_mask2], color='orange', s=1, label='Second Day Detections', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Quiver plots for flow vectors\n",
    "    deepflow_quiver = ax.quiver(lon[::quiver_step, ::quiver_step], lat[::quiver_step, ::quiver_step],\n",
    "                                u_deep.values[::quiver_step, ::quiver_step], v_deep.values[::quiver_step, ::quiver_step],\n",
    "                                color='red', scale=quiver_scale, label='DeepFlow Vectors', transform=ccrs.PlateCarree())\n",
    "    farneback_quiver = ax.quiver(lon[::quiver_step, ::quiver_step], lat[::quiver_step, ::quiver_step],\n",
    "                                 u_far.values[::quiver_step, ::quiver_step], v_far.values[::quiver_step, ::quiver_step],\n",
    "                                 color='blue', scale=quiver_scale, label='Farneback Vectors', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Add a quiver key\n",
    "    ax.quiverkey(deepflow_quiver, X=0.5, Y=0.05, U=0.1,\n",
    "                 label='0.1 m/s Flow Vector', labelpos='E', color='red')\n",
    "\n",
    "    # Adding legend and title\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_title('Comparative Visualization of Flow Vectors and Detections', fontsize=15)\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36e74899-54f1-4d45-ad9c-a7bf5375fcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yahia/anaconda3/lib/python3.11/site-packages/cartopy/mpl/geoaxes.py:525: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  return super().draw(renderer=renderer, **kwargs)\n",
      "/home/yahia/anaconda3/lib/python3.11/site-packages/cartopy/mpl/geoaxes.py:525: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  return super().draw(renderer=renderer, **kwargs)\n",
      "/home/yahia/anaconda3/lib/python3.11/site-packages/cartopy/mpl/geoaxes.py:499: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  return super().get_tightbbox(renderer, *args, **kwargs)\n",
      "/home/yahia/anaconda3/lib/python3.11/site-packages/cartopy/mpl/geoaxes.py:525: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  return super().draw(renderer=renderer, **kwargs)\n",
      "/home/yahia/anaconda3/lib/python3.11/site-packages/cartopy/mpl/geoaxes.py:525: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  return super().draw(renderer=renderer, **kwargs)\n",
      "/home/yahia/anaconda3/lib/python3.11/site-packages/cartopy/mpl/geoaxes.py:525: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  return super().draw(renderer=renderer, **kwargs)\n",
      "/home/yahia/anaconda3/lib/python3.11/site-packages/cartopy/mpl/geoaxes.py:525: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  return super().draw(renderer=renderer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Atlantic\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_comparative_flow(\n",
    "        deepflow_path='/media/yahia/ballena/Flow/DeepFlow_Overlay/Overlay_Masked_DeepFlow_Filtered_algae_distribution_20220723.nc',\n",
    "        farneback_path='/media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220723.nc',\n",
    "        output_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Overlay/Double_Overlay.png',\n",
    "        lat_range=None, lon_range=None, quiver_scale=5, quiver_step=20\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "727d3e30-647a-47b3-910f-fbd82e97d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antilles\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_comparative_flow(\n",
    "        deepflow_path='/media/yahia/ballena/Flow/DeepFlow_Overlay/Overlay_Masked_DeepFlow_Filtered_algae_distribution_20220723.nc',\n",
    "        farneback_path='/media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220723.nc',\n",
    "        output_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Overlay/Double_Overlay_Antilles.png',\n",
    "        lat_range=(12,17), lon_range=(-67,-60), quiver_scale=3, quiver_step=20\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cd17d7-c979-40db-9b02-0fe6b1213897",
   "metadata": {},
   "source": [
    "#### *visualize_comparative_flow_filtered*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d2d51661-e75c-4811-846d-838c6b206c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_comparative_flow_filtered(deepflow_path, farneback_path, output_path, lat_range=None, lon_range=None, quiver_scale=100, quiver_step=50):\n",
    "    # Load datasets\n",
    "    deepflow_data = xr.open_dataset(deepflow_path)\n",
    "    farneback_data = xr.open_dataset(farneback_path)\n",
    "\n",
    "    # Slice the dataset if latitude and longitude ranges are provided\n",
    "    if lat_range:\n",
    "        deepflow_data = deepflow_data.sel(latitude=slice(*lat_range))\n",
    "        farneback_data = farneback_data.sel(latitude=slice(*lat_range))\n",
    "    if lon_range:\n",
    "        deepflow_data = deepflow_data.sel(longitude=slice(*lon_range))\n",
    "        farneback_data = farneback_data.sel(longitude=slice(*lon_range))\n",
    "\n",
    "    # Extract flow vectors and detection mask\n",
    "    u_deep, v_deep = deepflow_data['flow_u_f'], deepflow_data['flow_v_f']\n",
    "    u_far, v_far = farneback_data['flow_u_f'], farneback_data['flow_v_f']\n",
    "    detection_mask1 = deepflow_data['filtered'] != 0\n",
    "    detection_mask2 = np.isnan(deepflow_data['flow_u_f'])\n",
    "\n",
    "    # Setup the plot with geographic projection\n",
    "    fig, ax = plt.subplots(figsize=(15, 10), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    ax.coastlines()\n",
    "\n",
    "    # Scatter plot for detections\n",
    "    lon, lat = np.meshgrid(deepflow_data.longitude, deepflow_data.latitude)\n",
    "    ax.scatter(lon[detection_mask1], lat[detection_mask1], color='cyan', s=1, label='First Day Detections', transform=ccrs.PlateCarree())\n",
    "    ax.scatter(lon[detection_mask2], lat[detection_mask2], color='orange', s=1, label='Second Day Detections', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Quiver plots for flow vectors\n",
    "    deepflow_quiver = ax.quiver(lon[::quiver_step, ::quiver_step], lat[::quiver_step, ::quiver_step],\n",
    "                                u_deep.values[::quiver_step, ::quiver_step], v_deep.values[::quiver_step, ::quiver_step],\n",
    "                                color='red', scale=quiver_scale, label='DeepFlow Vectors', transform=ccrs.PlateCarree())\n",
    "    farneback_quiver = ax.quiver(lon[::quiver_step, ::quiver_step], lat[::quiver_step, ::quiver_step],\n",
    "                                 u_far.values[::quiver_step, ::quiver_step], v_far.values[::quiver_step, ::quiver_step],\n",
    "                                 color='blue', scale=quiver_scale, label='Farneback Vectors', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Add a quiver key\n",
    "    ax.quiverkey(deepflow_quiver, X=0.5, Y=0.05, U=0.1,\n",
    "                 label='0.1 m/s Flow Vector', labelpos='E', color='red')\n",
    "\n",
    "    # Adding legend and title\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_title('Comparative Visualization of Flow Vectors and Detections', fontsize=15)\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3eb2a01b-d99e-4246-8a8c-2bf004eb4693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antilles Filtered\n",
    "if __name__ == \"__main__\":\n",
    "    visualize_comparative_flow_filtered(\n",
    "        deepflow_path='/media/yahia/ballena/Flow/DeepFlow_Overlay/Overlay_Masked_DeepFlow_Filtered_algae_distribution_20220723.nc',\n",
    "        farneback_path='/media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220723.nc',\n",
    "        output_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Overlay/Double_Overlay_Antilles_Filtered.png',\n",
    "        lat_range=(12,17), lon_range=(-67,-60), quiver_scale=3, quiver_step=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c09d368-3cb1-4c1b-a571-1c189fa31b72",
   "metadata": {},
   "source": [
    "#### *stitch_images*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "22db714d-41ff-48c8-9b7a-c5d6e39f0519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stitch_images(directory, output_path):\n",
    "#     \"\"\"\n",
    "#     Stitches all images in a directory into a single image, assuming filenames contain latitude and longitude.\n",
    "#     \"\"\"\n",
    "#     files = os.listdir(directory)\n",
    "#     files = [f for f in files if f.endswith('.png')]\n",
    "#     files = sort_files_by_coordinates(files)\n",
    "\n",
    "#     images = [Image.open(os.path.join(directory, file)) for file in files]\n",
    "    \n",
    "#     # Assuming all images have the same dimensions\n",
    "#     widths, heights = zip(*(i.size for i in images))\n",
    "    \n",
    "#     total_width = sum(widths)\n",
    "#     max_height = max(heights)\n",
    "\n",
    "#     # New image with summed width and max height of the individual images\n",
    "#     new_image = Image.new('RGB', (total_width, max_height))\n",
    "\n",
    "#     x_offset = 0\n",
    "#     for img in images:\n",
    "#         new_image.paste(img, (x_offset, 0))\n",
    "#         x_offset += img.width\n",
    "\n",
    "#     new_image.save(output_path)\n",
    "#     print(f\"Stitched image saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ccd45120-c130-4c4c-803e-4a6f5d99f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_images(image_directory, output_path, lat_partitions, lon_partitions):\n",
    "    \"\"\"\n",
    "    Stitch images in a grid based on their latitude and longitude range filenames,\n",
    "    starting from the bottom right to the left and then upwards.\n",
    "    \"\"\"\n",
    "    # Load all images and sort them by latitude then by longitude in descending order\n",
    "    images = []\n",
    "    for lat in reversed(lat_partitions):\n",
    "        row_images = []\n",
    "        for lon in reversed(lon_partitions):\n",
    "            filename = f\"{lat[0]}_{lon[0]}.png\"\n",
    "            file_path = os.path.join(image_directory, filename)\n",
    "            if os.path.exists(file_path):\n",
    "                img = Image.open(file_path)\n",
    "                row_images.append(img)\n",
    "            else:\n",
    "                print(f\"Missing image: {filename}\")\n",
    "        if row_images:  # Only append if row is not empty\n",
    "            images.append(row_images)\n",
    "\n",
    "    # Determine the size of the composite image\n",
    "    total_width = sum(img.size[0] for img in images[0])  # Width of any row (assuming all rows are complete)\n",
    "    total_height = sum(row[0].size[1] for row in images)  # Sum of heights of all rows\n",
    "\n",
    "    # Create a new empty image to place the individual images\n",
    "    composite_image = Image.new('RGB', (total_width, total_height))\n",
    "    \n",
    "    # Paste images into the composite image\n",
    "    y_offset = 0\n",
    "    for row in images:\n",
    "        x_offset = 0\n",
    "        for img in row:\n",
    "            composite_image.paste(img, (x_offset, y_offset))\n",
    "            x_offset += img.size[0]\n",
    "        y_offset += row[0].size[1]  # Increment y-offset by the height of the current row\n",
    "\n",
    "    # Save the stitched image\n",
    "    composite_image.save(output_path)\n",
    "    # composite_image.save(\"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Overlay/partitioned.jpg\", 'JPEG', quality=50)  # Adjust quality as needed\n",
    "    print(f\"Stitched image saved at {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6971f94d-3ad8-4efc-9830-c4a247dcb6fb",
   "metadata": {},
   "source": [
    "#### *create_partitions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ea054fa3-d812-4319-a5bc-d3eafb4faa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create partitions of a given width over a specified range\n",
    "def create_partitions(start, end, step):\n",
    "    return [(i, min(i + step, end)) for i in range(start, end, step)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d9ad02f7-67db-4017-9a1b-a848849755a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing partitions\n",
    "if __name__ == \"__main__\":\n",
    "    latitude_partitions = create_partitions(12, 41, 5)  # from 12 to 40 inclusive\n",
    "    longitude_partitions = create_partitions(-100, -11, 7)  # from -100 to -12 inclusive\n",
    "    deepflow_path='/media/yahia/ballena/Flow/DeepFlow_Overlay/Overlay_Masked_DeepFlow_Filtered_algae_distribution_20220723.nc'\n",
    "    farneback_path='/media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220723.nc'\n",
    "    images = []\n",
    "    \n",
    "    for lat_range in latitude_partitions:\n",
    "        for lon_range in longitude_partitions:\n",
    "            output_path_partition = '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Overlay/Partition/'\n",
    "            visualize_comparative_flow(deepflow_path, farneback_path, f\"{output_path_partition}{lat_range[0]}_{lon_range[0]}.png\", \n",
    "                                       lat_range, lon_range,quiver_scale=3, quiver_step=20)\n",
    "            images.append(f\"{output_path_partition}{lat_range[0]}_{lon_range[0]}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5fe5bd3d-98b8-4635-8943-bfc9b7149999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing partitions (filtered)\n",
    "if __name__ == \"__main__\":\n",
    "    latitude_partitions = create_partitions(12, 41, 5)  # from 12 to 40 inclusive\n",
    "    longitude_partitions = create_partitions(-100, -11, 7)  # from -100 to -12 inclusive\n",
    "    deepflow_path='/media/yahia/ballena/Flow/DeepFlow_Overlay/Overlay_Masked_DeepFlow_Filtered_algae_distribution_20220723.nc'\n",
    "    farneback_path='/media/yahia/ballena/Flow/Farneback_Overlay/Overlay_Masked_Farneback_Filtered_algae_distribution_20220723.nc'\n",
    "    images = []\n",
    "    \n",
    "    for lat_range in latitude_partitions:\n",
    "        for lon_range in longitude_partitions:\n",
    "            output_path_partition = '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Overlay/Partition_Filtered/'\n",
    "            visualize_comparative_flow(deepflow_path, farneback_path, f\"{output_path_partition}{lat_range[0]}_{lon_range[0]}.png\", \n",
    "                                       lat_range, lon_range,quiver_scale=3, quiver_step=20)\n",
    "            images.append(f\"{output_path_partition}{lat_range[0]}_{lon_range[0]}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "34f2ca1b-db93-4986-a2f1-6e80f499dcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stitched image saved at /home/yahia/Documents/Jupyter/Sargassum/Images/Test/Overlay/partitioned.png\n"
     ]
    }
   ],
   "source": [
    "# Stitching them together\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Overlay/Partition\"\n",
    "    stitch_images(image_path, '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Overlay/partitioned.png',lat_partitions=latitude_partitions, lon_partitions=longitude_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5387c44f-7d42-41d7-8f1f-387d5413891e",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Similar to the warp function in other notebooks, we're going to try to reproduce the second day using the calculated flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8455aa74-b82b-442b-bb92-6e7c26916f8d",
   "metadata": {},
   "source": [
    "### *predict*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4213ccfd-df37-45cf-aec2-e3c8c1a20506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(file_path, output_path, time_interval=86400):\n",
    "    # Load the NetCDF data\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    \n",
    "    # Calculate pixel distances\n",
    "    latitudes = ds.latitude.values\n",
    "    longitudes = ds.longitude.values\n",
    "    d_lat_m = haversine(longitudes[0], latitudes[0], longitudes[0], latitudes[1])*1000\n",
    "    d_lon_m = haversine(longitudes[0], latitudes[0], longitudes[1], latitudes[0])*1000\n",
    "    \n",
    "    # Convert flow from m/s back to pixel displacement\n",
    "    flow_u_pixels = ds['flow_u'].values * (time_interval / d_lon_m)\n",
    "    flow_v_pixels = ds['flow_v'].values * (time_interval / d_lat_m)\n",
    "    flow_u_f_pixels = ds['flow_u_f'].values * (time_interval / d_lon_m)\n",
    "    flow_v_f_pixels = ds['flow_v_f'].values * (time_interval / d_lat_m)\n",
    "\n",
    "    # Initialize prediction arrays\n",
    "    prediction = np.zeros_like(ds['fai_anomaly'].values)\n",
    "    prediction_f = np.zeros_like(ds['filtered'].values)\n",
    "    \n",
    "    # Function to update the position based on flow vectors\n",
    "    def update_position(data, flow_u, flow_v):\n",
    "        updated_data = np.zeros_like(data)\n",
    "        for i in range(data.shape[0]):\n",
    "            for j in range(data.shape[1]):\n",
    "                if data[i, j] != 0:\n",
    "                    new_i = int(round(i + flow_v[i, j]))\n",
    "                    new_j = int(round(j + flow_u[i, j]))\n",
    "                    if 0 <= new_i < data.shape[0] and 0 <= new_j < data.shape[1]:\n",
    "                        updated_data[new_i, new_j] = 255\n",
    "        return updated_data\n",
    "\n",
    "    # Apply flow data to update positions\n",
    "    prediction = update_position(ds['fai_anomaly'].values, flow_u_pixels, flow_v_pixels)\n",
    "    prediction_f = update_position(ds['filtered'].values, flow_u_f_pixels, flow_v_f_pixels)\n",
    "    \n",
    "    # Create a new dataset to hold the predictions\n",
    "    predicted_ds = xr.Dataset({\n",
    "        'prediction': (['latitude', 'longitude'], prediction),\n",
    "        'prediction_f': (['latitude', 'longitude'], prediction_f)\n",
    "    }, coords={'latitude': ds.latitude, 'longitude': ds.longitude})\n",
    "\n",
    "    # Save the dataset\n",
    "    predicted_ds.to_netcdf(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930110b-0745-4772-8649-cab814a92208",
   "metadata": {},
   "source": [
    "### *backtrack*\n",
    "We use the result of the *predict* function to recreate the first day and validate this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93a25424-cd9e-4f8b-9840-f042517a25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack(file_path, predicted_file_path, output_path, time_interval=86400):\n",
    "    # Load the NetCDF data for original and predicted\n",
    "    ds_original = xr.open_dataset(file_path)\n",
    "    ds_predicted = xr.open_dataset(predicted_file_path)\n",
    "    \n",
    "    # Calculate pixel distances\n",
    "    latitudes = ds_original.latitude.values\n",
    "    longitudes = ds_original.longitude.values\n",
    "    d_lat_m = haversine(longitudes[0], latitudes[0], longitudes[0], latitudes[1])\n",
    "    d_lon_m = haversine(longitudes[0], latitudes[0], longitudes[1], latitudes[0])\n",
    "    \n",
    "    # Convert flow from m/s back to pixel displacement (using negative for backtracking)\n",
    "    flow_u_pixels = -ds_original['flow_u'].values * (time_interval / d_lon_m)\n",
    "    flow_v_pixels = -ds_original['flow_v'].values * (time_interval / d_lat_m)\n",
    "    flow_u_f_pixels = -ds_original['flow_u_f'].values * (time_interval / d_lon_m)\n",
    "    flow_v_f_pixels = -ds_original['flow_v_f'].values * (time_interval / d_lat_m)\n",
    "\n",
    "    # Initialize backtracked prediction arrays\n",
    "    backtracked_prediction = np.zeros_like(ds_original['fai_anomaly'].values)\n",
    "    backtracked_prediction_f = np.zeros_like(ds_original['filtered'].values)\n",
    "    \n",
    "    # Function to update the position based on flow vectors\n",
    "    def update_position(data, flow_u, flow_v):\n",
    "        backtracked_data = np.zeros_like(data)\n",
    "        for i in range(data.shape[0]):\n",
    "            for j in range(data.shape[1]):\n",
    "                if data[i, j] != 0:\n",
    "                    new_i = int(round(i + flow_v[i, j]))\n",
    "                    new_j = int(round(j + flow_u[i, j]))\n",
    "                    if 0 <= new_i < data.shape[0] and 0 <= new_j < data.shape[1]:\n",
    "                        backtracked_data[new_i, new_j] = 255\n",
    "        return backtracked_data\n",
    "\n",
    "    # Apply reverse flow data to update positions\n",
    "    backtracked_prediction = update_position(ds_predicted['prediction'].values, flow_u_pixels, flow_v_pixels)\n",
    "    backtracked_prediction_f = update_position(ds_predicted['prediction_f'].values, flow_u_f_pixels, flow_v_f_pixels)\n",
    "    \n",
    "    # Create a new dataset to hold the backtracked predictions\n",
    "    backtracked_ds = xr.Dataset({\n",
    "        'backtracked_prediction': (['latitude', 'longitude'], backtracked_prediction),\n",
    "        'backtracked_prediction_f': (['latitude', 'longitude'], backtracked_prediction_f)\n",
    "    }, coords={'latitude': ds_original.latitude, 'longitude': ds_original.longitude})\n",
    "\n",
    "    # Save the dataset\n",
    "    backtracked_ds.to_netcdf(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f437d-aa8a-485d-be3e-9f2ba29cd0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473c71d-d708-4982-9d62-23c25b38e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/media/yahia/ballena/Flow/DeepFlow_Masked/Masked_DeepFlow_Filtered_algae_distribution_20220723.nc\"\n",
    "    output_path = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/predict.nc\"\n",
    "    predict(file_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea43fa5-3d80-476a-b1cc-20ab3ad3cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/media/yahia/ballena/Flow/DeepFlow_Masked/Masked_DeepFlow_Filtered_algae_distribution_20220723.nc\"\n",
    "    predicted_file_path = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/predict.nc\"\n",
    "    output_path = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/backtrack.nc\"\n",
    "    backtrack(file_path, predicted_file_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe53d96-de5e-4229-beb0-622e79126391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Farneback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37130e-5785-4118-8c16-3f4945629cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/media/yahia/ballena/Flow/Farneback_Masked/Masked_Farneback_Filtered_algae_distribution_20220723.nc\"\n",
    "    output_path = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/predict_farneback.nc\"\n",
    "    predict(file_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb043ec-a8a1-41da-ac9d-478fdb3dde19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/media/yahia/ballena/Flow/Farneback_Masked/Masked_Farneback_Filtered_algae_distribution_20220723.nc\"\n",
    "    predicted_file_path = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/predict_farneback.nc\"\n",
    "    output_path = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/backtrack_farneback.nc\"\n",
    "    backtrack(file_path, predicted_file_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b74f16-02e8-4c92-9ad9-034346171be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size-Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98934381-afba-46c3-bf7a-8380f3b8ec41",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'fai_anomaly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/core/dataset.py:1393\u001b[0m, in \u001b[0;36mDataset._construct_dataarray\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1393\u001b[0m     variable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variables[name]\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'fai_anomaly'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/yahia/Documents/Jupyter/Sargassum/Images/Test/matching.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/yahia/Documents/Jupyter/Sargassum/Images/Test/predict_size_matching.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m predict(file_path, output_path)\n",
      "Cell \u001b[0;32mIn[32], line 18\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(file_path, output_path, time_interval)\u001b[0m\n\u001b[1;32m     13\u001b[0m flow_v_pixels \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflow_v\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m*\u001b[39m (time_interval \u001b[38;5;241m/\u001b[39m d_lat_m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# flow_u_f_pixels = ds['flow_u_f'].values * (time_interval / d_lon_m)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# flow_v_f_pixels = ds['flow_v_f'].values * (time_interval / d_lat_m)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Initialize prediction arrays\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m prediction \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfai_anomaly\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     19\u001b[0m prediction_f \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiltered\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Function to update the position based on flow vectors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/core/dataset.py:1484\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkey)\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mhashable(key):\n\u001b[0;32m-> 1484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_dataarray(key)\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m utils\u001b[38;5;241m.\u001b[39miterable_of_hashable(key):\n\u001b[1;32m   1486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copy_listed(key)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/core/dataset.py:1395\u001b[0m, in \u001b[0;36mDataset._construct_dataarray\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1393\u001b[0m     variable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variables[name]\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m-> 1395\u001b[0m     _, name, variable \u001b[38;5;241m=\u001b[39m _get_virtual_variable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variables, name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims)\n\u001b[1;32m   1397\u001b[0m needed_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(variable\u001b[38;5;241m.\u001b[39mdims)\n\u001b[1;32m   1399\u001b[0m coords: \u001b[38;5;28mdict\u001b[39m[Hashable, Variable] \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/core/dataset.py:196\u001b[0m, in \u001b[0;36m_get_virtual_variable\u001b[0;34m(variables, key, dim_sizes)\u001b[0m\n\u001b[1;32m    194\u001b[0m split_key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(split_key) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    198\u001b[0m ref_name, var_name \u001b[38;5;241m=\u001b[39m split_key\n\u001b[1;32m    199\u001b[0m ref_var \u001b[38;5;241m=\u001b[39m variables[ref_name]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'fai_anomaly'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/matching.nc\"\n",
    "    output_path = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/predict_size_matching.nc\"\n",
    "    predict(file_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057b482-dd98-4f0b-983c-91c4386f6911",
   "metadata": {},
   "source": [
    "## Confidence Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7e66d6-b215-4e68-89ce-44ab226c60ca",
   "metadata": {},
   "source": [
    "### *calculate_sargassum_area_ratios*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7a62178-aa4b-464b-8db7-3b412724406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sargassum_area_ratios(detection_mask1, detection_mask2, block_size):\n",
    "    confidence_scores = np.zeros_like(detection_mask1, dtype=float)\n",
    "\n",
    "    for y in range(0, detection_mask1.shape[0], block_size[1]):\n",
    "        for x in range(0, detection_mask1.shape[1], block_size[0]):\n",
    "            block1 = detection_mask1[y:y+block_size[1], x:x+block_size[0]]\n",
    "            block2 = detection_mask2[y:y+block_size[1], x:x+block_size[0]]\n",
    "\n",
    "            area1 = np.sum(block1)\n",
    "            area2 = np.sum(block2)\n",
    "\n",
    "            if area1 > 0:  # Avoid division by zero\n",
    "                confidence_scores[y:y+block_size[1], x:x+block_size[0]] = area2 / area1\n",
    "            else:\n",
    "                confidence_scores[y:y+block_size[1], x:x+block_size[0]] = np.nan  # Mark as undefined if no sargassum was detected initially\n",
    "\n",
    "    return confidence_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42ca793-2777-4bd4-87ba-21008bcbd10a",
   "metadata": {},
   "source": [
    "### *update_netcdf_with_confidence_scores*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48132051-e88d-4772-826a-36c42c8a72d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_netcdf_with_confidence_scores(nc_file_path, new_nc_file_path, block_size):\n",
    "    # Open the existing dataset\n",
    "    ds = xr.open_dataset(nc_file_path)\n",
    "\n",
    "    # Extract relevant data\n",
    "    detection_mask1 = ds['fai_anomaly'] != 0\n",
    "    detection_mask2 = np.isnan(ds['flow_u'])  # Assuming NaNs in flow_u indicate detections for the second day\n",
    "\n",
    "    # Calculate confidence scores\n",
    "    confidence_scores = calculate_sargassum_area_ratios(detection_mask1, detection_mask2, block_size)\n",
    "    \n",
    "    # Add confidence scores to the dataset\n",
    "    ds['confidence_scores'] = (('latitude', 'longitude'), confidence_scores)\n",
    "\n",
    "    # Save the dataset as a new file\n",
    "    ds.to_netcdf(new_nc_file_path, mode='w')  # Write mode to create a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "556787fc-89c1-47ca-98d5-a65b370eba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    original_path = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Overlay_Masked_DeepFlow_Filtered_algae_distribution_20220723.nc\"\n",
    "    new_path = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/confidence.nc\"\n",
    "    update_netcdf_with_confidence_scores(original_path, new_path, block_size=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3057348a-6a5b-4988-881f-407890ba99c9",
   "metadata": {},
   "source": [
    "### *calculate_and_update_nc_with_ratios*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "adfde3ca-7bfe-457b-ab98-d96fc7c126ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_update_nc_with_ratios(nc_file_path_day1, nc_file_path_day2, new_nc_file_path, block_size, variable_key=\"fai_anomaly\"):\n",
    "    \"\"\"\n",
    "    Calculate block-level detection ratios and add them as a new variable to the original dataset dimensions.\n",
    "    \n",
    "    Parameters:\n",
    "    - nc_file_path_day1: Path to the NetCDF file for the first day.\n",
    "    - nc_file_path_day2: Path to the NetCDF file for the second day.\n",
    "    - new_nc_file_path: Path to save the updated NetCDF file.\n",
    "    - block_size: Tuple indicating the size of the blocks (height, width).\n",
    "    - variable_key: Key for the variable in the NetCDF file that indicates detections.\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    ds1 = xr.open_dataset(nc_file_path_day1)\n",
    "    ds2 = xr.open_dataset(nc_file_path_day2)\n",
    "\n",
    "    # Calculate ratios block by block and map to each pixel within blocks\n",
    "    num_rows, num_cols = ds1.dims['latitude'], ds1.dims['longitude']\n",
    "    ratios = np.full((num_rows, num_cols), np.nan)  # Initialize ratios array\n",
    "\n",
    "    detection_mask1 = ds1[variable_key] != 0\n",
    "    detection_mask2 = ds2[variable_key] != 0\n",
    "\n",
    "    for i in range(0, num_rows, block_size[0]):\n",
    "        for j in range(0, num_cols, block_size[1]):\n",
    "            block_mask1 = detection_mask1.isel(latitude=slice(i, i + block_size[0]), longitude=slice(j, j + block_size[1]))\n",
    "            block_mask2 = detection_mask2.isel(latitude=slice(i, i + block_size[0]), longitude=slice(j, j + block_size[1]))\n",
    "            count_day1 = block_mask1.sum().item()\n",
    "            count_day2 = block_mask2.sum().item()\n",
    "            ratio = count_day2 / count_day1 if count_day1 > 0 else np.nan\n",
    "            ratios[i:i + block_size[0], j:j + block_size[1]] = ratio\n",
    "\n",
    "    # Add the new ratios directly as a variable aligned with the original data dimensions\n",
    "    ds1[variable_key + '_ratios'] = (('latitude', 'longitude'), ratios)\n",
    "\n",
    "    # Save the updated dataset\n",
    "    ds1.to_netcdf(new_nc_file_path, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aac0f2c1-ac8b-4d5f-beeb-7085a63a52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    day1_path = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220723.nc\"\n",
    "    day2_path = \"/media/yahia/ballena/ABI/NetCDF/Atlantic/Filtered/Filtered_algae_distribution_20220724.nc\"\n",
    "    output_path = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/ratios.nc\"\n",
    "    block_size = (50, 50) \n",
    "    calculate_and_update_nc_with_ratios(day1_path, day2_path, output_path, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe9b6d-dc19-4a4d-9eac-c846e28824a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
