{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea458dd-4de5-4f05-8490-13406ecbbc07",
   "metadata": {},
   "source": [
    "# Comparison\n",
    "The point of this notebook is to compare the results of our algorithms to the glorys12 data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2810faf5-4fc1-4d56-8975-a921fb48ad6c",
   "metadata": {},
   "source": [
    "## Importing necessary libraries and notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530bdef0-1952-453a-afa0-de1b3906095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import io\n",
    "import os\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import netCDF4 as nc\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib import ticker\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from matplotlib.collections import LineCollection\n",
    "from IPython.display import Image, display, clear_output\n",
    "from PIL import Image as PILImage\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# Import the other notebooks without running their cells\n",
    "from ii_Data_Manipulation import visualize_4\n",
    "from iii_GOES_average import time_list, visualize_aggregate, calculate_median\n",
    "from iv_Image_Processing import collect_times, crop_image, save_aggregate, binarize_image, bilateral_image, process_dates, process_directory\n",
    "from vii_Flow_Analysis import haversine\n",
    "from v_i_OF_Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ee3d7-e306-4c36-8a97-e124dfb1c571",
   "metadata": {},
   "source": [
    "## *compare_flows*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4112df16-c04a-41f0-8df1-a8429a8d7fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_flows(my_dataset_path, output_path, glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc', comparison_date='2022-07-23'):\n",
    "    \"\"\"\n",
    "    Compares flow vectors from a custom dataset with ocean currents from the GLORYS12 dataset for a specific date\n",
    "    and saves the differences. The custom dataset is downsampled to match the GLORYS dataset grid.\n",
    "    \n",
    "    Parameters:\n",
    "    - my_dataset_path (str): Path to the custom NetCDF file with flow variables.\n",
    "    - glorys_dataset_path (str): Path to the GLORYS12 ocean currents NetCDF file.\n",
    "    - output_path (str): Path to save the resulting differences as a NetCDF file.\n",
    "    - comparison_date (str): Specific date to extract data for comparison (default '2022-07-23').\n",
    "    \"\"\"\n",
    "    # Load the custom and GLORYS datasets\n",
    "    my_data = xr.open_dataset(my_dataset_path)\n",
    "    glorys_data = xr.open_dataset(glorys_dataset_path)\n",
    "    \n",
    "    # Extract data for the specific date and remove singleton time dimensions\n",
    "    current_u = glorys_data['uo'].sel(time=comparison_date).squeeze()\n",
    "    current_v = glorys_data['vo'].sel(time=comparison_date).squeeze()\n",
    "    \n",
    "    # Optional: Subset GLORYS data to the extent of my_data to ensure proper overlap\n",
    "    current_u = current_u.sel(latitude=slice(my_data.latitude.min(), my_data.latitude.max()),\n",
    "                              longitude=slice(my_data.longitude.min(), my_data.longitude.max()))\n",
    "    current_v = current_v.sel(latitude=slice(my_data.latitude.min(), my_data.latitude.max()),\n",
    "                              longitude=slice(my_data.longitude.min(), my_data.longitude.max()))\n",
    "    \n",
    "    # Downsample my_data to match the GLORYS data grid\n",
    "    my_data_downsampled = my_data.reindex_like(current_u, method='nearest')\n",
    "\n",
    "    # Compute differences for each set of flow variables\n",
    "    d_flow_u = abs(my_data_downsampled['flow_u'] - current_u)\n",
    "    d_flow_v = abs(my_data_downsampled['flow_v'] - current_v)\n",
    "    d_flow_u_m = abs(my_data_downsampled['flow_u_m'] - current_u)\n",
    "    d_flow_v_m = abs(my_data_downsampled['flow_v_m'] - current_v)\n",
    "    d_flow_u_f = abs(my_data_downsampled['flow_u_f'] - current_u)\n",
    "    d_flow_v_f = abs(my_data_downsampled['flow_v_f'] - current_v)\n",
    "    \n",
    "    # Create a new dataset to store the differences\n",
    "    diff_dataset = xr.Dataset({\n",
    "        'd_flow_u': d_flow_u,\n",
    "        'd_flow_v': d_flow_v,\n",
    "        'd_flow_u_m': d_flow_u_m,\n",
    "        'd_flow_v_m': d_flow_v_m,\n",
    "        'd_flow_u_f': d_flow_u_f,\n",
    "        'd_flow_v_f': d_flow_v_f\n",
    "    })\n",
    "    \n",
    "    # Save the differences dataset to a NetCDF file\n",
    "    diff_dataset.to_netcdf(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2f1c80-c0a3-499f-8eee-28e7ba20a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFlow\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        output_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/difference_flow.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ea8b3-7057-4a0d-9b8d-0fdaf35b4da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Farneback\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        output_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/difference_farneback.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5f81a-6ab3-4eb4-8219-7ab1006b6695",
   "metadata": {},
   "source": [
    "## *compare_flows_scatter*\n",
    "Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dcaf75-abe3-464a-a0b1-d4dac5787cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_flows_scatter(my_dataset_path, glorys_dataset_path, comparison_date='2022-07-23', \n",
    "                                     flow_u_name='flow_u', flow_v_name='flow_v'):\n",
    "    # Load datasets\n",
    "    my_data = xr.open_dataset(my_dataset_path)\n",
    "    glorys_data = xr.open_dataset(glorys_dataset_path)\n",
    "\n",
    "    # Select specific date and squeeze out singletons for both u and v components from GLORYS dataset\n",
    "    glorys_u = glorys_data['uo'].sel(time=comparison_date).squeeze()\n",
    "    glorys_v = glorys_data['vo'].sel(time=comparison_date).squeeze()\n",
    "    \n",
    "    # Extract the custom flow components using the provided names\n",
    "    my_flow_u = my_data[flow_u_name]\n",
    "    my_flow_v = my_data[flow_v_name]\n",
    "\n",
    "    # Output minimum and maximum values for diagnostics\n",
    "    print(f\"Min/Max {flow_u_name}: {my_flow_u.min().data}, {my_flow_u.max().data}\")\n",
    "    print(f\"Min/Max {flow_v_name}: {my_flow_v.min().data}, {my_flow_v.max().data}\")\n",
    "\n",
    "    # Restrict both datasets to the common spatial extent (for safety)\n",
    "    common_lat_min = max(min(my_flow_u.latitude), min(glorys_u.latitude))\n",
    "    common_lat_max = min(max(my_flow_u.latitude), max(glorys_u.latitude))\n",
    "    common_lon_min = max(min(my_flow_u.longitude), min(glorys_u.longitude))\n",
    "    common_lon_max = min(max(my_flow_u.longitude), max(glorys_u.longitude))\n",
    "\n",
    "    # Restrict both datasets to the common spatial extent\n",
    "    my_flow_u = my_flow_u.sel(latitude=slice(common_lat_min, common_lat_max), longitude=slice(common_lon_min, common_lon_max))\n",
    "    my_flow_v = my_flow_v.sel(latitude=slice(common_lat_min, common_lat_max), longitude=slice(common_lon_min, common_lon_max))\n",
    "    glorys_u = glorys_u.sel(latitude=slice(common_lat_min, common_lat_max), longitude=slice(common_lon_min, common_lon_max))\n",
    "    glorys_v = glorys_v.sel(latitude=slice(common_lat_min, common_lat_max), longitude=slice(common_lon_min, common_lon_max))\n",
    "    \n",
    "    # Downsample my dataset to match GLORYS grid using reindex_like for both u and v components\n",
    "    my_flow_u_downsampled = my_flow_u.reindex_like(glorys_u, method='nearest')\n",
    "    my_flow_v_downsampled = my_flow_v.reindex_like(glorys_v, method='nearest')\n",
    "\n",
    "    # Flatten the data to 1D arrays for statistical analysis\n",
    "    glorys_u_flat = glorys_u.values.flatten()\n",
    "    my_flow_u_flat = my_flow_u_downsampled.values.flatten()\n",
    "    glorys_v_flat = glorys_v.values.flatten()\n",
    "    my_flow_v_flat = my_flow_v_downsampled.values.flatten()\n",
    "\n",
    "    # Clean non-finite values from data arrays\n",
    "    valid_indices_u = np.isfinite(glorys_u_flat) & np.isfinite(my_flow_u_flat)\n",
    "    valid_indices_v = np.isfinite(glorys_v_flat) & np.isfinite(my_flow_v_flat)\n",
    "    glorys_u_flat = glorys_u_flat[valid_indices_u]\n",
    "    my_flow_u_flat = my_flow_u_flat[valid_indices_u]\n",
    "    glorys_v_flat = glorys_v_flat[valid_indices_v]\n",
    "    my_flow_v_flat = my_flow_v_flat[valid_indices_v]\n",
    "\n",
    "    # Calculate correlation for both u and v components\n",
    "    if len(glorys_u_flat) == len(my_flow_u_flat) and len(glorys_v_flat) == len(my_flow_v_flat):\n",
    "        correlation_u, _ = scipy.stats.pearsonr(glorys_u_flat, my_flow_u_flat)\n",
    "        correlation_v, _ = scipy.stats.pearsonr(glorys_v_flat, my_flow_v_flat)\n",
    "        print(\"Correlation coefficient for u:\", correlation_u)\n",
    "        print(\"Correlation coefficient for v:\", correlation_v)\n",
    "    else:\n",
    "        print(\"Data arrays do not match in size or are empty after cleaning.\")\n",
    "\n",
    "    # Plotting for u component\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(glorys_u_flat, my_flow_u_flat, alpha=0.5)\n",
    "    plt.title(f'Scatter Plot of GLORYS uo vs. {flow_u_name}')\n",
    "    plt.xlabel('GLORYS uo (m/s)')\n",
    "    plt.ylabel(f'{flow_u_name} (m/s)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting for v component\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(glorys_v_flat, my_flow_v_flat, alpha=0.5)\n",
    "    plt.title(f'Scatter Plot of GLORYS vo vs. {flow_v_name}')\n",
    "    plt.xlabel('GLORYS vo (m/s)')\n",
    "    plt.ylabel(f'{flow_v_name} (m/s)')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25c68e0-876c-48cc-8429-bc25f9d8ca45",
   "metadata": {},
   "source": [
    "## DeepFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a6543-ca0c-443e-b046-ae94b59f762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFlow\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e72cd82-2a71-4c29-885c-3dfb37298f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked DeepFlow\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_m\", flow_v_name=\"flow_v_m\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f48ac-2bf4-401c-868f-cf823e730dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered DeepFlow\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_f\", flow_v_name=\"flow_v_f\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3358e9d-fed2-4124-8975-7fab06d1533b",
   "metadata": {},
   "source": [
    "## DeepFlow (Masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e28629-0acb-400e-bea5-7bf60aadbad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFlow\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow_masked.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff436c-7ff4-47db-ad69-d95cb2472553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked DeepFlow\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow_masked.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_m\", flow_v_name=\"flow_v_m\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2330c1-7aaf-4938-949a-3af74f5c6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered DeepFlow\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_flow_masked.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_f\", flow_v_name=\"flow_v_f\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c743e981-1045-4958-97eb-39f56e0d07d1",
   "metadata": {},
   "source": [
    "## Farneback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d00ef43-e48e-4644-b4b8-2e98c5e80f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Farneback\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd9824-e9da-48d4-acfb-c9f000dea53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Farneback\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_m\", flow_v_name=\"flow_v_m\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193229d2-d0e5-4942-b7fd-1ef149299478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered Farneback\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_f\", flow_v_name=\"flow_v_f\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79398ee9-3150-4f4e-82ce-7fda478b3239",
   "metadata": {},
   "source": [
    "## Farneback (Masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a730c20-94c4-4569-be9e-992fedbdbf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Farneback\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback_masked.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165104ea-1b44-4e0f-b66b-e1c7515057dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Farneback\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback_masked.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_m\", flow_v_name=\"flow_v_m\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821afb6d-af0e-46c1-94b6-8731c7c9c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered Farneback\n",
    "if __name__ == \"__main__\":\n",
    "    compare_flows_scatter(\n",
    "        my_dataset_path='/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Filtered_with_farneback_masked.nc',\n",
    "        glorys_dataset_path='/media/yahia/ballena/GLORYS12_SARG/glorys12_1d_2022.nc',\n",
    "        flow_u_name=\"flow_u_f\", flow_v_name=\"flow_v_f\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c4b8bb-63ab-4c68-a6b6-16e62ae1ef0a",
   "metadata": {},
   "source": [
    "## Overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7836a23-d5b0-4b54-a55b-e547e5a4f031",
   "metadata": {},
   "source": [
    "### Overlay (NetCDF)\n",
    "For every pair of days, we overlay the detections of the second day (as NaN) on the Masked Flow NetCDF that contains the flow values applied on the detections of the first day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8705010a-5a11-4d66-b003-b7e865e35a9d",
   "metadata": {},
   "source": [
    "#### *overlay_detections*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b68f200-b2af-45a4-9856-6f99a0794c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_detections(flow_data_path, detection_data_path, output_path):\n",
    "    \"\"\"\n",
    "    Load the flow data of the first day and the detection data of the second day.\n",
    "    Apply the detection mask from the second day onto the flow data of the first day.\n",
    "    \"\"\"\n",
    "    # Load the flow data from the first day\n",
    "    flow_data = xr.open_dataset(flow_data_path)\n",
    "    \n",
    "    # Load the detection data from the second day\n",
    "    detection_data = xr.open_dataset(detection_data_path)\n",
    "    \n",
    "    # Assume 'fai_anomaly' indicates the presence of detections, convert to mask\n",
    "    detection_mask = detection_data['fai_anomaly'] != 0\n",
    "    \n",
    "    # Overlay the detection mask onto the flow data\n",
    "    for var in flow_data.data_vars:\n",
    "        if 'flow' in var:  # Only modify flow variables\n",
    "            # Set flow data to NaN where there are detections on the second day\n",
    "            flow_data[var] = xr.where(detection_mask, np.nan, flow_data[var])\n",
    "    \n",
    "    # Save the modified flow data\n",
    "    flow_data.to_netcdf(output_path)\n",
    "    print(f\"Modified flow data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4530414b-9246-47f1-b1d1-c34d4ea41acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Paths\n",
    "    source_directory = '/media/yahia/ballena/Flow/DeepFlow_Masked'\n",
    "    output_directory = '/media/yahia/ballena/Flow/DeepFlow_Overlay'\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # List of files sorted to ensure chronological order\n",
    "    files = sorted([f for f in os.listdir(source_directory) if f.endswith('.nc')])\n",
    "    \n",
    "    # Process each pair of consecutive files\n",
    "    for i in range(len(files) - 1):\n",
    "        first_file = os.path.join(source_directory, files[i])\n",
    "        second_file = os.path.join(source_directory, files[i + 1])\n",
    "        \n",
    "        # Define the output path for the processed NetCDF file\n",
    "        output_path = os.path.join(output_directory, f'Overlay_{files[i]}')\n",
    "        \n",
    "        # Overlay the detections from the second day onto the flow data from the first day\n",
    "        overlay_detections(first_file, second_file, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cf745d-2d59-4b5b-a120-f0d07695c760",
   "metadata": {},
   "source": [
    "### Overlay (PNG)\n",
    "This second approach uses the result of the previous overlay function and plots corresponding flow vectors on a PNG image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1d24b5-89c9-4dd9-90db-ad3a2b430c7b",
   "metadata": {},
   "source": [
    "#### *overlay_png*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a25a7-5142-4baa-8404-5f51bc756f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_png(file_path, output_path, quiver_scale=100, quiver_step=50):\n",
    "    \"\"\"\n",
    "    Overlays flow vectors on the geographic map of a NetCDF file and saves it as a high-resolution PNG,\n",
    "    using flow_u or flow_v data to determine the overlay and base colors.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: Path to the NetCDF file.\n",
    "    - output_path: Path to save the PNG image.\n",
    "    - quiver_scale: Scaling factor for vectors to adjust their length.\n",
    "    - quiver_step: Sampling rate for vectors to avoid overcrowding the plot.\n",
    "    \"\"\"\n",
    "    # Load data from NetCDF\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    u = ds['flow_u']\n",
    "    v = ds['flow_v']\n",
    "\n",
    "    # Create the figure and axes with a geographic projection\n",
    "    fig, ax = plt.subplots(figsize=(15, 10), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "    ax.coastlines()  # Add coastlines for reference\n",
    "\n",
    "    # Create color mapping based on flow_u data\n",
    "    # Green where flow_u is not zero and not NaN, yellow where NaN\n",
    "    color = np.where(np.isnan(u), 'yellow', np.where(u != 0, 'green', 'none'))\n",
    "\n",
    "    # Create a meshgrid for the longitude and latitude\n",
    "    lon, lat = np.meshgrid(ds.longitude, ds.latitude)\n",
    "\n",
    "    # Scatter plot for color visualization\n",
    "    ax.scatter(lon.flatten(), lat.flatten(), color=color.flatten(), s=1, transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Mask the vector fields where flow_u is not zero and not NaN\n",
    "    mask = (u != 0) & np.isfinite(u)\n",
    "    U = u.where(mask)\n",
    "    V = v.where(mask)\n",
    "\n",
    "    # Quiver plot for the vector field\n",
    "    ax.quiver(lon[::quiver_step, ::quiver_step], lat[::quiver_step, ::quiver_step],\n",
    "              U[::quiver_step, ::quiver_step], V[::quiver_step, ::quiver_step],\n",
    "              scale=quiver_scale, color='red', transform=ccrs.PlateCarree())\n",
    "\n",
    "    # Set title and save the plot\n",
    "    ax.set_title('Overlay of Flow Vectors on Detection Data', fontsize=15)\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)  # Close the figure to free resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef914cb-220b-4b16-b7ae-b8849c8b6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    overlay_png('/media/yahia/ballena/Flow/DeepFlow_Overlay/Overlay_Masked_DeepFlow_Filtered_algae_distribution_20220723.nc',\n",
    "                '/home/yahia/Documents/Jupyter/Sargassum/Images/Test/Overlay.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5387c44f-7d42-41d7-8f1f-387d5413891e",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Similar to the warp function in other notebooks, we're going to try to reproduce the second day using the calculated flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8455aa74-b82b-442b-bb92-6e7c26916f8d",
   "metadata": {},
   "source": [
    "### *predict*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4213ccfd-df37-45cf-aec2-e3c8c1a20506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(file_path, output_path, time_interval=86400):\n",
    "    # Load the NetCDF data\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    \n",
    "    # Calculate pixel distances\n",
    "    latitudes = ds.latitude.values\n",
    "    longitudes = ds.longitude.values\n",
    "    d_lat_m = haversine(longitudes[0], latitudes[0], longitudes[0], latitudes[1])*1000\n",
    "    d_lon_m = haversine(longitudes[0], latitudes[0], longitudes[1], latitudes[0])*1000\n",
    "    \n",
    "    # Convert flow from m/s back to pixel displacement\n",
    "    flow_u_pixels = ds['flow_u'].values * (time_interval / d_lon_m)\n",
    "    flow_v_pixels = ds['flow_v'].values * (time_interval / d_lat_m)\n",
    "    flow_u_f_pixels = ds['flow_u_f'].values * (time_interval / d_lon_m)\n",
    "    flow_v_f_pixels = ds['flow_v_f'].values * (time_interval / d_lat_m)\n",
    "\n",
    "    # Initialize prediction arrays\n",
    "    prediction = np.zeros_like(ds['fai_anomaly'].values)\n",
    "    prediction_f = np.zeros_like(ds['filtered'].values)\n",
    "    \n",
    "    # Function to update the position based on flow vectors\n",
    "    def update_position(data, flow_u, flow_v):\n",
    "        updated_data = np.zeros_like(data)\n",
    "        for i in range(data.shape[0]):\n",
    "            for j in range(data.shape[1]):\n",
    "                if data[i, j] != 0:\n",
    "                    new_i = int(round(i + flow_v[i, j]))\n",
    "                    new_j = int(round(j + flow_u[i, j]))\n",
    "                    if 0 <= new_i < data.shape[0] and 0 <= new_j < data.shape[1]:\n",
    "                        updated_data[new_i, new_j] = 255\n",
    "        return updated_data\n",
    "\n",
    "    # Apply flow data to update positions\n",
    "    prediction = update_position(ds['fai_anomaly'].values, flow_u_pixels, flow_v_pixels)\n",
    "    prediction_f = update_position(ds['filtered'].values, flow_u_f_pixels, flow_v_f_pixels)\n",
    "    \n",
    "    # Create a new dataset to hold the predictions\n",
    "    predicted_ds = xr.Dataset({\n",
    "        'prediction': (['latitude', 'longitude'], prediction),\n",
    "        'prediction_f': (['latitude', 'longitude'], prediction_f)\n",
    "    }, coords={'latitude': ds.latitude, 'longitude': ds.longitude})\n",
    "\n",
    "    # Save the dataset\n",
    "    predicted_ds.to_netcdf(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930110b-0745-4772-8649-cab814a92208",
   "metadata": {},
   "source": [
    "### *backtrack*\n",
    "We use the result of the *predict* function to recreate the first day and validate this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a25424-cd9e-4f8b-9840-f042517a25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack(file_path, predicted_file_path, output_path, time_interval=86400):\n",
    "    # Load the NetCDF data for original and predicted\n",
    "    ds_original = xr.open_dataset(file_path)\n",
    "    ds_predicted = xr.open_dataset(predicted_file_path)\n",
    "    \n",
    "    # Calculate pixel distances\n",
    "    latitudes = ds_original.latitude.values\n",
    "    longitudes = ds_original.longitude.values\n",
    "    d_lat_m = haversine(longitudes[0], latitudes[0], longitudes[0], latitudes[1])\n",
    "    d_lon_m = haversine(longitudes[0], latitudes[0], longitudes[1], latitudes[0])\n",
    "    \n",
    "    # Convert flow from m/s back to pixel displacement (using negative for backtracking)\n",
    "    flow_u_pixels = -ds_original['flow_u'].values * (time_interval / d_lon_m)\n",
    "    flow_v_pixels = -ds_original['flow_v'].values * (time_interval / d_lat_m)\n",
    "    flow_u_f_pixels = -ds_original['flow_u_f'].values * (time_interval / d_lon_m)\n",
    "    flow_v_f_pixels = -ds_original['flow_v_f'].values * (time_interval / d_lat_m)\n",
    "\n",
    "    # Initialize backtracked prediction arrays\n",
    "    backtracked_prediction = np.zeros_like(ds_original['fai_anomaly'].values)\n",
    "    backtracked_prediction_f = np.zeros_like(ds_original['filtered'].values)\n",
    "    \n",
    "    # Function to update the position based on flow vectors\n",
    "    def update_position(data, flow_u, flow_v):\n",
    "        backtracked_data = np.zeros_like(data)\n",
    "        for i in range(data.shape[0]):\n",
    "            for j in range(data.shape[1]):\n",
    "                if data[i, j] != 0:\n",
    "                    new_i = int(round(i + flow_v[i, j]))\n",
    "                    new_j = int(round(j + flow_u[i, j]))\n",
    "                    if 0 <= new_i < data.shape[0] and 0 <= new_j < data.shape[1]:\n",
    "                        backtracked_data[new_i, new_j] = 255\n",
    "        return backtracked_data\n",
    "\n",
    "    # Apply reverse flow data to update positions\n",
    "    backtracked_prediction = update_position(ds_predicted['prediction'].values, flow_u_pixels, flow_v_pixels)\n",
    "    backtracked_prediction_f = update_position(ds_predicted['prediction_f'].values, flow_u_f_pixels, flow_v_f_pixels)\n",
    "    \n",
    "    # Create a new dataset to hold the backtracked predictions\n",
    "    backtracked_ds = xr.Dataset({\n",
    "        'backtracked_prediction': (['latitude', 'longitude'], backtracked_prediction),\n",
    "        'backtracked_prediction_f': (['latitude', 'longitude'], backtracked_prediction_f)\n",
    "    }, coords={'latitude': ds_original.latitude, 'longitude': ds_original.longitude})\n",
    "\n",
    "    # Save the dataset\n",
    "    backtracked_ds.to_netcdf(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f437d-aa8a-485d-be3e-9f2ba29cd0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473c71d-d708-4982-9d62-23c25b38e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/media/yahia/ballena/Flow/DeepFlow_Masked/Masked_DeepFlow_Filtered_algae_distribution_20220723.nc\"\n",
    "    output_path = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/predict.nc\"\n",
    "    predict(file_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea43fa5-3d80-476a-b1cc-20ab3ad3cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/media/yahia/ballena/Flow/DeepFlow_Masked/Masked_DeepFlow_Filtered_algae_distribution_20220723.nc\"\n",
    "    predicted_file_path = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/predict.nc\"\n",
    "    output_path = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/backtrack.nc\"\n",
    "    backtrack(file_path, predicted_file_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe53d96-de5e-4229-beb0-622e79126391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Farneback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37130e-5785-4118-8c16-3f4945629cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/media/yahia/ballena/Flow/Farneback_Masked/Masked_Farneback_Filtered_algae_distribution_20220723.nc\"\n",
    "    output_path = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/predict_farneback.nc\"\n",
    "    predict(file_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb043ec-a8a1-41da-ac9d-478fdb3dde19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"/media/yahia/ballena/Flow/Farneback_Masked/Masked_Farneback_Filtered_algae_distribution_20220723.nc\"\n",
    "    predicted_file_path = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/predict_farneback.nc\"\n",
    "    output_path = \"/home/yahia/Documents/Jupyter/Sargassum/Images/Test/backtrack_farneback.nc\"\n",
    "    backtrack(file_path, predicted_file_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d655e6d-49fa-4423-8f93-c60b7a00ad74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
